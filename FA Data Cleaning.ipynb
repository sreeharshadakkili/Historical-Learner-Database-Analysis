{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "813a194f-a227-4fd2-9640-04fc23331675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Error reading Learner+Enrollment+ENGLISH+(1.1.23)++2025-02-25-10-06-41+5016540.csv.csv: Error tokenizing data. C error: Expected 1 fields in line 3, saw 106\n",
      "\n",
      "‚ö†Ô∏è Error reading (OLD)+TGH+Learner+Enrollment+Form+2025-02-26-02-50-39+457499.csv.csv: Error tokenizing data. C error: Expected 1 fields in line 3, saw 14\n",
      "\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ace_tools'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# üîπ Convert file stats to a DataFrame for display\u001b[39;00m\n\u001b[1;32m     37\u001b[0m df_stats \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_dict(file_stats, orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mace_tools\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtools\u001b[39;00m\n\u001b[1;32m     39\u001b[0m tools\u001b[38;5;241m.\u001b[39mdisplay_dataframe_to_user(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile Statistics\u001b[39m\u001b[38;5;124m\"\u001b[39m, dataframe\u001b[38;5;241m=\u001b[39mdf_stats)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# üîπ Identify unique columns in each file\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ace_tools'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# üîπ Path to the folder where all CSV files are stored\n",
    "folder_path = \"/Users/sreeharsha/Documents/TGH Data Management Cleaning/Enrollment Survey\"  # Update if files are in a different location\n",
    "\n",
    "# üîπ List all CSV files in the folder\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith(\".csv\")]\n",
    "\n",
    "# üîπ Dictionary to store file stats\n",
    "file_stats = {}\n",
    "\n",
    "# üîπ Dictionary to store unique columns for each file\n",
    "columns_dict = {}\n",
    "\n",
    "# üîπ Read and analyze each file\n",
    "for file_name in csv_files:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "    try:\n",
    "        # Load CSV\n",
    "        df = pd.read_csv(file_path, low_memory=False)\n",
    "        \n",
    "        # Get number of rows and columns\n",
    "        num_rows, num_columns = df.shape\n",
    "\n",
    "        # Store file stats\n",
    "        file_stats[file_name] = {\"Rows\": num_rows, \"Columns\": num_columns}\n",
    "\n",
    "        # Store column names\n",
    "        columns_dict[file_name] = set(df.columns)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error reading {file_name}: {e}\")\n",
    "\n",
    "# üîπ Convert file stats to a DataFrame for display\n",
    "df_stats = pd.DataFrame.from_dict(file_stats, orient=\"index\")\n",
    "import ace_tools as tools\n",
    "tools.display_dataframe_to_user(name=\"File Statistics\", dataframe=df_stats)\n",
    "\n",
    "# üîπ Identify unique columns in each file\n",
    "all_columns = set.union(*columns_dict.values())  # Get all unique column names\n",
    "\n",
    "unique_columns = {file: cols - (all_columns - cols) for file, cols in columns_dict.items()}\n",
    "\n",
    "# üîπ Convert unique column data to a DataFrame\n",
    "df_unique_columns = pd.DataFrame(\n",
    "    [(file, list(unique_columns[file])) for file in unique_columns],\n",
    "    columns=[\"File Name\", \"Unique Columns\"]\n",
    ")\n",
    "\n",
    "tools.display_dataframe_to_user(name=\"Unique Columns per File\", dataframe=df_unique_columns)\n",
    "\n",
    "print(\"\\n‚úÖ Analysis completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7724683b-bc74-4967-a62c-5a69c20b6b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä File Statistics:\n",
      "                                                      Rows  Columns\n",
      "Learner+Enrollment+ENGLISH.csv                       9438      106\n",
      "Learner Enrollment HAITIAN CREOLE (3.8.23).csv       1383      105\n",
      "(OLD) TGH Learner Enrollment Form (6.2.2022).csv     2115       84\n",
      "(OLD)+TGH+Learner+Enrollment+Form+2025-02-26-02...  19150       14\n",
      "Learner Enrollment SPANISH (3.8.23).csv              2453      104\n",
      "Learner Enrollment CHINESE (3.8.23).csv               203       90\n",
      "\n",
      "üìä Unique Columns per File:\n",
      "                                            File Name  \\\n",
      "0                     Learner+Enrollment+ENGLISH.csv   \n",
      "1     Learner Enrollment HAITIAN CREOLE (3.8.23).csv   \n",
      "2   (OLD) TGH Learner Enrollment Form (6.2.2022).csv   \n",
      "3  (OLD)+TGH+Learner+Enrollment+Form+2025-02-26-0...   \n",
      "4            Learner Enrollment SPANISH (3.8.23).csv   \n",
      "5            Learner Enrollment CHINESE (3.8.23).csv   \n",
      "\n",
      "                                      Unique Columns  \n",
      "0  [What are your child's pronouns?, If you selec...  \n",
      "1  [Sitiyasyon Edikasyon- Educational Status, Sov...  \n",
      "2  [1. First Name, 6. Race/Ethnicity, 14. Number ...  \n",
      "3  [Child's Last Name, Household Income Level, Ch...  \n",
      "4  [Abrir y responder correos electr√≥nicos - Open...  \n",
      "5  [‰øùÂ≠òÊñá‰ª∂Âà∞ÁîµËÑëÔºå‰ª•ÂêéÂÜçÊü•Êâæ - Saving files to the computer ...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# üîπ Path to the folder where all CSV files are stored\n",
    "folder_path = \"/Users/sreeharsha/Documents/TGH Data Management Cleaning/Enrollment Survey\"  # Update if files are in a different location\n",
    "\n",
    "# üîπ List all CSV files in the folder\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith(\".csv\")]\n",
    "\n",
    "# üîπ Dictionary to store file stats\n",
    "file_stats = {}\n",
    "\n",
    "# üîπ Dictionary to store unique columns for each file\n",
    "columns_dict = {}\n",
    "\n",
    "# üîπ Read and analyze each file\n",
    "for file_name in csv_files:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "    try:\n",
    "        # Load CSV\n",
    "        df = pd.read_csv(file_path, low_memory=False)\n",
    "        \n",
    "        # Get number of rows and columns\n",
    "        num_rows, num_columns = df.shape\n",
    "\n",
    "        # Store file stats\n",
    "        file_stats[file_name] = {\"Rows\": num_rows, \"Columns\": num_columns}\n",
    "\n",
    "        # Store column names\n",
    "        columns_dict[file_name] = set(df.columns)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error reading {file_name}: {e}\")\n",
    "\n",
    "# üîπ Convert file stats to a DataFrame for display\n",
    "df_stats = pd.DataFrame.from_dict(file_stats, orient=\"index\")\n",
    "print(\"\\nüìä File Statistics:\\n\", df_stats)\n",
    "\n",
    "# üîπ Identify unique columns in each file\n",
    "all_columns = set.union(*columns_dict.values())  # Get all unique column names\n",
    "\n",
    "unique_columns = {file: cols - (all_columns - cols) for file, cols in columns_dict.items()}\n",
    "\n",
    "# üîπ Convert unique column data to a DataFrame\n",
    "df_unique_columns = pd.DataFrame(\n",
    "    [(file, list(unique_columns[file])) for file in unique_columns],\n",
    "    columns=[\"File Name\", \"Unique Columns\"]\n",
    ")\n",
    "\n",
    "print(\"\\nüìä Unique Columns per File:\\n\", df_unique_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e980e7c4-9b24-4674-9b08-cf099a5ffe77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä File Statistics:\n",
      "                                                      Rows  Columns\n",
      "(OLD) TGH Learner Post-Training Survey.csv          13259       57\n",
      "SPANISH - TGH Learner Post-Course Survey.csv         1411       90\n",
      "TGH Learner Post-Course Survey - Welcome Page.csv   10239       21\n",
      "English post course survey.csv                       5638       90\n",
      "Learner Post-Course Survey HAITIAN (3.8.23).csv      1088       90\n",
      "Simplified Chinese TGH Learner Post-Course Surv...    172       81\n",
      "\n",
      "üìä Unique Columns per File:\n",
      "                                            File Name  \\\n",
      "0         (OLD) TGH Learner Post-Training Survey.csv   \n",
      "1       SPANISH - TGH Learner Post-Course Survey.csv   \n",
      "2  TGH Learner Post-Course Survey - Welcome Page.csv   \n",
      "3                     English post course survey.csv   \n",
      "4    Learner Post-Course Survey HAITIAN (3.8.23).csv   \n",
      "5  Simplified Chinese TGH Learner Post-Course Sur...   \n",
      "\n",
      "                                      Unique Columns  \n",
      "0  [Child's Last Name, Child's First Name, Did TG...  \n",
      "1  [Acceso a recursos locales/municipales (asiste...  \n",
      "2  [Course Name, Name of Site/School, Unprotected...  \n",
      "3  [Finding housing (e.g. shelter, apartment hunt...  \n",
      "4  [Tanpri pataje poukisa w chwazi pa dak√≤ ditou ...  \n",
      "5  [Ë∞ÉÊï¥Âπ≥ÊùøÁîµËÑëÁöÑËÆæÁΩÆÔºàÂ¶ÇÊñáÊú¨ÁöÑÂ§ßÂ∞èÊàñËÉåÊôØÂõæÁâáÔºâ-Adjusting the tablet's...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# üîπ Path to the folder where all CSV files are stored\n",
    "folder_path = \"/Users/sreeharsha/Documents/TGH Data Management Cleaning/Post Course Forms\"  # Update if files are in a different location\n",
    "\n",
    "# üîπ List all CSV files in the folder\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith(\".csv\")]\n",
    "\n",
    "# üîπ Dictionary to store file stats\n",
    "file_stats = {}\n",
    "\n",
    "# üîπ Dictionary to store unique columns for each file\n",
    "columns_dict = {}\n",
    "\n",
    "# üîπ Read and analyze each file\n",
    "for file_name in csv_files:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "    try:\n",
    "        # Load CSV\n",
    "        df = pd.read_csv(file_path, low_memory=False)\n",
    "        \n",
    "        # Get number of rows and columns\n",
    "        num_rows, num_columns = df.shape\n",
    "\n",
    "        # Store file stats\n",
    "        file_stats[file_name] = {\"Rows\": num_rows, \"Columns\": num_columns}\n",
    "\n",
    "        # Store column names\n",
    "        columns_dict[file_name] = set(df.columns)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error reading {file_name}: {e}\")\n",
    "\n",
    "# üîπ Convert file stats to a DataFrame for display\n",
    "df_stats = pd.DataFrame.from_dict(file_stats, orient=\"index\")\n",
    "print(\"\\nüìä File Statistics:\\n\", df_stats)\n",
    "\n",
    "# üîπ Identify unique columns in each file\n",
    "all_columns = set.union(*columns_dict.values())  # Get all unique column names\n",
    "\n",
    "unique_columns = {file: cols - (all_columns - cols) for file, cols in columns_dict.items()}\n",
    "\n",
    "# üîπ Convert unique column data to a DataFrame\n",
    "df_unique_columns = pd.DataFrame(\n",
    "    [(file, list(unique_columns[file])) for file in unique_columns],\n",
    "    columns=[\"File Name\", \"Unique Columns\"]\n",
    ")\n",
    "\n",
    "print(\"\\nüìä Unique Columns per File:\\n\", df_unique_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6bccc43-f0ab-4e52-aeb7-8b4aac844a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä File Statistics:\n",
      "                                                     Rows  Columns\n",
      "Tech Goes Home 2022 Encuesta Anual.csv               139      112\n",
      "2019 Phone Tech Goes Home Annual Survey.csv           53       56\n",
      "2020 Spanish Phone Tech Goes Home Annual Survey...    36      111\n",
      "Annual Survey 2021 - English.csv                     343      130\n",
      "Phone Only - Tech Goes Home Annual Survey.csv         83      112\n",
      "Annual Survey 2021 - English Phone Bank.csv           11      129\n",
      "Annual Survey 2021 - Spanish.csv                     159      130\n",
      "2020 Haitian Creole Tech Goes Home Annual Surve...     4      112\n",
      "2019 Tech Goes Home Annual Survey.csv                408      106\n",
      "2020 Phone Tech Goes Home Annual Survey.csv          133      111\n",
      "Tech Goes Home 2024 Encuesta Anual.csv               164      132\n",
      "2020 Tech Goes Home Annual Survey.csv                398      112\n",
      "Annual Survey 2021 - English Phone Only.csv           71      131\n",
      "TGH 2023 Phone - English Annual Survey.csv            38       98\n",
      "Annual Survey 2021 - Haitian Creole.csv               15      130\n",
      "2020 Spanish Tech Goes Home Annual Survey.csv        108      112\n",
      "Annual Survey 2021 - English Phone.csv               171      133\n",
      "Annual Survey 2021 - Spanish Phone.csv                 1      133\n",
      "2019 Spanish Tech Goes Home Annual Survey.csv         39       56\n",
      "2018 Tech Goes Home Annual Survey.csv                173       42\n",
      "2018 Tech Goes Home Annual Survey (Spanish).csv       44       42\n",
      "Tech Goes Home 2023 Encuesta Anual.csv               123       97\n",
      "Tech Goes Home 2024 Sondaj Any√®l la.csv               72      114\n",
      "Tech Goes Home 2023 English Annual Survey.csv        173       97\n",
      "\n",
      "üìä Unique Columns per File:\n",
      "                                             File Name  \\\n",
      "0              Tech Goes Home 2022 Encuesta Anual.csv   \n",
      "1         2019 Phone Tech Goes Home Annual Survey.csv   \n",
      "2   2020 Spanish Phone Tech Goes Home Annual Surve...   \n",
      "3                    Annual Survey 2021 - English.csv   \n",
      "4       Phone Only - Tech Goes Home Annual Survey.csv   \n",
      "5         Annual Survey 2021 - English Phone Bank.csv   \n",
      "6                    Annual Survey 2021 - Spanish.csv   \n",
      "7   2020 Haitian Creole Tech Goes Home Annual Surv...   \n",
      "8               2019 Tech Goes Home Annual Survey.csv   \n",
      "9         2020 Phone Tech Goes Home Annual Survey.csv   \n",
      "10             Tech Goes Home 2024 Encuesta Anual.csv   \n",
      "11              2020 Tech Goes Home Annual Survey.csv   \n",
      "12        Annual Survey 2021 - English Phone Only.csv   \n",
      "13         TGH 2023 Phone - English Annual Survey.csv   \n",
      "14            Annual Survey 2021 - Haitian Creole.csv   \n",
      "15      2020 Spanish Tech Goes Home Annual Survey.csv   \n",
      "16             Annual Survey 2021 - English Phone.csv   \n",
      "17             Annual Survey 2021 - Spanish Phone.csv   \n",
      "18      2019 Spanish Tech Goes Home Annual Survey.csv   \n",
      "19              2018 Tech Goes Home Annual Survey.csv   \n",
      "20    2018 Tech Goes Home Annual Survey (Spanish).csv   \n",
      "21             Tech Goes Home 2023 Encuesta Anual.csv   \n",
      "22            Tech Goes Home 2024 Sondaj Any√®l la.csv   \n",
      "23      Tech Goes Home 2023 English Annual Survey.csv   \n",
      "\n",
      "                                       Unique Columns  \n",
      "0   [¬øParticip√≥ en TGH con un ni√±o?, Obtener emple...  \n",
      "1   [Did you make new friends or connections durin...  \n",
      "2   [¬øParticip√≥ en TGH con un ni√±o?, Obtener emple...  \n",
      "3   [Access health resources (Ex. testing sites, t...  \n",
      "4   [Recognizing numbers, counting, Please explain...  \n",
      "5   [Access health resources (Ex. testing sites, t...  \n",
      "6   [¬øParticip√≥ en TGH con un ni√±o?, Obtener emple...  \n",
      "7   [Pou pi fasil konekte ak fanmi ak zanmi, Tape....  \n",
      "8   [Recognizing numbers, counting, Please explain...  \n",
      "9   [Recognizing numbers, counting, Please explain...  \n",
      "10  [Me preocupa que el dinero que tengo o que voy...  \n",
      "11  [Recognizing numbers, counting, Please explain...  \n",
      "12  [Access health resources (Ex. testing sites, t...  \n",
      "13  [Recognizing numbers, counting, Please explain...  \n",
      "14  [√àske w se yon vot√® ki anrejistre nan Massachu...  \n",
      "15  [¬øParticip√≥ en TGH con un ni√±o?, Obtener emple...  \n",
      "16  [Access health resources (Ex. testing sites, t...  \n",
      "17  [¬øParticip√≥ en TGH con un ni√±o?, Obtener emple...  \n",
      "18  [¬øCu√°l fue su estado de empleo ANTES de partic...  \n",
      "19  [Job Searching Resources (ex. Indeed, LinkedIn...  \n",
      "20  [¬øCu√°l es su situaci√≥n actual de empleo?, ¬øC√≥m...  \n",
      "21  [¬øParticip√≥ en TGH con un ni√±o?, Obtener emple...  \n",
      "22  [√àske w ap patisipe kounye a nan pwogram ACP a...  \n",
      "23  [Recognizing numbers, counting, Please explain...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# üîπ Path to the folder where all CSV files are stored\n",
    "folder_path = \"/Users/sreeharsha/Documents/TGH Data Management Cleaning/Annual Survey\"  # Update if files are in a different location\n",
    "\n",
    "# üîπ List all CSV files in the folder\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith(\".csv\")]\n",
    "\n",
    "# üîπ Dictionary to store file stats\n",
    "file_stats = {}\n",
    "\n",
    "# üîπ Dictionary to store unique columns for each file\n",
    "columns_dict = {}\n",
    "\n",
    "# üîπ Read and analyze each file\n",
    "for file_name in csv_files:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "    try:\n",
    "        # Load CSV\n",
    "        df = pd.read_csv(file_path, low_memory=False)\n",
    "        \n",
    "        # Get number of rows and columns\n",
    "        num_rows, num_columns = df.shape\n",
    "\n",
    "        # Store file stats\n",
    "        file_stats[file_name] = {\"Rows\": num_rows, \"Columns\": num_columns}\n",
    "\n",
    "        # Store column names\n",
    "        columns_dict[file_name] = set(df.columns)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error reading {file_name}: {e}\")\n",
    "\n",
    "# üîπ Convert file stats to a DataFrame for display\n",
    "df_stats = pd.DataFrame.from_dict(file_stats, orient=\"index\")\n",
    "print(\"\\nüìä File Statistics:\\n\", df_stats)\n",
    "\n",
    "# üîπ Identify unique columns in each file\n",
    "all_columns = set.union(*columns_dict.values())  # Get all unique column names\n",
    "\n",
    "unique_columns = {file: cols - (all_columns - cols) for file, cols in columns_dict.items()}\n",
    "\n",
    "# üîπ Convert unique column data to a DataFrame\n",
    "df_unique_columns = pd.DataFrame(\n",
    "    [(file, list(unique_columns[file])) for file in unique_columns],\n",
    "    columns=[\"File Name\", \"Unique Columns\"]\n",
    ")\n",
    "\n",
    "print(\"\\nüìä Unique Columns per File:\\n\", df_unique_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abab6c35-c9ad-4767-9cd9-02d7ee4e85a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4eb1c065-b8ef-4ca6-9d19-547e36fef957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Column Differences Across Files:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Learner+Enrollment+ENGLISH.csv</th>\n",
       "      <th>Learner Enrollment HAITIAN CREOLE (3.8.23).csv</th>\n",
       "      <th>(OLD) TGH Learner Enrollment Form (6.2.2022).csv</th>\n",
       "      <th>(OLD)+TGH+Learner+Enrollment+Form+2025-02-26-02-50-39+457499.csv</th>\n",
       "      <th>Learner Enrollment SPANISH (3.8.23).csv</th>\n",
       "      <th>Learner Enrollment CHINESE (3.8.23).csv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6. Race/Ethnicity</th>\n",
       "      <td>‚ùå</td>\n",
       "      <td>‚ùå</td>\n",
       "      <td>‚úÖ</td>\n",
       "      <td>‚ùå</td>\n",
       "      <td>‚ùå</td>\n",
       "      <td>‚ùå</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sovgade fichye sou tabl√®t la epi jwenn yo annapre - Saving files to the tablet and finding them later</th>\n",
       "      <td>‚ùå</td>\n",
       "      <td>‚úÖ</td>\n",
       "      <td>‚ùå</td>\n",
       "      <td>‚ùå</td>\n",
       "      <td>‚ùå</td>\n",
       "      <td>‚ùå</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>What are your child's pronouns? (Ki pwonon pitit ou a?)</th>\n",
       "      <td>‚ùå</td>\n",
       "      <td>‚úÖ</td>\n",
       "      <td>‚ùå</td>\n",
       "      <td>‚ùå</td>\n",
       "      <td>‚ùå</td>\n",
       "      <td>‚ùå</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14. Number of people in Household</th>\n",
       "      <td>‚ùå</td>\n",
       "      <td>‚ùå</td>\n",
       "      <td>‚úÖ</td>\n",
       "      <td>‚ùå</td>\n",
       "      <td>‚ùå</td>\n",
       "      <td>‚ùå</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>¬øTiene acceso a Internet en su hogar (por ejemplo, wifi)? - Do you have home internet access (i.e. wifi)?</th>\n",
       "      <td>‚ùå</td>\n",
       "      <td>‚ùå</td>\n",
       "      <td>‚ùå</td>\n",
       "      <td>‚ùå</td>\n",
       "      <td>‚úÖ</td>\n",
       "      <td>‚ùå</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>√àske w gen aks√® ent√®n√®t (sa vle di wifi)? - Do you have home internet access (i.e. wifi)?</th>\n",
       "      <td>‚ùå</td>\n",
       "      <td>‚úÖ</td>\n",
       "      <td>‚ùå</td>\n",
       "      <td>‚ùå</td>\n",
       "      <td>‚ùå</td>\n",
       "      <td>‚ùå</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Were you born in the United States?</th>\n",
       "      <td>‚úÖ</td>\n",
       "      <td>‚ùå</td>\n",
       "      <td>‚ùå</td>\n",
       "      <td>‚ùå</td>\n",
       "      <td>‚ùå</td>\n",
       "      <td>‚ùå</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Guardar y buscar archivos en la tableta - Saving files to the tablet and finding them later</th>\n",
       "      <td>‚ùå</td>\n",
       "      <td>‚ùå</td>\n",
       "      <td>‚ùå</td>\n",
       "      <td>‚ùå</td>\n",
       "      <td>‚úÖ</td>\n",
       "      <td>‚ùå</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I am able to utilize my skills to move toward career goals.</th>\n",
       "      <td>‚úÖ</td>\n",
       "      <td>‚ùå</td>\n",
       "      <td>‚ùå</td>\n",
       "      <td>‚ùå</td>\n",
       "      <td>‚ùå</td>\n",
       "      <td>‚ùå</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I hereby give the unqualified right to the TGH program to take pictures and/ or video of me while participating in my course. I give permission for these images as well as materials that we have created in the TGH program to be used in print and web-based publications. The TGH program will put these images to legitimate use without limitation or reservation. If you do not wish for your photo to be used by TGH, please let your TGH instructors know so they will not send us pictures of you.</th>\n",
       "      <td>‚ùå</td>\n",
       "      <td>‚ùå</td>\n",
       "      <td>‚úÖ</td>\n",
       "      <td>‚ùå</td>\n",
       "      <td>‚ùå</td>\n",
       "      <td>‚ùå</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>409 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Learner+Enrollment+ENGLISH.csv  \\\n",
       "6. Race/Ethnicity                                                               ‚ùå   \n",
       "Sovgade fichye sou tabl√®t la epi jwenn yo annap...                              ‚ùå   \n",
       "What are your child's pronouns? (Ki pwonon piti...                              ‚ùå   \n",
       "14. Number of people in Household                                               ‚ùå   \n",
       "¬øTiene acceso a Internet en su hogar (por ejemp...                              ‚ùå   \n",
       "...                                                                           ...   \n",
       "√àske w gen aks√® ent√®n√®t (sa vle di wifi)? - Do ...                              ‚ùå   \n",
       "Were you born in the United States?                                             ‚úÖ   \n",
       "Guardar y buscar archivos en la tableta - Savin...                              ‚ùå   \n",
       "I am able to utilize my skills to move toward c...                              ‚úÖ   \n",
       "I hereby give the unqualified right to the TGH ...                              ‚ùå   \n",
       "\n",
       "                                                   Learner Enrollment HAITIAN CREOLE (3.8.23).csv  \\\n",
       "6. Race/Ethnicity                                                                               ‚ùå   \n",
       "Sovgade fichye sou tabl√®t la epi jwenn yo annap...                                              ‚úÖ   \n",
       "What are your child's pronouns? (Ki pwonon piti...                                              ‚úÖ   \n",
       "14. Number of people in Household                                                               ‚ùå   \n",
       "¬øTiene acceso a Internet en su hogar (por ejemp...                                              ‚ùå   \n",
       "...                                                                                           ...   \n",
       "√àske w gen aks√® ent√®n√®t (sa vle di wifi)? - Do ...                                              ‚úÖ   \n",
       "Were you born in the United States?                                                             ‚ùå   \n",
       "Guardar y buscar archivos en la tableta - Savin...                                              ‚ùå   \n",
       "I am able to utilize my skills to move toward c...                                              ‚ùå   \n",
       "I hereby give the unqualified right to the TGH ...                                              ‚ùå   \n",
       "\n",
       "                                                   (OLD) TGH Learner Enrollment Form (6.2.2022).csv  \\\n",
       "6. Race/Ethnicity                                                                                 ‚úÖ   \n",
       "Sovgade fichye sou tabl√®t la epi jwenn yo annap...                                                ‚ùå   \n",
       "What are your child's pronouns? (Ki pwonon piti...                                                ‚ùå   \n",
       "14. Number of people in Household                                                                 ‚úÖ   \n",
       "¬øTiene acceso a Internet en su hogar (por ejemp...                                                ‚ùå   \n",
       "...                                                                                             ...   \n",
       "√àske w gen aks√® ent√®n√®t (sa vle di wifi)? - Do ...                                                ‚ùå   \n",
       "Were you born in the United States?                                                               ‚ùå   \n",
       "Guardar y buscar archivos en la tableta - Savin...                                                ‚ùå   \n",
       "I am able to utilize my skills to move toward c...                                                ‚ùå   \n",
       "I hereby give the unqualified right to the TGH ...                                                ‚úÖ   \n",
       "\n",
       "                                                   (OLD)+TGH+Learner+Enrollment+Form+2025-02-26-02-50-39+457499.csv  \\\n",
       "6. Race/Ethnicity                                                                                   ‚ùå                 \n",
       "Sovgade fichye sou tabl√®t la epi jwenn yo annap...                                                  ‚ùå                 \n",
       "What are your child's pronouns? (Ki pwonon piti...                                                  ‚ùå                 \n",
       "14. Number of people in Household                                                                   ‚ùå                 \n",
       "¬øTiene acceso a Internet en su hogar (por ejemp...                                                  ‚ùå                 \n",
       "...                                                                                               ...                 \n",
       "√àske w gen aks√® ent√®n√®t (sa vle di wifi)? - Do ...                                                  ‚ùå                 \n",
       "Were you born in the United States?                                                                 ‚ùå                 \n",
       "Guardar y buscar archivos en la tableta - Savin...                                                  ‚ùå                 \n",
       "I am able to utilize my skills to move toward c...                                                  ‚ùå                 \n",
       "I hereby give the unqualified right to the TGH ...                                                  ‚ùå                 \n",
       "\n",
       "                                                   Learner Enrollment SPANISH (3.8.23).csv  \\\n",
       "6. Race/Ethnicity                                                                        ‚ùå   \n",
       "Sovgade fichye sou tabl√®t la epi jwenn yo annap...                                       ‚ùå   \n",
       "What are your child's pronouns? (Ki pwonon piti...                                       ‚ùå   \n",
       "14. Number of people in Household                                                        ‚ùå   \n",
       "¬øTiene acceso a Internet en su hogar (por ejemp...                                       ‚úÖ   \n",
       "...                                                                                    ...   \n",
       "√àske w gen aks√® ent√®n√®t (sa vle di wifi)? - Do ...                                       ‚ùå   \n",
       "Were you born in the United States?                                                      ‚ùå   \n",
       "Guardar y buscar archivos en la tableta - Savin...                                       ‚úÖ   \n",
       "I am able to utilize my skills to move toward c...                                       ‚ùå   \n",
       "I hereby give the unqualified right to the TGH ...                                       ‚ùå   \n",
       "\n",
       "                                                   Learner Enrollment CHINESE (3.8.23).csv  \n",
       "6. Race/Ethnicity                                                                        ‚ùå  \n",
       "Sovgade fichye sou tabl√®t la epi jwenn yo annap...                                       ‚ùå  \n",
       "What are your child's pronouns? (Ki pwonon piti...                                       ‚ùå  \n",
       "14. Number of people in Household                                                        ‚ùå  \n",
       "¬øTiene acceso a Internet en su hogar (por ejemp...                                       ‚ùå  \n",
       "...                                                                                    ...  \n",
       "√àske w gen aks√® ent√®n√®t (sa vle di wifi)? - Do ...                                       ‚ùå  \n",
       "Were you born in the United States?                                                      ‚ùå  \n",
       "Guardar y buscar archivos en la tableta - Savin...                                       ‚ùå  \n",
       "I am able to utilize my skills to move toward c...                                       ‚ùå  \n",
       "I hereby give the unqualified right to the TGH ...                                       ‚ùå  \n",
       "\n",
       "[409 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# üîπ Path to the folder where all CSV files are stored\n",
    "folder_path = \"/Users/sreeharsha/Documents/TGH Data Management Cleaning/Enrollment Survey\"  # Update this if files are in a different location\n",
    "\n",
    "# üîπ List all CSV files in the folder\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith(\".csv\")]\n",
    "\n",
    "# üîπ Dictionary to store columns for each file\n",
    "columns_dict = {}\n",
    "\n",
    "# üîπ Read and collect column names from each file\n",
    "for file_name in csv_files:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "    try:\n",
    "        # Load CSV (only first row for faster processing)\n",
    "        df = pd.read_csv(file_path, nrows=1, low_memory=False)\n",
    "\n",
    "        # Store column names\n",
    "        columns_dict[file_name] = set(df.columns)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error reading {file_name}: {e}\")\n",
    "\n",
    "# üîπ Get all unique columns across all files (Convert to List)\n",
    "all_columns = list(set.union(*columns_dict.values()))\n",
    "\n",
    "# üîπ Create a DataFrame to show which files contain which columns\n",
    "column_comparison = pd.DataFrame(index=all_columns, columns=csv_files)\n",
    "\n",
    "# üîπ Populate DataFrame: '‚úÖ' if column exists, '‚ùå' if missing\n",
    "for file, columns in columns_dict.items():\n",
    "    column_comparison[file] = [\"‚úÖ\" if col in columns else \"‚ùå\" for col in all_columns]\n",
    "\n",
    "# üîπ Display the DataFrame\n",
    "from IPython.display import display\n",
    "\n",
    "print(\"\\nüìä Column Differences Across Files:\")\n",
    "display(column_comparison)  # Use display() in Jupyter Notebook to properly format the table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1025bb3-072a-48bb-a9cf-736e0b23c7d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f78eef5-3a66-44cf-97d1-cf1cd4afcada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Please select the files (CSV or XLSM) you want to summarize...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-17 07:42:26.977 python[4544:1294437] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-03-17 07:42:27.687 python[4544:1294437] The class 'NSOpenPanel' overrides the method identifier.  This method is implemented by class 'NSWindow'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ You selected 6 files:\n",
      "\n",
      "- (OLD) TGH Learner Enrollment Form (6.2.2022).csv\n",
      "- (OLD)+TGH+Learner+Enrollment+Form+2025-03-06-11-24-39+457499.csv.csv\n",
      "- Cleaned_ES_translated_chinese.csv\n",
      "- Learner+Enrollment+ENGLISH.csv\n",
      "- translated_haitian_creole.xlsm\n",
      "- translated_spanish.xlsm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing selected files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Column Summary saved: /Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/Merge testing/Enrollment Survey/column_summary_selected_files.csv\n",
      "\n",
      "üéâ Analysis complete! Check the CSV file for your summary.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Sumarizing the updated Columns names\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from tkinter import Tk, filedialog\n",
    "\n",
    "# Step 1Ô∏è‚É£: OPEN FILE DIALOG to SELECT Multiple Files (CSV + XLSM)\n",
    "\n",
    "print(\"üìÇ Please select the files (CSV or XLSM) you want to summarize...\")\n",
    "\n",
    "root = Tk()\n",
    "root.withdraw()  # Hide the main window\n",
    "file_paths = filedialog.askopenfilenames(\n",
    "    title=\"Select Enrollment Survey Files\",\n",
    "    filetypes=[(\"CSV and XLSM Files\", \"*.csv *.xlsm\")]\n",
    ")\n",
    "\n",
    "file_paths = list(file_paths)\n",
    "\n",
    "if not file_paths:\n",
    "    print(\"‚ö†Ô∏è No files selected. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"\\n‚úÖ You selected {len(file_paths)} files:\\n\")\n",
    "for f in file_paths:\n",
    "    print(f\"- {os.path.basename(f)}\")\n",
    "\n",
    "# Step 2Ô∏è‚É£: Dictionary to store column statistics\n",
    "column_stats = defaultdict(lambda: {\"file_count\": 0, \"total_rows\": 0, \"non_empty_count\": 0})\n",
    "\n",
    "# Step 3Ô∏è‚É£: Process each selected file\n",
    "for file in tqdm(file_paths, desc=\"Processing selected files\"):\n",
    "    try:\n",
    "        if file.endswith(\".csv\"):\n",
    "            df = pd.read_csv(file, encoding=\"utf-8\", dtype=str)  # Read CSV\n",
    "        elif file.endswith(\".xlsm\"):\n",
    "            df = pd.read_excel(file, dtype=str)  # Read XLSM\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Unsupported file type: {file}\")\n",
    "            continue\n",
    "        \n",
    "        total_rows = len(df)\n",
    "\n",
    "        for col in df.columns:\n",
    "            non_empty_count = df[col].notna().sum()  # Count non-empty values\n",
    "            \n",
    "            # Update column statistics\n",
    "            column_stats[col][\"file_count\"] += 1\n",
    "            column_stats[col][\"total_rows\"] += total_rows\n",
    "            column_stats[col][\"non_empty_count\"] += non_empty_count\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error processing {file}: {e}\")\n",
    "\n",
    "# Step 4Ô∏è‚É£: Convert dictionary to DataFrame\n",
    "column_summary = pd.DataFrame.from_dict(column_stats, orient=\"index\").reset_index()\n",
    "column_summary.columns = [\"Column Name\", \"Files Appeared In\", \"Total Rows Across Files\", \"Total Non-Empty Values\"]\n",
    "\n",
    "# Sort for clarity\n",
    "column_summary.sort_values(by=[\"Files Appeared In\", \"Total Non-Empty Values\"], ascending=[False, False], inplace=True)\n",
    "\n",
    "# Step 5Ô∏è‚É£: Save summary in same folder\n",
    "summary_save_path = os.path.join(os.path.dirname(file_paths[0]), \"column_summary_selected_files.csv\")\n",
    "column_summary.to_csv(summary_save_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"\\n‚úÖ Column Summary saved: {summary_save_path}\")\n",
    "print(\"\\nüéâ Analysis complete! Check the CSV file for your summary.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef95404e-37ba-4015-ba38-3f465ee5a519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a0b43e2-f810-4cef-8d47-02095e60c810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Please select the files (CSV or XLSM) you want to summarize...\n",
      "\n",
      "‚úÖ You selected 6 files:\n",
      "\n",
      "- translated_spanish.xlsm\n",
      "- translated_haitian_creole.xlsm\n",
      "- (OLD) TGH Learner Enrollment Form (6.2.2022).csv\n",
      "- (OLD)+TGH+Learner+Enrollment+Form+2025-03-06-11-24-39+457499.csv.csv\n",
      "- Cleaned_ES_translated_chinese.csv\n",
      "- Learner+Enrollment+ENGLISH.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing selected files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  3.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Column Summary saved: /Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/Merge testing/Enrollment Survey/column_summary_with_files.csv\n",
      "\n",
      "üéâ Analysis complete! You can now see which columns appear in which files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Sumarizing the updated Columns names with file names\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from tkinter import Tk, filedialog\n",
    "\n",
    "# Step 1Ô∏è‚É£: FILE PICKER for CSV & XLSM files\n",
    "print(\"üìÇ Please select the files (CSV or XLSM) you want to summarize...\")\n",
    "\n",
    "root = Tk()\n",
    "root.withdraw()\n",
    "file_paths = filedialog.askopenfilenames(\n",
    "    title=\"Select Survey Files\",\n",
    "    filetypes=[(\"CSV and XLSM Files\", \"*.csv *.xlsm\")]\n",
    ")\n",
    "\n",
    "file_paths = list(file_paths)\n",
    "\n",
    "if not file_paths:\n",
    "    print(\"‚ö†Ô∏è No files selected. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"\\n‚úÖ You selected {len(file_paths)} files:\\n\")\n",
    "for f in file_paths:\n",
    "    print(f\"- {os.path.basename(f)}\")\n",
    "\n",
    "# Step 2Ô∏è‚É£: Dictionary to store column stats\n",
    "column_stats = defaultdict(lambda: {\n",
    "    \"file_count\": 0,\n",
    "    \"total_rows\": 0,\n",
    "    \"non_empty_count\": 0,\n",
    "    \"files_present\": set()  # <--- Track file names!\n",
    "})\n",
    "\n",
    "# Step 3Ô∏è‚É£: Process each file\n",
    "for file in tqdm(file_paths, desc=\"Processing selected files\"):\n",
    "    try:\n",
    "        if file.endswith(\".csv\"):\n",
    "            df = pd.read_csv(file, encoding=\"utf-8\", dtype=str)\n",
    "        elif file.endswith(\".xlsm\"):\n",
    "            df = pd.read_excel(file, dtype=str)\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Unsupported file type: {file}\")\n",
    "            continue\n",
    "        \n",
    "        total_rows = len(df)\n",
    "        file_name = os.path.basename(file)\n",
    "\n",
    "        for col in df.columns:\n",
    "            non_empty_count = df[col].notna().sum()\n",
    "\n",
    "            # Update stats\n",
    "            column_stats[col][\"file_count\"] += 1\n",
    "            column_stats[col][\"total_rows\"] += total_rows\n",
    "            column_stats[col][\"non_empty_count\"] += non_empty_count\n",
    "            column_stats[col][\"files_present\"].add(file_name)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error processing {file}: {e}\")\n",
    "\n",
    "# Step 4Ô∏è‚É£: Convert stats to DataFrame\n",
    "summary_data = []\n",
    "for col, stats in column_stats.items():\n",
    "    summary_data.append({\n",
    "        \"Column Name\": col,\n",
    "        \"Files Appeared In\": stats[\"file_count\"],\n",
    "        \"Total Rows Across Files\": stats[\"total_rows\"],\n",
    "        \"Total Non-Empty Values\": stats[\"non_empty_count\"],\n",
    "        \"Files Where Column Appears\": \", \".join(sorted(stats[\"files_present\"]))\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# Sort nicely\n",
    "summary_df.sort_values(by=[\"Files Appeared In\", \"Total Non-Empty Values\"], ascending=[False, False], inplace=True)\n",
    "\n",
    "# Step 5Ô∏è‚É£: Save summary\n",
    "summary_save_path = os.path.join(os.path.dirname(file_paths[0]), \"column_summary_with_files.csv\")\n",
    "summary_df.to_csv(summary_save_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"\\n‚úÖ Column Summary saved: {summary_save_path}\")\n",
    "print(\"\\nüéâ Analysis complete! You can now see which columns appear in which files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c09b15b7-3d44-41ad-9cf4-8c3b6e2ad77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Please select the files (CSV or XLSM) you want to summarize...\n",
      "\n",
      "‚úÖ You selected 6 files:\n",
      "\n",
      "- (OLD) TGH Learner Enrollment Form (6.2.2022).csv\n",
      "- (OLD)+TGH+Learner+Enrollment+Form+2025-03-06-11-24-39+457499.csv.csv\n",
      "- Cleaned_ES_translated_chinese.csv\n",
      "- Learner+Enrollment+ENGLISH.csv\n",
      "- translated_haitian_creole.xlsm\n",
      "- translated_spanish.xlsm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing selected files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  3.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Column Summary saved: /Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/Merge testing/Enrollment Survey/column_summary_sorted.csv\n",
      "\n",
      "üéâ Columns are now sorted by most common appearance AND natural position!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# sorting the column names\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from tkinter import Tk, filedialog\n",
    "\n",
    "print(\"üìÇ Please select the files (CSV or XLSM) you want to summarize...\")\n",
    "\n",
    "root = Tk()\n",
    "root.withdraw()\n",
    "file_paths = filedialog.askopenfilenames(\n",
    "    title=\"Select Survey Files\",\n",
    "    filetypes=[(\"CSV and XLSM Files\", \"*.csv *.xlsm\")]\n",
    ")\n",
    "\n",
    "file_paths = list(file_paths)\n",
    "\n",
    "if not file_paths:\n",
    "    print(\"‚ö†Ô∏è No files selected. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"\\n‚úÖ You selected {len(file_paths)} files:\\n\")\n",
    "for f in file_paths:\n",
    "    print(f\"- {os.path.basename(f)}\")\n",
    "\n",
    "# --- Stats dictionary\n",
    "column_stats = defaultdict(lambda: {\n",
    "    \"file_count\": 0,\n",
    "    \"total_rows\": 0,\n",
    "    \"non_empty_count\": 0,\n",
    "    \"files_present\": set(),\n",
    "    \"positions\": []  # <--- Track column index positions\n",
    "})\n",
    "\n",
    "# --- Process files\n",
    "for file in tqdm(file_paths, desc=\"Processing selected files\"):\n",
    "    try:\n",
    "        if file.endswith(\".csv\"):\n",
    "            df = pd.read_csv(file, encoding=\"utf-8\", dtype=str)\n",
    "        elif file.endswith(\".xlsm\"):\n",
    "            df = pd.read_excel(file, dtype=str)\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Unsupported file type: {file}\")\n",
    "            continue\n",
    "        \n",
    "        total_rows = len(df)\n",
    "        file_name = os.path.basename(file)\n",
    "\n",
    "        for idx, col in enumerate(df.columns):\n",
    "            non_empty_count = df[col].notna().sum()\n",
    "\n",
    "            column_stats[col][\"file_count\"] += 1\n",
    "            column_stats[col][\"total_rows\"] += total_rows\n",
    "            column_stats[col][\"non_empty_count\"] += non_empty_count\n",
    "            column_stats[col][\"files_present\"].add(file_name)\n",
    "            column_stats[col][\"positions\"].append(idx)  # Save index position\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error processing {file}: {e}\")\n",
    "\n",
    "# --- Create summary dataframe\n",
    "summary_data = []\n",
    "for col, stats in column_stats.items():\n",
    "    avg_position = sum(stats[\"positions\"]) / len(stats[\"positions\"])  # Avg index\n",
    "    summary_data.append({\n",
    "        \"Column Name\": col,\n",
    "        \"Files Appeared In\": stats[\"file_count\"],\n",
    "        \"Total Rows Across Files\": stats[\"total_rows\"],\n",
    "        \"Total Non-Empty Values\": stats[\"non_empty_count\"],\n",
    "        \"Files Where Column Appears\": \", \".join(sorted(stats[\"files_present\"])),\n",
    "        \"Average Position\": avg_position\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# --- Sort: First by number of files, then average position\n",
    "summary_df.sort_values(by=[\"Files Appeared In\", \"Average Position\"], ascending=[False, True], inplace=True)\n",
    "\n",
    "# --- Save summary\n",
    "summary_save_path = os.path.join(os.path.dirname(file_paths[0]), \"column_summary_sorted.csv\")\n",
    "summary_df.to_csv(summary_save_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"\\n‚úÖ Column Summary saved: {summary_save_path}\")\n",
    "print(\"\\nüéâ Columns are now sorted by most common appearance AND natural position!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac10d6b-1abf-43a0-bdcd-60f48de028b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5a43512-74a4-441a-bae5-5499ecfd7e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original columns:\n",
      "['Submitted Date', 'Course Name', 'Course Type', 'First Name', 'Middle Initial', 'Last Name', 'Date of Birth', 'Email Address', 'Please describe your confidence level using the internet to find information you need:', 'Creating and sending emails', 'Opening and replying to emails', 'Downloading attachments I receive in emails (like documents or pictures)', 'Adding an attachment to an email I am sending', 'Turning the computer on and off', \"Adjusting the computer's settings (like the size of the text or the background picture)\", 'Connecting the computer to wifi', 'Installing applications from the Chromebook store', 'Saving files to the computer and finding them later', 'Using a mouse, headset and/or other accessories', 'Turning the tablet on and off', \"Adjusting the tablet's settings (like the size of the text or the background picture)\", 'Connecting the tablet to wifi', 'Installing applications (Apps)', 'Saving files to the tablet and finding them later', 'Finding/Researching general information online (e.g. Google)', 'Finding housing (e.g. shelter, apartment hunting/rentals, sublets)', 'Accessing health and wellness information and resources (e.g. telehealth, health portals, managing insurance online)', 'Managing finances online (e.g. create budgets, online banking, paying bills)', 'Accessing local/city resources (e.g. food assistance)', 'Civic Engagement (e.g. access voting information, filling out the census, attending virtual town/city meetings)', 'Education for myself (e.g. schoolwork, applying to school)', 'Helping my child with school', 'Using job resources (e.g. Indeed, LinkedIn, resume building, applying for jobs online)', 'Using digital skills for work/current job', 'Data Entry (e.g. Google Sheets, Microsoft Excel)', 'Word Processing and/or presentation skills (e.g. Google Docs, Google slides)', 'Communicating with others by email', 'Using social media (e.g. Facebook, Instagram, Twitter)', 'Video chat and or other visual communication tools (e.g. Zoom, Skype, Google Hangout/Chat)', 'What was the best part of your TGH course? Please describe your experience.', 'Do you plan to participate in future activities at the site where you took TGH?', 'I learned skills during my TGH courses that can help me improve my life.', 'How do you think your life will improve?', 'Please share why you selected strongly disagree or disagree for the statement above.', 'What other digital skill(s) would you like to learn?', 'Overall, how satisfied were you with your TGH experience?', 'How satisfied were you with your TGH instructor(s)?', 'What would you tell a friend about the TGH Course?', 'Anything else you would like to add about your TGH course or instructor?', 'Internet Access', 'Why did you request a hotspot instead of home internet service through Comcast Internet Essentials?', 'Did you request a TGH-sponsored Comcast Internet Essentials Promo code?', 'Was Comcast Internet Essentials successfully set up in your home?', 'How could your internet setup experience have been improved?', \"Why wasn't internet service set up in your home?\", '53. Are you aware of the Affordable Connectivity Program (ACP), an FCC (Federal Communications Commission) program to help families and households afford internet service? (More info here: https://www.fcc.gov/acp)', '54. Are you enrolled in the ACP Program?', 'I feel the course content was presented for:', 'Do you do educational activities with your child more often than you did before taking TGH?', 'I am aware of what my skills are to be employed in a good job (e.g., digital skills, specific trade skills, etc.).', 'I am aware of what my resources are to be employed in a good job (e.g., resources for job searching, child care resources, etc.).', 'I am able to utilize my skills to move toward career goals.', 'I am able to utilize my resources to move toward career goals', 'Even if I am not able to achieve my financial goals right away, I will find a way to get there.', 'Because of my money situation, I feel like I will never have the things I want in life.', 'I am just getting by financially.', 'I am concerned that the money I have or will save won‚Äôt last.', 'Since participating in the Tech Goes Home, have you (check all that apply):', 'Did TGH make it more likely that you will use the Internet for any of the following (select all that apply)?', 'If you selected \"Other\", please explain:', 'Did TGH help you create a website for your business or entrepreneurial project?', 'What is the web address (URL) of the website?', 'Did TGH help you use social media for your business or entrepreneurial project?', 'Please share any/all of your social media handles separated by a comma(,) (e.g. @techgoeshome, facebook.com/yourbusinessname)', 'Course ID', 'Form Name', 'Creation Date', 'Modified Date', 'Completion Time', 'Response Html Beta', 'Response Html', 'Response Text', 'Response', 'Response Url', 'Resume Email', 'File List', 'Unprotected File List', 'Response Id', 'Referrer', 'Ip Address']\n",
      "Standardized columns:\n",
      "['submitted_date', 'course_name', 'course_type', 'first_name', 'middle_initial', 'last_name', 'date_of_birth', 'email_address', 'please_describe_your_confidence_level_using_the_internet_to_find_information_you_need', 'creating_and_sending_emails', 'opening_and_replying_to_emails', 'downloading_attachments_i_receive_in_emails_like_documents_or_pictures', 'adding_an_attachment_to_an_email_i_am_sending', 'turning_the_computer_on_and_off', 'adjusting_the_computers_settings_like_the_size_of_the_text_or_the_background_picture', 'connecting_the_computer_to_wifi', 'installing_applications_from_the_chromebook_store', 'saving_files_to_the_computer_and_finding_them_later', 'using_a_mouse_headset_andor_other_accessories', 'turning_the_tablet_on_and_off', 'adjusting_the_tablets_settings_like_the_size_of_the_text_or_the_background_picture', 'connecting_the_tablet_to_wifi', 'installing_applications_apps', 'saving_files_to_the_tablet_and_finding_them_later', 'findingresearching_general_information_online_eg_google', 'finding_housing_eg_shelter_apartment_huntingrentals_sublets', 'accessing_health_and_wellness_information_and_resources_eg_telehealth_health_portals_managing_insurance_online', 'managing_finances_online_eg_create_budgets_online_banking_paying_bills', 'accessing_localcity_resources_eg_food_assistance', 'civic_engagement_eg_access_voting_information_filling_out_the_census_attending_virtual_towncity_meetings', 'education_for_myself_eg_schoolwork_applying_to_school', 'helping_my_child_with_school', 'using_job_resources_eg_indeed_linkedin_resume_building_applying_for_jobs_online', 'using_digital_skills_for_workcurrent_job', 'data_entry_eg_google_sheets_microsoft_excel', 'word_processing_andor_presentation_skills_eg_google_docs_google_slides', 'communicating_with_others_by_email', 'using_social_media_eg_facebook_instagram_twitter', 'video_chat_and_or_other_visual_communication_tools_eg_zoom_skype_google_hangoutchat', 'what_was_the_best_part_of_your_tgh_course_please_describe_your_experience', 'do_you_plan_to_participate_in_future_activities_at_the_site_where_you_took_tgh', 'i_learned_skills_during_my_tgh_courses_that_can_help_me_improve_my_life', 'how_do_you_think_your_life_will_improve', 'please_share_why_you_selected_strongly_disagree_or_disagree_for_the_statement_above', 'what_other_digital_skills_would_you_like_to_learn', 'overall_how_satisfied_were_you_with_your_tgh_experience', 'how_satisfied_were_you_with_your_tgh_instructors', 'what_would_you_tell_a_friend_about_the_tgh_course', 'anything_else_you_would_like_to_add_about_your_tgh_course_or_instructor', 'internet_access', 'why_did_you_request_a_hotspot_instead_of_home_internet_service_through_comcast_internet_essentials', 'did_you_request_a_tghsponsored_comcast_internet_essentials_promo_code', 'was_comcast_internet_essentials_successfully_set_up_in_your_home', 'how_could_your_internet_setup_experience_have_been_improved', 'why_wasnt_internet_service_set_up_in_your_home', '53_are_you_aware_of_the_affordable_connectivity_program_acp_an_fcc_federal_communications_commission_program_to_help_families_and_households_afford_internet_service_more_info_here_httpswwwfccgovacp', '54_are_you_enrolled_in_the_acp_program', 'i_feel_the_course_content_was_presented_for', 'do_you_do_educational_activities_with_your_child_more_often_than_you_did_before_taking_tgh', 'i_am_aware_of_what_my_skills_are_to_be_employed_in_a_good_job_eg_digital_skills_specific_trade_skills_etc', 'i_am_aware_of_what_my_resources_are_to_be_employed_in_a_good_job_eg_resources_for_job_searching_child_care_resources_etc', 'i_am_able_to_utilize_my_skills_to_move_toward_career_goals', 'i_am_able_to_utilize_my_resources_to_move_toward_career_goals', 'even_if_i_am_not_able_to_achieve_my_financial_goals_right_away_i_will_find_a_way_to_get_there', 'because_of_my_money_situation_i_feel_like_i_will_never_have_the_things_i_want_in_life', 'i_am_just_getting_by_financially', 'i_am_concerned_that_the_money_i_have_or_will_save_wont_last', 'since_participating_in_the_tech_goes_home_have_you_check_all_that_apply', 'did_tgh_make_it_more_likely_that_you_will_use_the_internet_for_any_of_the_following_select_all_that_apply', 'if_you_selected_other_please_explain', 'did_tgh_help_you_create_a_website_for_your_business_or_entrepreneurial_project', 'what_is_the_web_address_url_of_the_website', 'did_tgh_help_you_use_social_media_for_your_business_or_entrepreneurial_project', 'please_share_anyall_of_your_social_media_handles_separated_by_a_comma_eg_techgoeshome_facebookcomyourbusinessname', 'course_id', 'form_name', 'creation_date', 'modified_date', 'completion_time', 'response_html_beta', 'response_html', 'response_text', 'response', 'response_url', 'resume_email', 'file_list', 'unprotected_file_list', 'response_id', 'referrer', 'ip_address']\n",
      "Standardized file saved to: /Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/Translated PCS/PCS_English_standardized.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def standardize_column(col):\n",
    "    \"\"\"\n",
    "    Standardizes a column name by:\n",
    "      - Removing leading/trailing whitespace\n",
    "      - Converting to lowercase\n",
    "      - Normalizing accented characters\n",
    "      - Removing special characters (punctuation, etc.)\n",
    "      - Replacing internal spaces with a single underscore\n",
    "      - Removing any underscores at the beginning or end\n",
    "    \"\"\"\n",
    "    # Trim extra spaces and convert to lowercase\n",
    "    col = col.strip().lower()\n",
    "    # Normalize accented characters (e.g., √© -> e)\n",
    "    col = unicodedata.normalize('NFKD', col).encode('ascii', 'ignore').decode('utf-8')\n",
    "    # Remove punctuation and special characters (keeping alphanumerics and whitespace)\n",
    "    col = re.sub(r'[^\\w\\s]', '', col)\n",
    "    # Replace one or more whitespace with a single underscore\n",
    "    col = re.sub(r'\\s+', '_', col)\n",
    "    # Ensure no leading or trailing underscores remain\n",
    "    col = col.strip('_')\n",
    "    return col\n",
    "\n",
    "# Update the file path with your file to standardize\n",
    "file_path = '/Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/Translated PCS/(9.1.22)+ENGLISH+TGH+Learner+Post-Course+Survey+2025-03-22-02-35-44+4939458.csv'  # <-- Update this path\n",
    "\n",
    "# Load the file (assuming CSV; use pd.read_excel for xlsm files)\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display original columns\n",
    "print(\"Original columns:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Apply standardization to each column name\n",
    "new_columns = [standardize_column(col) for col in df.columns]\n",
    "df.columns = new_columns\n",
    "\n",
    "# Display standardized columns\n",
    "print(\"Standardized columns:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Save the updated DataFrame to a new file (update the output path as needed)\n",
    "output_path = '/Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/Translated PCS/PCS_English_standardized.csv'  # <-- Update this path\n",
    "df.to_csv(output_path, index=False)\n",
    "print(\"Standardized file saved to:\", output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd667158-e478-4cde-8331-bc3c2d6eecde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e016c6-2bd4-41d3-bdd8-8f781ea0078c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed77bfa7-9e79-4020-9ad0-97fd248cef17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original columns:\n",
      "['Submitted Date', 'First Name', 'Last Name', 'Email Address', 'Year', 'To get access to a computer/iPad', 'To get access to the Internet', 'To learn Internet skills', 'To gain employment', 'To get a better job', 'To go to school (Ex. college, GED, job/vocational training)', 'To help my child with school', 'To access information about my local neighborhood and/or city', 'To more easily connect with family and friends', 'To improve my English', 'Other', 'Please explain the reason you chose other.', 'Job searching resources (Ex. Indeed, LinkedIn, resume building)', 'Work purposes (for your current job)', 'Financial management (Ex. creating budgets, online banking, paying bills)', 'Educational purposes (Ex. schoolwork, applying to school)', 'City resources (Ex. City of Boston, Find It Cambridge)', 'Health and wellness', 'Finding general information', 'Communicating with others (Ex. email, video chat)', 'Social networking (Ex. Facebook)', 'Other.1', 'Please explain the reason you chose other..1', 'Typing', 'Word processing and/or presentation skills (Google Docs, Google Slides)', 'Research and/or finding resources', 'Managing personal finances (Ex. creating budgets, online banking, paying bills)', 'Accessing health resources (Ex. telehealth, health portals, managing insurance online)', 'Completing online job applications', 'Data entry', 'Helping my child with school', 'Other.2', 'Please explain the reason you chose other..2', 'What have you found to be the most important skill you learned during the program and how has it helped you further your personal goals?', 'Reading, listening to, and/or watching the news', 'Working from home', 'Continuing my education online', 'Helping my child with school.1', 'Accessing information about the virus (Ex. symptoms, how to protect myself, who is most vulnerable)', 'Accessing health resources (Ex. testing sites, treatment and care options)', 'Researching government policies (Ex. stay-at-home guidelines, reopening plans)', 'Getting life essentials online (Ex. medicine, food, cleaning supplies, masks)', 'Financial assistance (Ex. filing for unemployment, grant applications, government aid programs)', 'Telehealth (Ex. communicating with a health professional online)', 'Other.3', 'x', 'Did you make new friends or connections during the TGH course?', 'Since the end of your class, have you participated in any other activities organized by your TGH course site?', 'Since the end of your class, have you participated in an additional computer, Internet, or language class in person or online?', 'What was your employment status BEFORE participating in the TGH program?', 'What was your employment status BEFORE the outbreak of coronavirus in the United States (before March 13, 2020)?', 'Since participating in the TGH program, have you been able to:', 'Do you feel that the skills you learned or the connections you made during the TGH course helped you achieve the selection above?', 'What is your current employment status?', 'Has COVID-19 affected your employment status?', 'Work from home', 'Communicate with my employer/employees/coworkers', 'Job search and apply for jobs online', 'Continue with my education and/or job training', 'Get unemployment benefits online', 'Other.4', 'Please explain the reason you chose other..3', 'Did you take the TGH course focused on building small business skills like marketing, website design, and business planning? (ex. TGH Small Business)', 'Have you opened a small business since taking the TGH Course?', 'Are the skills you learned in the TGH course helping your business grow? Is your business earning more money or have more customers since the end of the TGH course?', 'What is the most valuable skill you learned during the course for your small business or future small business?', 'Did you take TGH with a child?', 'How often does your child use their TGH iPad/computer for learning?', 'Recognizing letters', 'Recognizing numbers, counting', 'Problem solving', 'Social skills', 'Creativity (Ex. art, storytelling)', 'Vocabulary (Ex. communicating wants and needs)', 'Internet safety and awareness', 'Parenting with screen time', 'Other.5', 'Please explain the reason you chose other..4', 'Typing.1', 'Word processing and/or presentation skills (Ex. Google Docs, Google Slides)', 'Research and/or finding resources.1', 'Doing school activities and homework online', 'Internet safety and privacy awareness', 'Checking grades and/or communicating with teacher online', 'Other.6', 'Please explain the reason you chose other..5', \"Has your child's academic performance changed because of the TGH School course?\", \"How has your involvement in your child's education changed because of TGH?\", \"As a caregiver, do you feel more confident about your child's safety on the Internet after the TGH course?\", 'Do you have quality Internet that you can afford?', 'Is the Internet service you pay for Comcast Internet Essentials for $9.99/month?', 'Is there anything else you would like to add about your experience with Tech Goes Home?', 'Form Name', 'Creation Date', 'Modified Date', 'Completion Time', 'Response Html Beta', 'Response Html', 'Response Text', 'Response', 'Response Url', 'Resume Email', 'File List', 'Unprotected File List', 'Response Id', 'Referrer', 'Ip Address']\n",
      "Standardized columns:\n",
      "['submitted_date', 'first_name', 'last_name', 'email_address', 'year', 'to_get_access_to_a_computeripad', 'to_get_access_to_the_internet', 'to_learn_internet_skills', 'to_gain_employment', 'to_get_a_better_job', 'to_go_to_school_ex_college_ged_jobvocational_training', 'to_help_my_child_with_school', 'to_access_information_about_my_local_neighborhood_andor_city', 'to_more_easily_connect_with_family_and_friends', 'to_improve_my_english', 'other', 'please_explain_the_reason_you_chose_other', 'job_searching_resources_ex_indeed_linkedin_resume_building', 'work_purposes_for_your_current_job', 'financial_management_ex_creating_budgets_online_banking_paying_bills', 'educational_purposes_ex_schoolwork_applying_to_school', 'city_resources_ex_city_of_boston_find_it_cambridge', 'health_and_wellness', 'finding_general_information', 'communicating_with_others_ex_email_video_chat', 'social_networking_ex_facebook', 'other1', 'please_explain_the_reason_you_chose_other1', 'typing', 'word_processing_andor_presentation_skills_google_docs_google_slides', 'research_andor_finding_resources', 'managing_personal_finances_ex_creating_budgets_online_banking_paying_bills', 'accessing_health_resources_ex_telehealth_health_portals_managing_insurance_online', 'completing_online_job_applications', 'data_entry', 'helping_my_child_with_school', 'other2', 'please_explain_the_reason_you_chose_other2', 'what_have_you_found_to_be_the_most_important_skill_you_learned_during_the_program_and_how_has_it_helped_you_further_your_personal_goals', 'reading_listening_to_andor_watching_the_news', 'working_from_home', 'continuing_my_education_online', 'helping_my_child_with_school1', 'accessing_information_about_the_virus_ex_symptoms_how_to_protect_myself_who_is_most_vulnerable', 'accessing_health_resources_ex_testing_sites_treatment_and_care_options', 'researching_government_policies_ex_stayathome_guidelines_reopening_plans', 'getting_life_essentials_online_ex_medicine_food_cleaning_supplies_masks', 'financial_assistance_ex_filing_for_unemployment_grant_applications_government_aid_programs', 'telehealth_ex_communicating_with_a_health_professional_online', 'other3', 'x', 'did_you_make_new_friends_or_connections_during_the_tgh_course', 'since_the_end_of_your_class_have_you_participated_in_any_other_activities_organized_by_your_tgh_course_site', 'since_the_end_of_your_class_have_you_participated_in_an_additional_computer_internet_or_language_class_in_person_or_online', 'what_was_your_employment_status_before_participating_in_the_tgh_program', 'what_was_your_employment_status_before_the_outbreak_of_coronavirus_in_the_united_states_before_march_13_2020', 'since_participating_in_the_tgh_program_have_you_been_able_to', 'do_you_feel_that_the_skills_you_learned_or_the_connections_you_made_during_the_tgh_course_helped_you_achieve_the_selection_above', 'what_is_your_current_employment_status', 'has_covid19_affected_your_employment_status', 'work_from_home', 'communicate_with_my_employeremployeescoworkers', 'job_search_and_apply_for_jobs_online', 'continue_with_my_education_andor_job_training', 'get_unemployment_benefits_online', 'other4', 'please_explain_the_reason_you_chose_other3', 'did_you_take_the_tgh_course_focused_on_building_small_business_skills_like_marketing_website_design_and_business_planning_ex_tgh_small_business', 'have_you_opened_a_small_business_since_taking_the_tgh_course', 'are_the_skills_you_learned_in_the_tgh_course_helping_your_business_grow_is_your_business_earning_more_money_or_have_more_customers_since_the_end_of_the_tgh_course', 'what_is_the_most_valuable_skill_you_learned_during_the_course_for_your_small_business_or_future_small_business', 'did_you_take_tgh_with_a_child', 'how_often_does_your_child_use_their_tgh_ipadcomputer_for_learning', 'recognizing_letters', 'recognizing_numbers_counting', 'problem_solving', 'social_skills', 'creativity_ex_art_storytelling', 'vocabulary_ex_communicating_wants_and_needs', 'internet_safety_and_awareness', 'parenting_with_screen_time', 'other5', 'please_explain_the_reason_you_chose_other4', 'typing1', 'word_processing_andor_presentation_skills_ex_google_docs_google_slides', 'research_andor_finding_resources1', 'doing_school_activities_and_homework_online', 'internet_safety_and_privacy_awareness', 'checking_grades_andor_communicating_with_teacher_online', 'other6', 'please_explain_the_reason_you_chose_other5', 'has_your_childs_academic_performance_changed_because_of_the_tgh_school_course', 'how_has_your_involvement_in_your_childs_education_changed_because_of_tgh', 'as_a_caregiver_do_you_feel_more_confident_about_your_childs_safety_on_the_internet_after_the_tgh_course', 'do_you_have_quality_internet_that_you_can_afford', 'is_the_internet_service_you_pay_for_comcast_internet_essentials_for_999month', 'is_there_anything_else_you_would_like_to_add_about_your_experience_with_tech_goes_home', 'form_name', 'creation_date', 'modified_date', 'completion_time', 'response_html_beta', 'response_html', 'response_text', 'response', 'response_url', 'resume_email', 'file_list', 'unprotected_file_list', 'response_id', 'referrer', 'ip_address']\n",
      "Standardized file saved to: /Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/Translated AS/2020_English_AS_standardized.csv\n"
     ]
    }
   ],
   "source": [
    "#Standerdizing annual survey columns\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def standardize_column(col):\n",
    "    \"\"\"\n",
    "    Standardizes a column name by:\n",
    "      - Removing leading/trailing whitespace\n",
    "      - Converting to lowercase\n",
    "      - Normalizing accented characters\n",
    "      - Removing special characters (punctuation, etc.)\n",
    "      - Replacing internal spaces with a single underscore\n",
    "      - Removing any underscores at the beginning or end\n",
    "    \"\"\"\n",
    "    # Trim extra spaces and convert to lowercase\n",
    "    col = col.strip().lower()\n",
    "    # Normalize accented characters (e.g., √© -> e)\n",
    "    col = unicodedata.normalize('NFKD', col).encode('ascii', 'ignore').decode('utf-8')\n",
    "    # Remove punctuation and special characters (keeping alphanumerics and whitespace)\n",
    "    col = re.sub(r'[^\\w\\s]', '', col)\n",
    "    # Replace one or more whitespace with a single underscore\n",
    "    col = re.sub(r'\\s+', '_', col)\n",
    "    # Ensure no leading or trailing underscores remain\n",
    "    col = col.strip('_')\n",
    "    return col\n",
    "\n",
    "# Update the file path with your file to standardize\n",
    "file_path = '/Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/Translated AS/2020+Tech+Goes+Home+Annual+Survey.csv'  # <-- Update this path\n",
    "\n",
    "# Load the file (assuming CSV; use pd.read_excel for xlsm files)\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display original columns\n",
    "print(\"Original columns:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Apply standardization to each column name\n",
    "new_columns = [standardize_column(col) for col in df.columns]\n",
    "df.columns = new_columns\n",
    "\n",
    "# Display standardized columns\n",
    "print(\"Standardized columns:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Save the updated DataFrame to a new file (update the output path as needed)\n",
    "output_path = '/Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/Translated AS/2020_English_AS_standardized.csv'  # <-- Update this path\n",
    "df.to_csv(output_path, index=False)\n",
    "print(\"Standardized file saved to:\", output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5062e6-8289-4d74-903c-89dec6a5dc09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74d5cdf-57c4-4839-a934-c0917952183b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81c11340-adfa-49b0-93dd-95017e34e54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Simple Data Quality Inspector\n",
      "==================================================\n",
      "üìÇ Loading: merged_translated_cleaned_output.csv\n",
      "‚úÖ Data loaded successfully!\n",
      "\n",
      "üìä DATASET OVERVIEW:\n",
      "   Shape: 3,242 rows √ó 412 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q6/v__5dqvn4fsgnld9cp_x0v1m0000gn/T/ipykernel_50272/936732129.py:23: DtypeWarning: Columns (2,17,23,26,29,43,49,52,54,57,60,61,62,63,64,65,67,69,71,72,73,74,75,77,78,89,90,92,93,95,106,107,108,112,113,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,160,176,192,196,198,205,206,207,208,209,210,212,213,214,215,216,217,219,220,221,222,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,280,281,282,283,284,285,286,287,293,296,298,299,300,301,304,305,307,309,311,312,313,314,315,316,317,320,321,322,324,325,326,327,328,329,331,332,333,334,341,342,343,355,362,363,364,365,366,367,368,369,370,371,372,373,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,402,405,406,408,409,410,411) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  self.df = pd.read_csv(self.file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Memory usage: 42.07 MB\n",
      "\n",
      "   Data types:\n",
      "      object: 366 columns\n",
      "      float64: 45 columns\n",
      "      int64: 1 columns\n",
      "\n",
      "üìà DATA QUALITY ASSESSMENT:\n",
      "   Missing data: 86.2% (1,151,205 out of 1,335,704 cells)\n",
      "   Duplicate rows: 0 (0.0%)\n",
      "   Completely empty columns: 33\n",
      "   üìä Overall quality score: 13.8/100\n",
      "\n",
      "üî§ COLUMN ANALYSIS:\n",
      "   Column quality distribution:\n",
      "      üü¢ Excellent (<10% missing): 9 columns\n",
      "      üîµ Good (10-25% missing): 7 columns\n",
      "      üü° Fair (25-50% missing): 18 columns\n",
      "      üü† Poor (50-90% missing): 110 columns\n",
      "      üî¥ Terrible (>90% missing): 268 columns\n",
      "\n",
      "   üèÜ Best quality columns:\n",
      "      creation date: 0.0% missing\n",
      "      modified date: 0.0% missing\n",
      "      submitted date: 1.6% missing\n",
      "      completion time: 0.0% missing\n",
      "      response url: 0.0% missing\n",
      "\n",
      "   ‚ö†Ô∏è  Worst quality columns:\n",
      "      word processing related skills and digital present: 94.2% missing\n",
      "      please explain why you chose it: 99.3% missing\n",
      "      please explain why you chose it_1: 99.5% missing\n",
      "      formed new friendships or made new connections thr: 91.2% missing\n",
      "      got a new job_1: 94.9% missing\n",
      "\n",
      "üëÄ DATA PREVIEW:\n",
      "   First 10 columns:\n",
      "      1. email address\n",
      "      2. to access a computer ipad\n",
      "      3. to get access to the internet\n",
      "      4. to learn internet skills\n",
      "      5. to gain employment\n",
      "      6. to go to school college ged job vocational training\n",
      "      7. help my child with school\n",
      "      8. get information about my local neighborhood and city\n",
      "      9. to more easily connect with family and friends\n",
      "      10. improve my english\n",
      "      ... and 402 more columns\n",
      "\n",
      "   Sample data (first 3 rows):\n",
      "      Row 1:\n",
      "         email address: santiagocarmencs7w@gmail.com\n",
      "         get information about my local neighborhood and city: No\n",
      "         improve my english: No\n",
      "         other: No\n",
      "         city resources ex city of boston find it cambridge: Not applicable\n",
      "      Row 2:\n",
      "         email address: flawer.pint@gmail.com\n",
      "         get information about my local neighborhood and city: Yes\n",
      "         improve my english: Yes\n",
      "         other: No\n",
      "         city resources ex city of boston find it cambridge: Yes\n",
      "      Row 3:\n",
      "         email address: f1i2gueroa@gmail.com\n",
      "         get information about my local neighborhood and city: Not applicable\n",
      "         improve my english: Yes\n",
      "         other: Not applicable\n",
      "         city resources ex city of boston find it cambridge: Yes\n",
      "\n",
      "‚ùì MISSING DATA PATTERNS:\n",
      "   Row completeness:\n",
      "      Complete rows (100%): 0\n",
      "      Mostly complete (75-99%): 0\n",
      "      Partial data (25-74%): 2\n",
      "      Mostly empty (<25%): 3,240\n",
      "   üìä Average row completeness: 13.8%\n",
      "\n",
      "üí° RECOMMENDATIONS:\n",
      "   1. üö® CRITICAL: >70% missing data - consider major cleanup\n",
      "   2. üóëÔ∏è  Remove 268 columns with >90% missing data\n",
      "   3. üìè Dataset is very wide - consider column reduction for analysis\n",
      "   4. üî§ Many text columns - consider categorical conversion\n",
      "\n",
      "üìã SUMMARY STATISTICS:\n",
      "   Numeric columns (46):\n",
      "       word processing related skills and digital presentations e g google docs and google slides_1  \\\n",
      "count                                                0.0                                              \n",
      "mean                                                 NaN                                              \n",
      "std                                                  NaN                                              \n",
      "min                                                  NaN                                              \n",
      "25%                                                  NaN                                              \n",
      "50%                                                  NaN                                              \n",
      "75%                                                  NaN                                              \n",
      "max                                                  NaN                                              \n",
      "\n",
      "       other_7  please explain why you chose it_5  response html beta  \\\n",
      "count      0.0                                0.0                 0.0   \n",
      "mean       NaN                                NaN                 NaN   \n",
      "std        NaN                                NaN                 NaN   \n",
      "min        NaN                                NaN                 NaN   \n",
      "25%        NaN                                NaN                 NaN   \n",
      "50%        NaN                                NaN                 NaN   \n",
      "75%        NaN                                NaN                 NaN   \n",
      "max        NaN                                NaN                 NaN   \n",
      "\n",
      "       response html  response text  response  resume email  file list  \\\n",
      "count            0.0            0.0       0.0           0.0        0.0   \n",
      "mean             NaN            NaN       NaN           NaN        NaN   \n",
      "std              NaN            NaN       NaN           NaN        NaN   \n",
      "min              NaN            NaN       NaN           NaN        NaN   \n",
      "25%              NaN            NaN       NaN           NaN        NaN   \n",
      "50%              NaN            NaN       NaN           NaN        NaN   \n",
      "75%              NaN            NaN       NaN           NaN        NaN   \n",
      "max              NaN            NaN       NaN           NaN        NaN   \n",
      "\n",
      "       unprotected file list  ...  \\\n",
      "count                    0.0  ...   \n",
      "mean                     NaN  ...   \n",
      "std                      NaN  ...   \n",
      "min                      NaN  ...   \n",
      "25%                      NaN  ...   \n",
      "50%                      NaN  ...   \n",
      "75%                      NaN  ...   \n",
      "max                      NaN  ...   \n",
      "\n",
      "       i am better able to continue with my education and or job training  \\\n",
      "count                                                0.0                    \n",
      "mean                                                 NaN                    \n",
      "std                                                  NaN                    \n",
      "min                                                  NaN                    \n",
      "25%                                                  NaN                    \n",
      "50%                                                  NaN                    \n",
      "75%                                                  NaN                    \n",
      "max                                                  NaN                    \n",
      "\n",
      "       has your small business been able to stay open during the covid 19 pandemic  \\\n",
      "count                                                0.0                             \n",
      "mean                                                 NaN                             \n",
      "std                                                  NaN                             \n",
      "min                                                  NaN                             \n",
      "25%                                                  NaN                             \n",
      "50%                                                  NaN                             \n",
      "75%                                                  NaN                             \n",
      "max                                                  NaN                             \n",
      "\n",
      "       did you apply for a small business assistance fund grant during the covid 19 pandemic  \\\n",
      "count                                                0.0                                       \n",
      "mean                                                 NaN                                       \n",
      "std                                                  NaN                                       \n",
      "min                                                  NaN                                       \n",
      "25%                                                  NaN                                       \n",
      "50%                                                  NaN                                       \n",
      "75%                                                  NaN                                       \n",
      "max                                                  NaN                                       \n",
      "\n",
      "       was the course ipad based tgh early childhood or computer based tgh school  \\\n",
      "count                                                0.0                            \n",
      "mean                                                 NaN                            \n",
      "std                                                  NaN                            \n",
      "min                                                  NaN                            \n",
      "25%                                                  NaN                            \n",
      "50%                                                  NaN                            \n",
      "75%                                                  NaN                            \n",
      "max                                                  NaN                            \n",
      "\n",
      "       recognizing letters and words reading  typing word processing_1  \\\n",
      "count                                    0.0                       0.0   \n",
      "mean                                     NaN                       NaN   \n",
      "std                                      NaN                       NaN   \n",
      "min                                      NaN                       NaN   \n",
      "25%                                      NaN                       NaN   \n",
      "50%                                      NaN                       NaN   \n",
      "75%                                      NaN                       NaN   \n",
      "max                                      NaN                       NaN   \n",
      "\n",
      "       accessing health information_1  \\\n",
      "count                             0.0   \n",
      "mean                              NaN   \n",
      "std                               NaN   \n",
      "min                               NaN   \n",
      "25%                               NaN   \n",
      "50%                               NaN   \n",
      "75%                               NaN   \n",
      "max                               NaN   \n",
      "\n",
      "       how often is a computer or other digital device available to your children for educational purposes  \\\n",
      "count                                                0.0                                                     \n",
      "mean                                                 NaN                                                     \n",
      "std                                                  NaN                                                     \n",
      "min                                                  NaN                                                     \n",
      "25%                                                  NaN                                                     \n",
      "50%                                                  NaN                                                     \n",
      "75%                                                  NaN                                                     \n",
      "max                                                  NaN                                                     \n",
      "\n",
      "       is the computer or other digital device  \\\n",
      "count                                      0.0   \n",
      "mean                                       NaN   \n",
      "std                                        NaN   \n",
      "min                                        NaN   \n",
      "25%                                        NaN   \n",
      "50%                                        NaN   \n",
      "75%                                        NaN   \n",
      "max                                        NaN   \n",
      "\n",
      "       how reliable is the internet connection in your household ability to handle increased internet demands during the pandemic such as video conferencing  \n",
      "count                                                0.0                                                                                                      \n",
      "mean                                                 NaN                                                                                                      \n",
      "std                                                  NaN                                                                                                      \n",
      "min                                                  NaN                                                                                                      \n",
      "25%                                                  NaN                                                                                                      \n",
      "50%                                                  NaN                                                                                                      \n",
      "75%                                                  NaN                                                                                                      \n",
      "max                                                  NaN                                                                                                      \n",
      "\n",
      "[8 rows x 46 columns]\n",
      "\n",
      "   Text columns (366):\n",
      "   Sample unique value counts:\n",
      "      email address: 1781 unique values\n",
      "      to access a computer ipad: 8 unique values\n",
      "      to get access to the internet: 4 unique values\n",
      "      to learn internet skills: 4 unique values\n",
      "      to gain employment: 4 unique values\n",
      "\n",
      "üéØ INSPECTION COMPLETE!\n",
      "Review the analysis above to understand your data quality.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class SimpleDataInspector:\n",
    "    \"\"\"\n",
    "    Simple, focused data quality inspector\n",
    "    Provides clear insights without overwhelming details\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.df = None\n",
    "    \n",
    "    def load_and_inspect(self):\n",
    "        \"\"\"Load data and run comprehensive inspection\"\"\"\n",
    "        \n",
    "        print(\"üîç Simple Data Quality Inspector\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Load the data\n",
    "        try:\n",
    "            print(f\"üìÇ Loading: {self.file_path.split('/')[-1]}\")\n",
    "            self.df = pd.read_csv(self.file_path)\n",
    "            print(f\"‚úÖ Data loaded successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading file: {str(e)}\")\n",
    "            return False\n",
    "        \n",
    "        # Basic overview\n",
    "        self.basic_overview()\n",
    "        \n",
    "        # Data quality check\n",
    "        self.data_quality_check()\n",
    "        \n",
    "        # Column analysis\n",
    "        self.column_analysis()\n",
    "        \n",
    "        # Sample data preview\n",
    "        self.data_preview()\n",
    "        \n",
    "        # Missing data patterns\n",
    "        self.missing_data_analysis()\n",
    "        \n",
    "        # Recommendations\n",
    "        self.generate_recommendations()\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def basic_overview(self):\n",
    "        \"\"\"Basic dataset overview\"\"\"\n",
    "        \n",
    "        print(f\"\\nüìä DATASET OVERVIEW:\")\n",
    "        print(f\"   Shape: {self.df.shape[0]:,} rows √ó {self.df.shape[1]:,} columns\")\n",
    "        print(f\"   Memory usage: {self.df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "        \n",
    "        # Data types\n",
    "        dtype_counts = self.df.dtypes.value_counts()\n",
    "        print(f\"\\n   Data types:\")\n",
    "        for dtype, count in dtype_counts.items():\n",
    "            print(f\"      {str(dtype)}: {count} columns\")\n",
    "    \n",
    "    def data_quality_check(self):\n",
    "        \"\"\"Quick data quality assessment\"\"\"\n",
    "        \n",
    "        print(f\"\\nüìà DATA QUALITY ASSESSMENT:\")\n",
    "        \n",
    "        # Overall missing data\n",
    "        total_cells = self.df.shape[0] * self.df.shape[1]\n",
    "        missing_cells = self.df.isnull().sum().sum()\n",
    "        missing_pct = (missing_cells / total_cells) * 100\n",
    "        \n",
    "        print(f\"   Missing data: {missing_pct:.1f}% ({missing_cells:,} out of {total_cells:,} cells)\")\n",
    "        \n",
    "        # Duplicate rows\n",
    "        duplicates = self.df.duplicated().sum()\n",
    "        print(f\"   Duplicate rows: {duplicates:,} ({duplicates/len(self.df)*100:.1f}%)\")\n",
    "        \n",
    "        # Empty columns\n",
    "        empty_cols = (self.df.isnull().all()).sum()\n",
    "        print(f\"   Completely empty columns: {empty_cols}\")\n",
    "        \n",
    "        # Quality score\n",
    "        quality_score = max(0, 100 - missing_pct - (duplicates/len(self.df)*10))\n",
    "        print(f\"   üìä Overall quality score: {quality_score:.1f}/100\")\n",
    "    \n",
    "    def column_analysis(self):\n",
    "        \"\"\"Analyze column patterns and quality\"\"\"\n",
    "        \n",
    "        print(f\"\\nüî§ COLUMN ANALYSIS:\")\n",
    "        \n",
    "        # Missing data by column\n",
    "        missing_by_col = self.df.isnull().sum()\n",
    "        missing_pct_by_col = (missing_by_col / len(self.df)) * 100\n",
    "        \n",
    "        # Categorize columns by data quality\n",
    "        excellent = (missing_pct_by_col < 10).sum()\n",
    "        good = ((missing_pct_by_col >= 10) & (missing_pct_by_col < 25)).sum()\n",
    "        fair = ((missing_pct_by_col >= 25) & (missing_pct_by_col < 50)).sum()\n",
    "        poor = ((missing_pct_by_col >= 50) & (missing_pct_by_col < 90)).sum()\n",
    "        terrible = (missing_pct_by_col >= 90).sum()\n",
    "        \n",
    "        print(f\"   Column quality distribution:\")\n",
    "        print(f\"      üü¢ Excellent (<10% missing): {excellent} columns\")\n",
    "        print(f\"      üîµ Good (10-25% missing): {good} columns\")\n",
    "        print(f\"      üü° Fair (25-50% missing): {fair} columns\")\n",
    "        print(f\"      üü† Poor (50-90% missing): {poor} columns\")\n",
    "        print(f\"      üî¥ Terrible (>90% missing): {terrible} columns\")\n",
    "        \n",
    "        # Show best and worst columns\n",
    "        if excellent > 0:\n",
    "            best_cols = missing_pct_by_col[missing_pct_by_col < 10].head()\n",
    "            print(f\"\\n   üèÜ Best quality columns:\")\n",
    "            for col, pct in best_cols.items():\n",
    "                print(f\"      {col[:50]}: {pct:.1f}% missing\")\n",
    "        \n",
    "        if terrible > 0:\n",
    "            worst_cols = missing_pct_by_col[missing_pct_by_col >= 90].head()\n",
    "            print(f\"\\n   ‚ö†Ô∏è  Worst quality columns:\")\n",
    "            for col, pct in worst_cols.items():\n",
    "                print(f\"      {col[:50]}: {pct:.1f}% missing\")\n",
    "    \n",
    "    def data_preview(self):\n",
    "        \"\"\"Show sample of the data\"\"\"\n",
    "        \n",
    "        print(f\"\\nüëÄ DATA PREVIEW:\")\n",
    "        \n",
    "        # Show column names (first 10)\n",
    "        print(f\"   First 10 columns:\")\n",
    "        for i, col in enumerate(self.df.columns[:10]):\n",
    "            print(f\"      {i+1}. {col}\")\n",
    "        \n",
    "        if len(self.df.columns) > 10:\n",
    "            print(f\"      ... and {len(self.df.columns) - 10} more columns\")\n",
    "        \n",
    "        # Show sample data for key columns\n",
    "        key_cols = []\n",
    "        for col in self.df.columns[:20]:  # Check first 20 columns\n",
    "            if self.df[col].count() > len(self.df) * 0.5:  # At least 50% non-null\n",
    "                key_cols.append(col)\n",
    "                if len(key_cols) >= 5:  # Show max 5 columns\n",
    "                    break\n",
    "        \n",
    "        if key_cols:\n",
    "            print(f\"\\n   Sample data (first 3 rows):\")\n",
    "            sample_data = self.df[key_cols].head(3)\n",
    "            for idx, row in sample_data.iterrows():\n",
    "                print(f\"      Row {idx + 1}:\")\n",
    "                for col in key_cols:\n",
    "                    val = str(row[col])[:30] + \"...\" if len(str(row[col])) > 30 else str(row[col])\n",
    "                    print(f\"         {col}: {val}\")\n",
    "    \n",
    "    def missing_data_analysis(self):\n",
    "        \"\"\"Analyze missing data patterns\"\"\"\n",
    "        \n",
    "        print(f\"\\n‚ùì MISSING DATA PATTERNS:\")\n",
    "        \n",
    "        # Rows with different levels of completeness\n",
    "        row_completeness = self.df.count(axis=1) / len(self.df.columns) * 100\n",
    "        \n",
    "        complete_rows = (row_completeness == 100).sum()\n",
    "        mostly_complete = ((row_completeness >= 75) & (row_completeness < 100)).sum()\n",
    "        partial = ((row_completeness >= 25) & (row_completeness < 75)).sum()\n",
    "        mostly_empty = (row_completeness < 25).sum()\n",
    "        \n",
    "        print(f\"   Row completeness:\")\n",
    "        print(f\"      Complete rows (100%): {complete_rows:,}\")\n",
    "        print(f\"      Mostly complete (75-99%): {mostly_complete:,}\")\n",
    "        print(f\"      Partial data (25-74%): {partial:,}\")\n",
    "        print(f\"      Mostly empty (<25%): {mostly_empty:,}\")\n",
    "        \n",
    "        # Average completeness\n",
    "        avg_completeness = row_completeness.mean()\n",
    "        print(f\"   üìä Average row completeness: {avg_completeness:.1f}%\")\n",
    "    \n",
    "    def generate_recommendations(self):\n",
    "        \"\"\"Generate actionable recommendations\"\"\"\n",
    "        \n",
    "        print(f\"\\nüí° RECOMMENDATIONS:\")\n",
    "        \n",
    "        recommendations = []\n",
    "        \n",
    "        # Missing data recommendations\n",
    "        missing_pct = (self.df.isnull().sum().sum() / (self.df.shape[0] * self.df.shape[1])) * 100\n",
    "        \n",
    "        if missing_pct > 70:\n",
    "            recommendations.append(\"üö® CRITICAL: >70% missing data - consider major cleanup\")\n",
    "        elif missing_pct > 50:\n",
    "            recommendations.append(\"‚ö†Ô∏è  HIGH: >50% missing data - column cleanup needed\")\n",
    "        elif missing_pct > 25:\n",
    "            recommendations.append(\"üü° MODERATE: >25% missing data - some cleanup beneficial\")\n",
    "        else:\n",
    "            recommendations.append(\"‚úÖ GOOD: Missing data levels are manageable\")\n",
    "        \n",
    "        # Column recommendations\n",
    "        terrible_cols = (self.df.isnull().sum() / len(self.df) * 100 >= 90).sum()\n",
    "        if terrible_cols > 50:\n",
    "            recommendations.append(f\"üóëÔ∏è  Remove {terrible_cols} columns with >90% missing data\")\n",
    "        \n",
    "        # Duplicate recommendations\n",
    "        duplicates = self.df.duplicated().sum()\n",
    "        if duplicates > 0:\n",
    "            recommendations.append(f\"üîÑ Remove {duplicates} duplicate rows\")\n",
    "        \n",
    "        # Size recommendations\n",
    "        if self.df.shape[1] > 200:\n",
    "            recommendations.append(\"üìè Dataset is very wide - consider column reduction for analysis\")\n",
    "        \n",
    "        if self.df.shape[0] > 10000:\n",
    "            recommendations.append(\"üìä Large dataset - consider sampling for initial analysis\")\n",
    "        \n",
    "        # Data type recommendations\n",
    "        object_cols = (self.df.dtypes == 'object').sum()\n",
    "        if object_cols / len(self.df.columns) > 0.8:\n",
    "            recommendations.append(\"üî§ Many text columns - consider categorical conversion\")\n",
    "        \n",
    "        # Print recommendations\n",
    "        for i, rec in enumerate(recommendations, 1):\n",
    "            print(f\"   {i}. {rec}\")\n",
    "        \n",
    "        if not recommendations:\n",
    "            print(\"   ‚úÖ Data quality looks good - no major issues detected!\")\n",
    "    \n",
    "    def summary_stats(self):\n",
    "        \"\"\"Generate summary statistics\"\"\"\n",
    "        \n",
    "        print(f\"\\nüìã SUMMARY STATISTICS:\")\n",
    "        \n",
    "        # Numeric columns summary\n",
    "        numeric_cols = self.df.select_dtypes(include=[np.number]).columns\n",
    "        if len(numeric_cols) > 0:\n",
    "            print(f\"   Numeric columns ({len(numeric_cols)}):\")\n",
    "            numeric_summary = self.df[numeric_cols].describe()\n",
    "            print(numeric_summary.round(2))\n",
    "        \n",
    "        # Text columns summary\n",
    "        text_cols = self.df.select_dtypes(include=['object']).columns\n",
    "        if len(text_cols) > 0:\n",
    "            print(f\"\\n   Text columns ({len(text_cols)}):\")\n",
    "            print(\"   Sample unique value counts:\")\n",
    "            for col in text_cols[:5]:  # Show first 5 text columns\n",
    "                unique_count = self.df[col].nunique()\n",
    "                print(f\"      {col[:40]}: {unique_count} unique values\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run data inspection\"\"\"\n",
    "    \n",
    "    # File path\n",
    "    file_path = \"/Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/AS/merged_translated_cleaned_output.csv\"\n",
    "    \n",
    "    # Initialize inspector\n",
    "    inspector = SimpleDataInspector(file_path)\n",
    "    \n",
    "    # Run inspection\n",
    "    success = inspector.load_and_inspect()\n",
    "    \n",
    "    if success:\n",
    "        # Generate summary stats\n",
    "        inspector.summary_stats()\n",
    "        \n",
    "        print(f\"\\nüéØ INSPECTION COMPLETE!\")\n",
    "        print(f\"Review the analysis above to understand your data quality.\")\n",
    "        \n",
    "    return success\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a53a85b-a118-4bce-a8a8-afbe5e9661ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78287680-967f-4b81-a041-4da2375c0a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Inspecting: ASmergedFile.xlsx\n",
      "üîç Simple Data Quality Inspector\n",
      "==================================================\n",
      "üìÇ Loading: ASmergedFile.xlsx\n",
      "‚úÖ Data loaded successfully!\n",
      "\n",
      "üìä DATASET OVERVIEW:\n",
      "   Shape: 3,242 rows √ó 113 columns\n",
      "   Memory usage: 13.12 MB\n",
      "\n",
      "   Data types:\n",
      "      object: 103 columns\n",
      "      float64: 8 columns\n",
      "      int64: 2 columns\n",
      "\n",
      "üìà DATA QUALITY ASSESSMENT:\n",
      "   Missing data: 70.7% (259,060 out of 366,346 cells)\n",
      "   Duplicate rows: 0 (0.0%)\n",
      "   Completely empty columns: 7\n",
      "   üìä Overall quality score: 29.3/100\n",
      "\n",
      "üî§ COLUMN ANALYSIS:\n",
      "   Column quality distribution:\n",
      "      üü¢ Excellent (<10% missing): 9 columns\n",
      "      üîµ Good (10-25% missing): 5 columns\n",
      "      üü° Fair (25-50% missing): 9 columns\n",
      "      üü† Poor (50-90% missing): 57 columns\n",
      "      üî¥ Terrible (>90% missing): 33 columns\n",
      "\n",
      "   üèÜ Best quality columns:\n",
      "      submitted_date: 1.6% missing\n",
      "      creation_date: 0.0% missing\n",
      "      modified_date: 0.0% missing\n",
      "      completion_time: 0.0% missing\n",
      "      response_url: 0.0% missing\n",
      "\n",
      "   ‚ö†Ô∏è  Worst quality columns:\n",
      "      please_explain_the_reason_you_chose_other1: 96.9% missing\n",
      "      please_explain_the_reason_you_chose_other2: 97.8% missing\n",
      "      x: 99.5% missing\n",
      "      communicate_with_my_employeremployeescoworkers: 91.1% missing\n",
      "      job_search_and_apply_for_jobs_online: 91.7% missing\n",
      "\n",
      "üëÄ DATA PREVIEW:\n",
      "   First 10 columns:\n",
      "      1. submitted_date\n",
      "      2. first_name\n",
      "      3. last_name\n",
      "      4. email_address\n",
      "      5. year\n",
      "      6. to_get_access_to_a_computer_ipad\n",
      "      7. to_get_access_to_the_internet\n",
      "      8. to_learn_internet_skills\n",
      "      9. to_gain_employment\n",
      "      10. to_get_a_better_job\n",
      "      ... and 103 more columns\n",
      "\n",
      "   Sample data (first 3 rows):\n",
      "      Row 1:\n",
      "         submitted_date: 7/24/20 0:39\n",
      "         first_name: Junior \n",
      "         last_name: Gabriel \n",
      "         email_address: Juniorgab499@gmail.com\n",
      "         year: 2020.0\n",
      "      Row 2:\n",
      "         submitted_date: 8/3/20 18:33\n",
      "         first_name: Joanne\n",
      "         last_name: Noel\n",
      "         email_address: Joannec86@gmail.com\n",
      "         year: 2020.0\n",
      "      Row 3:\n",
      "         submitted_date: 7/28/20 17:11\n",
      "         first_name: Athena\n",
      "         last_name: Sabanoglou \n",
      "         email_address: Athena.sabanoglou@yahoo.com\n",
      "         year: 2020.0\n",
      "\n",
      "‚ùì MISSING DATA PATTERNS:\n",
      "   Row completeness:\n",
      "      Complete rows (100%): 0\n",
      "      Mostly complete (75-99%): 88\n",
      "      Partial data (25-74%): 1,562\n",
      "      Mostly empty (<25%): 1,592\n",
      "   üìä Average row completeness: 29.3%\n",
      "\n",
      "üí° RECOMMENDATIONS:\n",
      "   1. üö® CRITICAL: >70% missing data - consider major cleanup\n",
      "   2. üî§ Many text columns - consider categorical conversion\n",
      "\n",
      "üìã SUMMARY STATISTICS:\n",
      "   Numeric columns (10):\n",
      "          year  response_html_beta  response_html  response_text  response  \\\n",
      "count  2845.00                 0.0            0.0            0.0       0.0   \n",
      "mean   2020.73                 NaN            NaN            NaN       NaN   \n",
      "std       1.11                 NaN            NaN            NaN       NaN   \n",
      "min    2020.00                 NaN            NaN            NaN       NaN   \n",
      "25%    2020.00                 NaN            NaN            NaN       NaN   \n",
      "50%    2020.00                 NaN            NaN            NaN       NaN   \n",
      "75%    2021.00                 NaN            NaN            NaN       NaN   \n",
      "max    2024.00                 NaN            NaN            NaN       NaN   \n",
      "\n",
      "       resume_email  file_list  unprotected_file_list   response_id  \\\n",
      "count           0.0        0.0                    0.0  3.242000e+03   \n",
      "mean            NaN        NaN                    NaN  1.919774e+08   \n",
      "std             NaN        NaN                    NaN  4.792594e+07   \n",
      "min             NaN        NaN                    NaN  1.147234e+08   \n",
      "25%             NaN        NaN                    NaN  1.708177e+08   \n",
      "50%             NaN        NaN                    NaN  1.728017e+08   \n",
      "75%             NaN        NaN                    NaN  2.151854e+08   \n",
      "max             NaN        NaN                    NaN  2.835116e+08   \n",
      "\n",
      "       # of responses  \n",
      "count         3242.00  \n",
      "mean            32.09  \n",
      "std             19.26  \n",
      "min              9.00  \n",
      "25%             21.00  \n",
      "50%             28.00  \n",
      "75%             39.00  \n",
      "max             93.00  \n",
      "\n",
      "   Text columns (103):\n",
      "   Sample unique value counts:\n",
      "      submitted_date: 2444 unique values\n",
      "      first_name: 1450 unique values\n",
      "      last_name: 1420 unique values\n",
      "      email_address: 1781 unique values\n",
      "      to_get_access_to_a_computer_ipad: 5 unique values\n",
      "\n",
      "üéØ INSPECTION COMPLETE!\n",
      "Review the analysis above to understand your data quality.\n"
     ]
    }
   ],
   "source": [
    "#AS Merged File\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class SimpleDataInspector:\n",
    "    \"\"\"\n",
    "    Simple, focused data quality inspector\n",
    "    Provides clear insights without overwhelming details\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.df = None\n",
    "    \n",
    "    def load_and_inspect(self):\n",
    "        \"\"\"Load data and run comprehensive inspection\"\"\"\n",
    "        \n",
    "        print(\"üîç Simple Data Quality Inspector\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Load the data\n",
    "        try:\n",
    "            file_name = self.file_path.split('/')[-1]\n",
    "            print(f\"üìÇ Loading: {file_name}\")\n",
    "            \n",
    "            # Handle different file types\n",
    "            if self.file_path.endswith('.xlsx') or self.file_path.endswith('.xls'):\n",
    "                self.df = pd.read_excel(self.file_path)\n",
    "            elif self.file_path.endswith('.csv'):\n",
    "                self.df = pd.read_csv(self.file_path)\n",
    "            else:\n",
    "                print(f\"‚ùå Unsupported file type\")\n",
    "                return False\n",
    "                \n",
    "            print(f\"‚úÖ Data loaded successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading file: {str(e)}\")\n",
    "            return False\n",
    "        \n",
    "        # Basic overview\n",
    "        self.basic_overview()\n",
    "        \n",
    "        # Data quality check\n",
    "        self.data_quality_check()\n",
    "        \n",
    "        # Column analysis\n",
    "        self.column_analysis()\n",
    "        \n",
    "        # Sample data preview\n",
    "        self.data_preview()\n",
    "        \n",
    "        # Missing data patterns\n",
    "        self.missing_data_analysis()\n",
    "        \n",
    "        # Recommendations\n",
    "        self.generate_recommendations()\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def basic_overview(self):\n",
    "        \"\"\"Basic dataset overview\"\"\"\n",
    "        \n",
    "        print(f\"\\nüìä DATASET OVERVIEW:\")\n",
    "        print(f\"   Shape: {self.df.shape[0]:,} rows √ó {self.df.shape[1]:,} columns\")\n",
    "        print(f\"   Memory usage: {self.df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "        \n",
    "        # Data types\n",
    "        dtype_counts = self.df.dtypes.value_counts()\n",
    "        print(f\"\\n   Data types:\")\n",
    "        for dtype, count in dtype_counts.items():\n",
    "            print(f\"      {str(dtype)}: {count} columns\")\n",
    "    \n",
    "    def data_quality_check(self):\n",
    "        \"\"\"Quick data quality assessment\"\"\"\n",
    "        \n",
    "        print(f\"\\nüìà DATA QUALITY ASSESSMENT:\")\n",
    "        \n",
    "        # Overall missing data\n",
    "        total_cells = self.df.shape[0] * self.df.shape[1]\n",
    "        missing_cells = self.df.isnull().sum().sum()\n",
    "        missing_pct = (missing_cells / total_cells) * 100\n",
    "        \n",
    "        print(f\"   Missing data: {missing_pct:.1f}% ({missing_cells:,} out of {total_cells:,} cells)\")\n",
    "        \n",
    "        # Duplicate rows\n",
    "        duplicates = self.df.duplicated().sum()\n",
    "        print(f\"   Duplicate rows: {duplicates:,} ({duplicates/len(self.df)*100:.1f}%)\")\n",
    "        \n",
    "        # Empty columns\n",
    "        empty_cols = (self.df.isnull().all()).sum()\n",
    "        print(f\"   Completely empty columns: {empty_cols}\")\n",
    "        \n",
    "        # Quality score\n",
    "        quality_score = max(0, 100 - missing_pct - (duplicates/len(self.df)*10))\n",
    "        print(f\"   üìä Overall quality score: {quality_score:.1f}/100\")\n",
    "    \n",
    "    def column_analysis(self):\n",
    "        \"\"\"Analyze column patterns and quality\"\"\"\n",
    "        \n",
    "        print(f\"\\nüî§ COLUMN ANALYSIS:\")\n",
    "        \n",
    "        # Missing data by column\n",
    "        missing_by_col = self.df.isnull().sum()\n",
    "        missing_pct_by_col = (missing_by_col / len(self.df)) * 100\n",
    "        \n",
    "        # Categorize columns by data quality\n",
    "        excellent = (missing_pct_by_col < 10).sum()\n",
    "        good = ((missing_pct_by_col >= 10) & (missing_pct_by_col < 25)).sum()\n",
    "        fair = ((missing_pct_by_col >= 25) & (missing_pct_by_col < 50)).sum()\n",
    "        poor = ((missing_pct_by_col >= 50) & (missing_pct_by_col < 90)).sum()\n",
    "        terrible = (missing_pct_by_col >= 90).sum()\n",
    "        \n",
    "        print(f\"   Column quality distribution:\")\n",
    "        print(f\"      üü¢ Excellent (<10% missing): {excellent} columns\")\n",
    "        print(f\"      üîµ Good (10-25% missing): {good} columns\")\n",
    "        print(f\"      üü° Fair (25-50% missing): {fair} columns\")\n",
    "        print(f\"      üü† Poor (50-90% missing): {poor} columns\")\n",
    "        print(f\"      üî¥ Terrible (>90% missing): {terrible} columns\")\n",
    "        \n",
    "        # Show best and worst columns\n",
    "        if excellent > 0:\n",
    "            best_cols = missing_pct_by_col[missing_pct_by_col < 10].head()\n",
    "            print(f\"\\n   üèÜ Best quality columns:\")\n",
    "            for col, pct in best_cols.items():\n",
    "                print(f\"      {col[:50]}: {pct:.1f}% missing\")\n",
    "        \n",
    "        if terrible > 0:\n",
    "            worst_cols = missing_pct_by_col[missing_pct_by_col >= 90].head()\n",
    "            print(f\"\\n   ‚ö†Ô∏è  Worst quality columns:\")\n",
    "            for col, pct in worst_cols.items():\n",
    "                print(f\"      {col[:50]}: {pct:.1f}% missing\")\n",
    "    \n",
    "    def data_preview(self):\n",
    "        \"\"\"Show sample of the data\"\"\"\n",
    "        \n",
    "        print(f\"\\nüëÄ DATA PREVIEW:\")\n",
    "        \n",
    "        # Show column names (first 10)\n",
    "        print(f\"   First 10 columns:\")\n",
    "        for i, col in enumerate(self.df.columns[:10]):\n",
    "            print(f\"      {i+1}. {col}\")\n",
    "        \n",
    "        if len(self.df.columns) > 10:\n",
    "            print(f\"      ... and {len(self.df.columns) - 10} more columns\")\n",
    "        \n",
    "        # Show sample data for key columns\n",
    "        key_cols = []\n",
    "        for col in self.df.columns[:20]:  # Check first 20 columns\n",
    "            if self.df[col].count() > len(self.df) * 0.5:  # At least 50% non-null\n",
    "                key_cols.append(col)\n",
    "                if len(key_cols) >= 5:  # Show max 5 columns\n",
    "                    break\n",
    "        \n",
    "        if key_cols:\n",
    "            print(f\"\\n   Sample data (first 3 rows):\")\n",
    "            sample_data = self.df[key_cols].head(3)\n",
    "            for idx, row in sample_data.iterrows():\n",
    "                print(f\"      Row {idx + 1}:\")\n",
    "                for col in key_cols:\n",
    "                    val = str(row[col])[:30] + \"...\" if len(str(row[col])) > 30 else str(row[col])\n",
    "                    print(f\"         {col}: {val}\")\n",
    "    \n",
    "    def missing_data_analysis(self):\n",
    "        \"\"\"Analyze missing data patterns\"\"\"\n",
    "        \n",
    "        print(f\"\\n‚ùì MISSING DATA PATTERNS:\")\n",
    "        \n",
    "        # Rows with different levels of completeness\n",
    "        row_completeness = self.df.count(axis=1) / len(self.df.columns) * 100\n",
    "        \n",
    "        complete_rows = (row_completeness == 100).sum()\n",
    "        mostly_complete = ((row_completeness >= 75) & (row_completeness < 100)).sum()\n",
    "        partial = ((row_completeness >= 25) & (row_completeness < 75)).sum()\n",
    "        mostly_empty = (row_completeness < 25).sum()\n",
    "        \n",
    "        print(f\"   Row completeness:\")\n",
    "        print(f\"      Complete rows (100%): {complete_rows:,}\")\n",
    "        print(f\"      Mostly complete (75-99%): {mostly_complete:,}\")\n",
    "        print(f\"      Partial data (25-74%): {partial:,}\")\n",
    "        print(f\"      Mostly empty (<25%): {mostly_empty:,}\")\n",
    "        \n",
    "        # Average completeness\n",
    "        avg_completeness = row_completeness.mean()\n",
    "        print(f\"   üìä Average row completeness: {avg_completeness:.1f}%\")\n",
    "    \n",
    "    def generate_recommendations(self):\n",
    "        \"\"\"Generate actionable recommendations\"\"\"\n",
    "        \n",
    "        print(f\"\\nüí° RECOMMENDATIONS:\")\n",
    "        \n",
    "        recommendations = []\n",
    "        \n",
    "        # Missing data recommendations\n",
    "        missing_pct = (self.df.isnull().sum().sum() / (self.df.shape[0] * self.df.shape[1])) * 100\n",
    "        \n",
    "        if missing_pct > 70:\n",
    "            recommendations.append(\"üö® CRITICAL: >70% missing data - consider major cleanup\")\n",
    "        elif missing_pct > 50:\n",
    "            recommendations.append(\"‚ö†Ô∏è  HIGH: >50% missing data - column cleanup needed\")\n",
    "        elif missing_pct > 25:\n",
    "            recommendations.append(\"üü° MODERATE: >25% missing data - some cleanup beneficial\")\n",
    "        else:\n",
    "            recommendations.append(\"‚úÖ GOOD: Missing data levels are manageable\")\n",
    "        \n",
    "        # Column recommendations\n",
    "        terrible_cols = (self.df.isnull().sum() / len(self.df) * 100 >= 90).sum()\n",
    "        if terrible_cols > 50:\n",
    "            recommendations.append(f\"üóëÔ∏è  Remove {terrible_cols} columns with >90% missing data\")\n",
    "        \n",
    "        # Duplicate recommendations\n",
    "        duplicates = self.df.duplicated().sum()\n",
    "        if duplicates > 0:\n",
    "            recommendations.append(f\"üîÑ Remove {duplicates} duplicate rows\")\n",
    "        \n",
    "        # Size recommendations\n",
    "        if self.df.shape[1] > 200:\n",
    "            recommendations.append(\"üìè Dataset is very wide - consider column reduction for analysis\")\n",
    "        \n",
    "        if self.df.shape[0] > 10000:\n",
    "            recommendations.append(\"üìä Large dataset - consider sampling for initial analysis\")\n",
    "        \n",
    "        # Data type recommendations\n",
    "        object_cols = (self.df.dtypes == 'object').sum()\n",
    "        if object_cols / len(self.df.columns) > 0.8:\n",
    "            recommendations.append(\"üî§ Many text columns - consider categorical conversion\")\n",
    "        \n",
    "        # Print recommendations\n",
    "        for i, rec in enumerate(recommendations, 1):\n",
    "            print(f\"   {i}. {rec}\")\n",
    "        \n",
    "        if not recommendations:\n",
    "            print(\"   ‚úÖ Data quality looks good - no major issues detected!\")\n",
    "    \n",
    "    def summary_stats(self):\n",
    "        \"\"\"Generate summary statistics\"\"\"\n",
    "        \n",
    "        print(f\"\\nüìã SUMMARY STATISTICS:\")\n",
    "        \n",
    "        # Numeric columns summary\n",
    "        numeric_cols = self.df.select_dtypes(include=[np.number]).columns\n",
    "        if len(numeric_cols) > 0:\n",
    "            print(f\"   Numeric columns ({len(numeric_cols)}):\")\n",
    "            numeric_summary = self.df[numeric_cols].describe()\n",
    "            print(numeric_summary.round(2))\n",
    "        \n",
    "        # Text columns summary\n",
    "        text_cols = self.df.select_dtypes(include=['object']).columns\n",
    "        if len(text_cols) > 0:\n",
    "            print(f\"\\n   Text columns ({len(text_cols)}):\")\n",
    "            print(\"   Sample unique value counts:\")\n",
    "            for col in text_cols[:5]:  # Show first 5 text columns\n",
    "                unique_count = self.df[col].nunique()\n",
    "                print(f\"      {col[:40]}: {unique_count} unique values\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run data inspection\"\"\"\n",
    "    \n",
    "    # File path - Updated to ASmergedFile.xlsx\n",
    "    file_path = \"/Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/AS/ASmergedFile.xlsx\"\n",
    "    \n",
    "    print(f\"üéØ Inspecting: {file_path.split('/')[-1]}\")\n",
    "    \n",
    "    # Initialize inspector\n",
    "    inspector = SimpleDataInspector(file_path)\n",
    "    \n",
    "    # Run inspection\n",
    "    success = inspector.load_and_inspect()\n",
    "    \n",
    "    if success:\n",
    "        # Generate summary stats\n",
    "        inspector.summary_stats()\n",
    "        \n",
    "        print(f\"\\nüéØ INSPECTION COMPLETE!\")\n",
    "        print(f\"Review the analysis above to understand your data quality.\")\n",
    "        \n",
    "    return success\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "085925da-0094-411c-aae8-cd738dacf2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ DEEP DIVE DATA PATTERN ANALYZER\n",
      "==================================================\n",
      "‚úÖ Loaded: (3242, 113)\n",
      "\n",
      "üîç DEEP COLUMN PATTERN ANALYSIS\n",
      "============================================================\n",
      "\n",
      "üìä COLUMN CATEGORIZATION\n",
      "========================================\n",
      "\n",
      "PARTICIPANT INFO (4 columns):\n",
      "   - first_name: 17.4% null\n",
      "   - last_name: 20.1% null\n",
      "   - email_address: 26.1% null\n",
      "   - form_name: 87.7% null\n",
      "\n",
      "SURVEY QUESTIONS (34 columns):\n",
      "   - what_is_your_current_employment_status: 16.0% null\n",
      "   - what_was_your_employment_status_before_participati: 23.1% null\n",
      "   - did_you_take_the_tgh_course_focused_on_building_sm: 35.7% null\n",
      "   - did_you_take_tgh_with_a_child: 41.9% null\n",
      "   - communicating_with_others_ex_email_video_chat: 43.4% null\n",
      "   ... and 29 more\n",
      "\n",
      "METADATA (53 columns):\n",
      "   - response_url: 0.0% null\n",
      "   - response_id: 0.0% null\n",
      "   - referrer: 0.0% null\n",
      "   - ip_address: 0.0% null\n",
      "   - year: 12.2% null\n",
      "   ... and 48 more\n",
      "\n",
      "EMPTY COLUMNS (17 columns):\n",
      "   - other5: 95.3% null\n",
      "   - please_explain_the_reason_you_chose_other1: 96.9% null\n",
      "   - please_explain_the_reason_you_chose_other2: 97.8% null\n",
      "   - recognizing_letters: 98.0% null\n",
      "   - recognizing_numbers_counting: 98.0% null\n",
      "   ... and 12 more\n",
      "\n",
      "DATE FIELDS (4 columns):\n",
      "   - creation_date: 0.0% null\n",
      "   - modified_date: 0.0% null\n",
      "   - completion_time: 0.0% null\n",
      "   - submitted_date: 1.6% null\n",
      "\n",
      "NUMERIC FIELDS (1 columns):\n",
      "   - # of responses: 0.0% null\n",
      "\n",
      "üìã ROW COMPLETENESS ANALYSIS\n",
      "========================================\n",
      "Row completeness distribution:\n",
      "   üî¥ Completely empty (0%): 0\n",
      "   üü† Mostly empty (<10%): 503\n",
      "   üü° Sparse (10-25%): 1089\n",
      "   üîµ Partial (25-75%): 1562\n",
      "   üü¢ Mostly complete (>75%): 88\n",
      "\n",
      "üéØ CLEANUP OPPORTUNITIES\n",
      "========================================\n",
      "COLUMN DELETION CANDIDATES (17):\n",
      "   - other5: 95.3% null\n",
      "   - please_explain_the_reason_you_chose_other1: 96.9% null\n",
      "   - please_explain_the_reason_you_chose_other2: 97.8% null\n",
      "   - recognizing_letters: 98.0% null\n",
      "   - recognizing_numbers_counting: 98.0% null\n",
      "   - please_explain_the_reason_you_chose_other3: 98.7% null\n",
      "   - please_explain_the_reason_you_chose_other4: 99.2% null\n",
      "   - x: 99.5% null\n",
      "   - are_the_skills_you_learned_in_the_tgh_course_helpi: 99.6% null\n",
      "   - please_explain_the_reason_you_chose_other5: 99.6% null\n",
      "\n",
      "COLUMN REVIEW CANDIDATES (16):\n",
      "   - as_a_caregiver_do_you_feel_more_confident_about_yo: 90.9% null\n",
      "   - communicate_with_my_employeremployeescoworkers: 91.1% null\n",
      "   - continue_with_my_education_andor_job_training: 91.5% null\n",
      "   - what_is_the_most_valuable_skill_you_learned_during: 91.5% null\n",
      "   - how_has_your_involvement_in_your_childs_education_: 91.5% null\n",
      "\n",
      "ROW DELETION CANDIDATES: 0 rows\n",
      "\n",
      "STANDARDIZATION OPPORTUNITIES (7):\n",
      "   - did_you_take_the_tgh_course_focused_on_building_sm: ['Yes', 'No', \"I don't know and/or don't want to answer\", 'I don&#39;t know and/or I don&#39;t want to answer', 'Prefer not to answer']\n",
      "   - communicating_with_others_ex_email_video_chat: ['Yes', 'No', 'Not applicable', 'No Aplica']\n",
      "   - data_entry: ['Yes', 'No', 'Not applicable', 'Wi', 'Non']\n",
      "   - other: ['Yes', 'No', 'Not Available', 'Not applicable', 'No Aplica']\n",
      "   - since_the_end_of_your_class_have_you_participated_: ['No', 'Yes', \"I don't know and/or don't want to answer\", 'I don&#39;t know and/or I don&#39;t want to answer', 'I prefer not to answer']\n",
      "\n",
      "üîß CLEANUP SCRIPT PREVIEW\n",
      "========================================\n",
      "PROPOSED CLEANUP IMPACT:\n",
      "   Original size: 3,242 rows √ó 113 columns\n",
      "   After cleanup: 3,242 rows √ó 96 columns\n",
      "   Reduction: 0 rows (0.0%), 17 columns (15.0%)\n",
      "   Estimated meaningful data preserved: 85.0%\n",
      "\n",
      "RECOMMENDED ACTIONS:\n",
      "   1. Delete 17 low-quality columns\n",
      "   2. Remove 0 mostly-empty rows\n",
      "   3. Standardize 7 Yes/No columns\n",
      "   4. Review 16 medium-quality columns\n",
      "\n",
      "‚úÖ DEEP ANALYSIS COMPLETE!\n",
      "\n",
      "üéØ NEXT STEPS:\n",
      "1. Review the cleanup opportunities above\n",
      "2. Decide which columns/rows to remove\n",
      "3. Create and run the cleanup script\n",
      "4. Validate the cleaned dataset\n",
      "5. Proceed with learner journey analysis\n"
     ]
    }
   ],
   "source": [
    "#Deeper dive of Column Analysis\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class DeepDiveAnalyzer:\n",
    "    \"\"\"\n",
    "    Deep dive analyzer for understanding data patterns\n",
    "    and identifying cleanup opportunities\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.df = None\n",
    "        self.analysis_results = {}\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"Load the data file\"\"\"\n",
    "        try:\n",
    "            if self.file_path.endswith('.xlsx') or self.file_path.endswith('.xls'):\n",
    "                self.df = pd.read_excel(self.file_path)\n",
    "            else:\n",
    "                self.df = pd.read_csv(self.file_path)\n",
    "            \n",
    "            print(f\"‚úÖ Loaded: {self.df.shape}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def analyze_column_patterns(self):\n",
    "        \"\"\"Deep analysis of each column's data patterns\"\"\"\n",
    "        \n",
    "        print(\"\\nüîç DEEP COLUMN PATTERN ANALYSIS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        column_details = []\n",
    "        \n",
    "        for col in self.df.columns:\n",
    "            # Basic stats\n",
    "            total_rows = len(self.df)\n",
    "            non_null_count = self.df[col].count()\n",
    "            null_count = total_rows - non_null_count\n",
    "            null_pct = (null_count / total_rows) * 100\n",
    "            \n",
    "            # Unique values analysis\n",
    "            unique_count = self.df[col].nunique()\n",
    "            unique_values = self.df[col].dropna().unique()\n",
    "            \n",
    "            # Data type and pattern detection\n",
    "            col_info = {\n",
    "                'column': col,\n",
    "                'null_count': null_count,\n",
    "                'null_pct': null_pct,\n",
    "                'non_null_count': non_null_count,\n",
    "                'unique_count': unique_count,\n",
    "                'data_type': str(self.df[col].dtype),\n",
    "                'sample_values': list(unique_values[:5]) if len(unique_values) > 0 else [],\n",
    "                'pattern_type': self._detect_pattern_type(col, unique_values),\n",
    "                'is_survey_question': self._is_survey_question(col),\n",
    "                'cleanup_recommendation': self._get_cleanup_recommendation(null_pct, unique_count, col)\n",
    "            }\n",
    "            \n",
    "            column_details.append(col_info)\n",
    "        \n",
    "        # Sort by data quality (null percentage)\n",
    "        column_details.sort(key=lambda x: x['null_pct'])\n",
    "        \n",
    "        self.analysis_results['column_details'] = column_details\n",
    "        \n",
    "        return column_details\n",
    "    \n",
    "    def _detect_pattern_type(self, col_name, unique_values):\n",
    "        \"\"\"Detect the type of data pattern in the column\"\"\"\n",
    "        \n",
    "        if len(unique_values) == 0:\n",
    "            return \"empty\"\n",
    "        \n",
    "        # Convert to strings for analysis\n",
    "        str_values = [str(val).lower() for val in unique_values if pd.notna(val)]\n",
    "        \n",
    "        # Yes/No questions\n",
    "        yes_no_indicators = ['yes', 'no', 'y', 'n', 'true', 'false', '1', '0']\n",
    "        if len(str_values) <= 10 and any(val in yes_no_indicators for val in str_values):\n",
    "            return \"yes_no_question\"\n",
    "        \n",
    "        # Email pattern\n",
    "        if 'email' in col_name.lower():\n",
    "            return \"email_field\"\n",
    "        \n",
    "        # Name pattern\n",
    "        if any(word in col_name.lower() for word in ['name', 'first', 'last']):\n",
    "            return \"name_field\"\n",
    "        \n",
    "        # Date pattern\n",
    "        if any(word in col_name.lower() for word in ['date', 'time', 'created', 'submitted']):\n",
    "            return \"date_field\"\n",
    "        \n",
    "        # Categorical with few options\n",
    "        if len(unique_values) <= 20:\n",
    "            return \"categorical\"\n",
    "        \n",
    "        # Free text\n",
    "        if len(unique_values) > 100:\n",
    "            return \"free_text\"\n",
    "        \n",
    "        # Numeric\n",
    "        if all(str(val).replace('.', '').isdigit() for val in str_values[:5]):\n",
    "            return \"numeric\"\n",
    "        \n",
    "        return \"mixed\"\n",
    "    \n",
    "    def _is_survey_question(self, col_name):\n",
    "        \"\"\"Determine if column represents a survey question\"\"\"\n",
    "        \n",
    "        question_indicators = [\n",
    "            'to_get', 'to_learn', 'to_gain', 'to_help', 'to_access', 'to_improve',\n",
    "            'what_', 'how_', 'do_you', 'have_you', 'did_you', 'are_you', 'will_you',\n",
    "            'please_explain', 'other'\n",
    "        ]\n",
    "        \n",
    "        col_lower = col_name.lower()\n",
    "        return any(indicator in col_lower for indicator in question_indicators)\n",
    "    \n",
    "    def _get_cleanup_recommendation(self, null_pct, unique_count, col_name):\n",
    "        \"\"\"Get cleanup recommendation for each column\"\"\"\n",
    "        \n",
    "        if null_pct >= 95:\n",
    "            return \"DELETE - Almost empty\"\n",
    "        elif null_pct >= 90:\n",
    "            return \"REVIEW - Very sparse data\"\n",
    "        elif null_pct >= 75:\n",
    "            return \"EVALUATE - Low response rate\"\n",
    "        elif unique_count <= 1 and null_pct < 50:\n",
    "            return \"DELETE - No variation\"\n",
    "        elif 'other' in col_name.lower() and null_pct > 80:\n",
    "            return \"CONSIDER_DELETE - Sparse other field\"\n",
    "        else:\n",
    "            return \"KEEP - Good quality\"\n",
    "    \n",
    "    def categorize_columns(self):\n",
    "        \"\"\"Categorize columns by their purpose and quality\"\"\"\n",
    "        \n",
    "        print(f\"\\nüìä COLUMN CATEGORIZATION\")\n",
    "        print(\"=\"*40)\n",
    "        \n",
    "        categories = {\n",
    "            'participant_info': [],\n",
    "            'survey_questions': [],\n",
    "            'metadata': [],\n",
    "            'empty_columns': [],\n",
    "            'other_responses': [],\n",
    "            'date_fields': [],\n",
    "            'numeric_fields': []\n",
    "        }\n",
    "        \n",
    "        for col_info in self.analysis_results['column_details']:\n",
    "            col = col_info['column']\n",
    "            \n",
    "            if col_info['null_pct'] >= 95:\n",
    "                categories['empty_columns'].append(col_info)\n",
    "            elif col_info['pattern_type'] in ['email_field', 'name_field']:\n",
    "                categories['participant_info'].append(col_info)\n",
    "            elif col_info['is_survey_question']:\n",
    "                categories['survey_questions'].append(col_info)\n",
    "            elif col_info['pattern_type'] == 'date_field':\n",
    "                categories['date_fields'].append(col_info)\n",
    "            elif 'other' in col.lower():\n",
    "                categories['other_responses'].append(col_info)\n",
    "            elif col_info['pattern_type'] == 'numeric':\n",
    "                categories['numeric_fields'].append(col_info)\n",
    "            else:\n",
    "                categories['metadata'].append(col_info)\n",
    "        \n",
    "        # Print categorization summary\n",
    "        for category, columns in categories.items():\n",
    "            if columns:\n",
    "                print(f\"\\n{category.upper().replace('_', ' ')} ({len(columns)} columns):\")\n",
    "                for col_info in columns[:5]:  # Show first 5\n",
    "                    print(f\"   - {col_info['column'][:50]}: {col_info['null_pct']:.1f}% null\")\n",
    "                if len(columns) > 5:\n",
    "                    print(f\"   ... and {len(columns) - 5} more\")\n",
    "        \n",
    "        self.analysis_results['categories'] = categories\n",
    "        return categories\n",
    "    \n",
    "    def analyze_row_patterns(self):\n",
    "        \"\"\"Analyze row completeness patterns\"\"\"\n",
    "        \n",
    "        print(f\"\\nüìã ROW COMPLETENESS ANALYSIS\")\n",
    "        print(\"=\"*40)\n",
    "        \n",
    "        # Calculate completeness for each row\n",
    "        row_completeness = self.df.count(axis=1) / len(self.df.columns) * 100\n",
    "        \n",
    "        # Identify different types of rows\n",
    "        completely_empty = (row_completeness == 0).sum()\n",
    "        mostly_empty = (row_completeness < 10).sum()\n",
    "        sparse = ((row_completeness >= 10) & (row_completeness < 25)).sum()\n",
    "        partial = ((row_completeness >= 25) & (row_completeness < 75)).sum()\n",
    "        mostly_complete = (row_completeness >= 75).sum()\n",
    "        \n",
    "        print(f\"Row completeness distribution:\")\n",
    "        print(f\"   üî¥ Completely empty (0%): {completely_empty}\")\n",
    "        print(f\"   üü† Mostly empty (<10%): {mostly_empty}\")\n",
    "        print(f\"   üü° Sparse (10-25%): {sparse}\")\n",
    "        print(f\"   üîµ Partial (25-75%): {partial}\")\n",
    "        print(f\"   üü¢ Mostly complete (>75%): {mostly_complete}\")\n",
    "        \n",
    "        # Find rows that are candidates for deletion\n",
    "        deletion_candidates = row_completeness[row_completeness < 5]  # Less than 5% complete\n",
    "        \n",
    "        if len(deletion_candidates) > 0:\n",
    "            print(f\"\\nüóëÔ∏è  ROW DELETION CANDIDATES:\")\n",
    "            print(f\"   {len(deletion_candidates)} rows with <5% completeness\")\n",
    "            print(f\"   Sample row indices: {list(deletion_candidates.index[:10])}\")\n",
    "        \n",
    "        self.analysis_results['row_analysis'] = {\n",
    "            'deletion_candidates': len(deletion_candidates),\n",
    "            'deletion_indices': deletion_candidates.index.tolist(),\n",
    "            'completeness_stats': {\n",
    "                'completely_empty': completely_empty,\n",
    "                'mostly_empty': mostly_empty,\n",
    "                'sparse': sparse,\n",
    "                'partial': partial,\n",
    "                'mostly_complete': mostly_complete\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return deletion_candidates\n",
    "    \n",
    "    def identify_cleanup_opportunities(self):\n",
    "        \"\"\"Identify specific cleanup opportunities\"\"\"\n",
    "        \n",
    "        print(f\"\\nüéØ CLEANUP OPPORTUNITIES\")\n",
    "        print(\"=\"*40)\n",
    "        \n",
    "        opportunities = {\n",
    "            'columns_to_delete': [],\n",
    "            'columns_to_review': [],\n",
    "            'rows_to_delete': [],\n",
    "            'data_standardization': []\n",
    "        }\n",
    "        \n",
    "        # Column cleanup opportunities\n",
    "        for col_info in self.analysis_results['column_details']:\n",
    "            if col_info['cleanup_recommendation'].startswith('DELETE'):\n",
    "                opportunities['columns_to_delete'].append(col_info)\n",
    "            elif col_info['cleanup_recommendation'].startswith('REVIEW'):\n",
    "                opportunities['columns_to_review'].append(col_info)\n",
    "        \n",
    "        # Row cleanup opportunities\n",
    "        if 'row_analysis' in self.analysis_results:\n",
    "            opportunities['rows_to_delete'] = self.analysis_results['row_analysis']['deletion_indices']\n",
    "        \n",
    "        # Data standardization opportunities\n",
    "        for col_info in self.analysis_results['column_details']:\n",
    "            if col_info['pattern_type'] == 'yes_no_question' and col_info['null_pct'] < 50:\n",
    "                opportunities['data_standardization'].append({\n",
    "                    'column': col_info['column'],\n",
    "                    'type': 'yes_no_standardization',\n",
    "                    'sample_values': col_info['sample_values']\n",
    "                })\n",
    "        \n",
    "        # Print opportunities\n",
    "        print(f\"COLUMN DELETION CANDIDATES ({len(opportunities['columns_to_delete'])}):\")\n",
    "        for col_info in opportunities['columns_to_delete'][:10]:\n",
    "            print(f\"   - {col_info['column'][:50]}: {col_info['null_pct']:.1f}% null\")\n",
    "        \n",
    "        print(f\"\\nCOLUMN REVIEW CANDIDATES ({len(opportunities['columns_to_review'])}):\")\n",
    "        for col_info in opportunities['columns_to_review'][:5]:\n",
    "            print(f\"   - {col_info['column'][:50]}: {col_info['null_pct']:.1f}% null\")\n",
    "        \n",
    "        print(f\"\\nROW DELETION CANDIDATES: {len(opportunities['rows_to_delete'])} rows\")\n",
    "        \n",
    "        print(f\"\\nSTANDARDIZATION OPPORTUNITIES ({len(opportunities['data_standardization'])}):\")\n",
    "        for std_opp in opportunities['data_standardization'][:5]:\n",
    "            print(f\"   - {std_opp['column'][:50]}: {std_opp['sample_values']}\")\n",
    "        \n",
    "        self.analysis_results['cleanup_opportunities'] = opportunities\n",
    "        return opportunities\n",
    "    \n",
    "    def generate_cleanup_script_preview(self):\n",
    "        \"\"\"Generate preview of cleanup actions\"\"\"\n",
    "        \n",
    "        print(f\"\\nüîß CLEANUP SCRIPT PREVIEW\")\n",
    "        print(\"=\"*40)\n",
    "        \n",
    "        opportunities = self.analysis_results.get('cleanup_opportunities', {})\n",
    "        \n",
    "        # Calculate impact\n",
    "        cols_to_delete = len(opportunities.get('columns_to_delete', []))\n",
    "        rows_to_delete = len(opportunities.get('rows_to_delete', []))\n",
    "        \n",
    "        original_size = self.df.shape\n",
    "        new_size = (original_size[0] - rows_to_delete, original_size[1] - cols_to_delete)\n",
    "        \n",
    "        print(f\"PROPOSED CLEANUP IMPACT:\")\n",
    "        print(f\"   Original size: {original_size[0]:,} rows √ó {original_size[1]} columns\")\n",
    "        print(f\"   After cleanup: {new_size[0]:,} rows √ó {new_size[1]} columns\")\n",
    "        print(f\"   Reduction: {rows_to_delete:,} rows ({rows_to_delete/original_size[0]*100:.1f}%), {cols_to_delete} columns ({cols_to_delete/original_size[1]*100:.1f}%)\")\n",
    "        \n",
    "        # Data preservation estimate\n",
    "        original_cells = original_size[0] * original_size[1]\n",
    "        deleted_cells = (rows_to_delete * original_size[1]) + (cols_to_delete * (original_size[0] - rows_to_delete))\n",
    "        meaningful_data_preserved = ((original_cells - deleted_cells) / original_cells) * 100\n",
    "        \n",
    "        print(f\"   Estimated meaningful data preserved: {meaningful_data_preserved:.1f}%\")\n",
    "        \n",
    "        print(f\"\\nRECOMMENDED ACTIONS:\")\n",
    "        print(f\"   1. Delete {cols_to_delete} low-quality columns\")\n",
    "        print(f\"   2. Remove {rows_to_delete} mostly-empty rows\")\n",
    "        print(f\"   3. Standardize {len(opportunities.get('data_standardization', []))} Yes/No columns\")\n",
    "        print(f\"   4. Review {len(opportunities.get('columns_to_review', []))} medium-quality columns\")\n",
    "    \n",
    "    def run_deep_analysis(self):\n",
    "        \"\"\"Run complete deep dive analysis\"\"\"\n",
    "        \n",
    "        print(\"üî¨ DEEP DIVE DATA PATTERN ANALYZER\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        if not self.load_data():\n",
    "            return False\n",
    "        \n",
    "        # Run all analyses\n",
    "        self.analyze_column_patterns()\n",
    "        self.categorize_columns()\n",
    "        self.analyze_row_patterns()\n",
    "        self.identify_cleanup_opportunities()\n",
    "        self.generate_cleanup_script_preview()\n",
    "        \n",
    "        print(f\"\\n‚úÖ DEEP ANALYSIS COMPLETE!\")\n",
    "        \n",
    "        return self.analysis_results\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run deep dive analysis\"\"\"\n",
    "    \n",
    "    file_path = \"/Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/AS/ASmergedFile.xlsx\"\n",
    "    \n",
    "    analyzer = DeepDiveAnalyzer(file_path)\n",
    "    results = analyzer.run_deep_analysis()\n",
    "    \n",
    "    if results:\n",
    "        print(f\"\\nüéØ NEXT STEPS:\")\n",
    "        print(\"1. Review the cleanup opportunities above\")\n",
    "        print(\"2. Decide which columns/rows to remove\")\n",
    "        print(\"3. Create and run the cleanup script\")\n",
    "        print(\"4. Validate the cleaned dataset\")\n",
    "        print(\"5. Proceed with learner journey analysis\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67b22bce-a409-4839-bd96-d9d0ea9d2a41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Input: ASmergedFile.xlsx\n",
      "üìÇ Output: ASmergedFile_CLEANED.xlsx\n",
      "üßπ TARGETED DATA CLEANUP - ASmergedFile.xlsx\n",
      "============================================================\n",
      "‚úÖ Data loaded: (3242, 113)\n",
      "\n",
      "üóëÔ∏è  DELETING PROBLEMATIC COLUMNS\n",
      "========================================\n",
      "Deleting 25 columns:\n",
      "   - other5: 95.3% null\n",
      "   - please_explain_the_reason_you_chose_other1: 96.9% null\n",
      "   - please_explain_the_reason_you_chose_other2: 97.8% null\n",
      "   - please_explain_the_reason_you_chose_other3: 98.7% null\n",
      "   - please_explain_the_reason_you_chose_other4: 99.2% null\n",
      "   - please_explain_the_reason_you_chose_other5: 99.6% null\n",
      "   - recognizing_letters: 98.0% null\n",
      "   - recognizing_numbers_counting: 98.0% null\n",
      "   - x: 99.5% null\n",
      "   - problem_solving: 94.2% null\n",
      "   - social_skills: 92.3% null\n",
      "   - creativity_ex_art_storytelling: 92.6% null\n",
      "   - vocabulary_ex_communicating_wants_and_needs: 92.9% null\n",
      "   - internet_safety_and_awareness: 94.2% null\n",
      "   - parenting_with_screen_time: 91.8% null\n",
      "   - other6: 88.1% null\n",
      "   - response_html_beta: 100.0% null\n",
      "   - response_html: 100.0% null\n",
      "   - response_text: 100.0% null\n",
      "   - response: 100.0% null\n",
      "   - file_list: 100.0% null\n",
      "   - unprotected_file_list: 100.0% null\n",
      "   - resume_email: 100.0% null\n",
      "   - referrer: 0.0% null\n",
      "   - ip_address: 0.0% null\n",
      "‚úÖ Deleted 25 columns\n",
      "   New shape: (3242, 88)\n",
      "\n",
      "üîß STANDARDIZING SURVEY RESPONSES\n",
      "========================================\n",
      "   ‚úÖ other: 8 ‚Üí 4 unique values\n",
      "   ‚úÖ health_and_wellness: 4 ‚Üí 3 unique values\n",
      "   ‚úÖ data_entry: 6 ‚Üí 3 unique values\n",
      "   ‚úÖ telehealth_ex_communicating_with_a_health_professi: 6 ‚Üí 4 unique values\n",
      "   ‚úÖ since_the_end_of_your_class_have_you_participated_: 5 ‚Üí 3 unique values\n",
      "   ‚úÖ do_you_feel_that_the_skills_you_learned_or_the_con: 6 ‚Üí 5 unique values\n",
      "   ‚úÖ work_from_home: 7 ‚Üí 4 unique values\n",
      "   ‚úÖ have_you_opened_a_small_business_since_taking_the_: 13 ‚Üí 11 unique values\n",
      "   ‚úÖ internet_safety_and_privacy_awareness: 5 ‚Üí 3 unique values\n",
      "   ‚úÖ has_your_childs_academic_performance_changed_becau: 14 ‚Üí 13 unique values\n",
      "   ‚úÖ how_has_your_involvement_in_your_childs_education_: 9 ‚Üí 8 unique values\n",
      "   ‚úÖ do_you_have_quality_internet_that_you_can_afford: 13 ‚Üí 11 unique values\n",
      "‚úÖ Standardized responses in 59 columns\n",
      "   Total response values standardized: 43,586\n",
      "\n",
      "üìß STANDARDIZING EMAIL ADDRESSES\n",
      "========================================\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "\"value\" parameter must be a scalar, dict or Series, but you passed a \"Index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/q6/v__5dqvn4fsgnld9cp_x0v1m0000gn/T/ipykernel_50272/783350785.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/q6/v__5dqvn4fsgnld9cp_x0v1m0000gn/T/ipykernel_50272/783350785.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;31m# Initialize cleaner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0mcleaner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTargetedDataCleaner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Run complete cleanup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0msuccess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcleaner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_complete_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\u001b[0m\u001b[0;34m\\n‚ú® SUCCESS! Your dataset is now clean and ready for learner journey analysis!\u001b[0m\u001b[0;34m\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/q6/v__5dqvn4fsgnld9cp_x0v1m0000gn/T/ipykernel_50272/783350785.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, output_path)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;31m# Run all cleanup steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_problematic_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstandardize_responses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstandardize_email_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstandardize_name_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstandardize_date_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_year_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/q6/v__5dqvn4fsgnld9cp_x0v1m0000gn/T/ipykernel_50272/783350785.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'email_standardized'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;31m# Create participant IDs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'participant_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'email_standardized'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'participant_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;31m# Extract domain for analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'email_domain'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'email_standardized'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'@([^.]+\\.[^.]+)$'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, value, method, axis, inplace, limit, downcast)\u001b[0m\n\u001b[1;32m   7339\u001b[0m                     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7340\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7341\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7342\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7343\u001b[0;31m                     raise TypeError(\n\u001b[0m\u001b[1;32m   7344\u001b[0m                         \u001b[0;34m'\"value\" parameter must be a scalar, dict '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7345\u001b[0m                         \u001b[0;34m\"or Series, but you passed a \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7346\u001b[0m                         \u001b[0;34mf'\u001b[0m\u001b[0;34m\"\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\"\u001b[0m\u001b[0;34m'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: \"value\" parameter must be a scalar, dict or Series, but you passed a \"Index\""
     ]
    }
   ],
   "source": [
    "#smart cleaning \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "class TargetedDataCleaner:\n",
    "    \"\"\"\n",
    "    Targeted cleanup script for ASmergedFile.xlsx\n",
    "    Based on deep dive analysis findings\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.df = None\n",
    "        self.original_df = None\n",
    "        self.cleanup_stats = {\n",
    "            'original_shape': None,\n",
    "            'final_shape': None,\n",
    "            'columns_deleted': 0,\n",
    "            'rows_deleted': 0,\n",
    "            'responses_standardized': 0,\n",
    "            'emails_cleaned': 0,\n",
    "            'dates_standardized': 0\n",
    "        }\n",
    "        \n",
    "        # Configure logging\n",
    "        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"Load the dataset\"\"\"\n",
    "        try:\n",
    "            self.df = pd.read_excel(self.file_path)\n",
    "            self.original_df = self.df.copy()  # Backup\n",
    "            self.cleanup_stats['original_shape'] = self.df.shape\n",
    "            print(f\"‚úÖ Data loaded: {self.df.shape}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading data: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def delete_problematic_columns(self):\n",
    "        \"\"\"Delete columns identified as problematic in deep dive analysis\"\"\"\n",
    "        \n",
    "        print(\"\\nüóëÔ∏è  DELETING PROBLEMATIC COLUMNS\")\n",
    "        print(\"=\"*40)\n",
    "        \n",
    "        # Columns to delete (>95% null from deep dive analysis)\n",
    "        columns_to_delete = [\n",
    "            'other5',\n",
    "            'please_explain_the_reason_you_chose_other1',\n",
    "            'please_explain_the_reason_you_chose_other2',\n",
    "            'please_explain_the_reason_you_chose_other3',\n",
    "            'please_explain_the_reason_you_chose_other4',\n",
    "            'please_explain_the_reason_you_chose_other5',\n",
    "            'recognizing_letters',\n",
    "            'recognizing_numbers_counting',\n",
    "            'x',\n",
    "            'are_the_skills_you_learned_in_the_tgh_course_helpi',\n",
    "            'problem_solving',\n",
    "            'social_skills',\n",
    "            'creativity_ex_art_storytelling',\n",
    "            'vocabulary_ex_communicating_wants_and_needs',\n",
    "            'internet_safety_and_awareness',\n",
    "            'parenting_with_screen_time',\n",
    "            'other6'\n",
    "        ]\n",
    "        \n",
    "        # Additional metadata columns to delete (keeping date-related ones)\n",
    "        metadata_to_delete = [\n",
    "            'response_html_beta',\n",
    "            'response_html', \n",
    "            'response_text',\n",
    "            'response',\n",
    "            'file_list',\n",
    "            'unprotected_file_list',\n",
    "            'resume_email',\n",
    "            'referrer',\n",
    "            'ip_address'\n",
    "        ]\n",
    "        \n",
    "        all_columns_to_delete = columns_to_delete + metadata_to_delete\n",
    "        \n",
    "        # Filter to only existing columns\n",
    "        existing_columns_to_delete = [col for col in all_columns_to_delete if col in self.df.columns]\n",
    "        \n",
    "        print(f\"Deleting {len(existing_columns_to_delete)} columns:\")\n",
    "        for col in existing_columns_to_delete:\n",
    "            null_pct = (self.df[col].isnull().sum() / len(self.df)) * 100\n",
    "            print(f\"   - {col[:50]}: {null_pct:.1f}% null\")\n",
    "        \n",
    "        # Delete columns\n",
    "        self.df.drop(columns=existing_columns_to_delete, inplace=True, errors='ignore')\n",
    "        self.cleanup_stats['columns_deleted'] = len(existing_columns_to_delete)\n",
    "        \n",
    "        print(f\"‚úÖ Deleted {len(existing_columns_to_delete)} columns\")\n",
    "        print(f\"   New shape: {self.df.shape}\")\n",
    "    \n",
    "    def standardize_responses(self):\n",
    "        \"\"\"Standardize survey responses across columns\"\"\"\n",
    "        \n",
    "        print(\"\\nüîß STANDARDIZING SURVEY RESPONSES\")\n",
    "        print(\"=\"*40)\n",
    "        \n",
    "        # Define standardization mappings\n",
    "        response_mappings = {\n",
    "            # Yes variations\n",
    "            'Yes': [\n",
    "                'yes', 'y', 'Yes', 'YES', 'Wi', 'S√≠', 'Si', 's√≠', 'si',\n",
    "                'yeah', 'yep', 'true', 'True', '1'\n",
    "            ],\n",
    "            # No variations  \n",
    "            'No': [\n",
    "                'no', 'n', 'No', 'NO', 'Non', 'non', \n",
    "                'false', 'False', '0'\n",
    "            ],\n",
    "            # Not Applicable variations (separate category)\n",
    "            'Not Applicable': [\n",
    "                'Not applicable', 'not applicable', 'Not Applicable', 'NOT APPLICABLE',\n",
    "                'No Aplica', 'no aplica', 'N/A', 'n/a', 'NA', 'na'\n",
    "            ],\n",
    "            # Not Available variations (separate category)\n",
    "            'Not Available': [\n",
    "                'Not Available', 'not available', 'Not available', 'NOT AVAILABLE'\n",
    "            ],\n",
    "            # Unknown/Prefer not to answer variations\n",
    "            'Prefer not to answer': [\n",
    "                \"I don't know\", \"I don&#39;t know\", \"I don&#39;t know and/or I don&#39;t want to answer\",\n",
    "                \"I don't know and/or don't want to answer\", \"Prefer not to answer\", \n",
    "                \"I prefer not to answer\", \"prefer not to answer\", \"Don't know\", \"don't know\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Apply standardization to likely survey columns\n",
    "        survey_columns = []\n",
    "        standardized_count = 0\n",
    "        \n",
    "        for col in self.df.columns:\n",
    "            # Skip clearly non-survey columns\n",
    "            if any(skip in col.lower() for skip in ['date', 'time', 'id', 'url', 'name', 'email']):\n",
    "                continue\n",
    "            \n",
    "            # Check if column has survey-like responses\n",
    "            if self.df[col].dtype == 'object':\n",
    "                unique_vals = self.df[col].dropna().unique()\n",
    "                \n",
    "                if len(unique_vals) <= 20:  # Likely categorical\n",
    "                    survey_columns.append(col)\n",
    "                    \n",
    "                    # Apply standardization\n",
    "                    original_unique = len(unique_vals)\n",
    "                    \n",
    "                    for standard_response, variations in response_mappings.items():\n",
    "                        mask = self.df[col].isin(variations)\n",
    "                        if mask.any():\n",
    "                            self.df.loc[mask, col] = standard_response\n",
    "                            standardized_count += mask.sum()\n",
    "                    \n",
    "                    new_unique = self.df[col].nunique()\n",
    "                    \n",
    "                    if new_unique < original_unique:\n",
    "                        print(f\"   ‚úÖ {col[:50]}: {original_unique} ‚Üí {new_unique} unique values\")\n",
    "        \n",
    "        self.cleanup_stats['responses_standardized'] = standardized_count\n",
    "        print(f\"‚úÖ Standardized responses in {len(survey_columns)} columns\")\n",
    "        print(f\"   Total response values standardized: {standardized_count:,}\")\n",
    "    \n",
    "    def standardize_email_column(self):\n",
    "        \"\"\"Standardize email addresses and create participant IDs\"\"\"\n",
    "        \n",
    "        print(\"\\nüìß STANDARDIZING EMAIL ADDRESSES\")\n",
    "        print(\"=\"*40)\n",
    "        \n",
    "        if 'email_address' not in self.df.columns:\n",
    "            print(\"‚ùå No email_address column found\")\n",
    "            return\n",
    "        \n",
    "        original_emails = self.df['email_address'].count()\n",
    "        \n",
    "        # Clean and standardize emails\n",
    "        self.df['email_standardized'] = self.df['email_address'].astype(str).str.lower().str.strip()\n",
    "        \n",
    "        # Remove invalid entries\n",
    "        self.df['email_standardized'] = self.df['email_standardized'].replace(['nan', 'none', 'n/a', ''], np.nan)\n",
    "        \n",
    "        # Basic email validation\n",
    "        email_pattern = re.compile(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$')\n",
    "        \n",
    "        valid_emails = 0\n",
    "        for idx, email in self.df['email_standardized'].items():\n",
    "            if pd.notna(email) and email_pattern.match(str(email)):\n",
    "                valid_emails += 1\n",
    "            else:\n",
    "                self.df.at[idx, 'email_standardized'] = np.nan\n",
    "        \n",
    "        # Create participant IDs\n",
    "        self.df['participant_id'] = self.df['email_standardized'].fillna('participant_' + self.df.index.astype(str))\n",
    "        \n",
    "        # Extract domain for analysis\n",
    "        self.df['email_domain'] = self.df['email_standardized'].str.extract(r'@([^.]+\\.[^.]+)$')\n",
    "        \n",
    "        cleaned_emails = self.df['email_standardized'].count()\n",
    "        \n",
    "        print(f\"   Original emails: {original_emails:,}\")\n",
    "        print(f\"   Valid emails after cleaning: {cleaned_emails:,}\")\n",
    "        print(f\"   Emails cleaned/removed: {original_emails - cleaned_emails:,}\")\n",
    "        \n",
    "        # Show top domains\n",
    "        if cleaned_emails > 0:\n",
    "            top_domains = self.df['email_domain'].value_counts().head()\n",
    "            print(f\"   Top email domains:\")\n",
    "            for domain, count in top_domains.items():\n",
    "                print(f\"      {domain}: {count}\")\n",
    "        \n",
    "        self.cleanup_stats['emails_cleaned'] = cleaned_emails\n",
    "    \n",
    "    def standardize_name_columns(self):\n",
    "        \"\"\"Standardize name columns\"\"\"\n",
    "        \n",
    "        print(\"\\nüë§ STANDARDIZING NAME COLUMNS\")\n",
    "        print(\"=\"*40)\n",
    "        \n",
    "        name_columns = ['first_name', 'last_name']\n",
    "        \n",
    "        for col in name_columns:\n",
    "            if col in self.df.columns:\n",
    "                # Clean names\n",
    "                self.df[col] = self.df[col].astype(str).str.strip().str.title()\n",
    "                \n",
    "                # Remove invalid entries\n",
    "                self.df[col] = self.df[col].replace(['Nan', 'None', 'N/A', ''], np.nan)\n",
    "                \n",
    "                valid_names = self.df[col].count()\n",
    "                print(f\"   ‚úÖ {col}: {valid_names:,} valid entries\")\n",
    "        \n",
    "        # Create full name column\n",
    "        if 'first_name' in self.df.columns and 'last_name' in self.df.columns:\n",
    "            self.df['full_name'] = (\n",
    "                self.df['first_name'].fillna('') + ' ' + self.df['last_name'].fillna('')\n",
    "            ).str.strip()\n",
    "            \n",
    "            # Replace empty strings with NaN\n",
    "            self.df['full_name'] = self.df['full_name'].replace('', np.nan)\n",
    "            \n",
    "            full_names = self.df['full_name'].count()\n",
    "            print(f\"   ‚úÖ Created full_name column: {full_names:,} entries\")\n",
    "    \n",
    "    def standardize_date_columns(self):\n",
    "        \"\"\"Standardize date columns\"\"\"\n",
    "        \n",
    "        print(\"\\nüìÖ STANDARDIZING DATE COLUMNS\")\n",
    "        print(\"=\"*40)\n",
    "        \n",
    "        date_columns = [\n",
    "            'submitted_date', 'creation_date', 'modified_date', 'completion_time'\n",
    "        ]\n",
    "        \n",
    "        dates_standardized = 0\n",
    "        \n",
    "        for col in date_columns:\n",
    "            if col in self.df.columns:\n",
    "                try:\n",
    "                    # Convert to datetime\n",
    "                    self.df[f\"{col}_standardized\"] = pd.to_datetime(self.df[col], errors='coerce')\n",
    "                    \n",
    "                    valid_dates = self.df[f\"{col}_standardized\"].count()\n",
    "                    dates_standardized += valid_dates\n",
    "                    \n",
    "                    if valid_dates > 0:\n",
    "                        date_range = {\n",
    "                            'min': self.df[f\"{col}_standardized\"].min(),\n",
    "                            'max': self.df[f\"{col}_standardized\"].max()\n",
    "                        }\n",
    "                        print(f\"   ‚úÖ {col}: {valid_dates:,} valid dates ({date_range['min'].date()} to {date_range['max'].date()})\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  {col}: Could not standardize - {str(e)}\")\n",
    "        \n",
    "        self.cleanup_stats['dates_standardized'] = dates_standardized\n",
    "    \n",
    "    def handle_year_column(self):\n",
    "        \"\"\"Standardize year column\"\"\"\n",
    "        \n",
    "        print(\"\\nüìä STANDARDIZING YEAR COLUMN\")\n",
    "        print(\"=\"*40)\n",
    "        \n",
    "        if 'year' in self.df.columns:\n",
    "            # Convert to numeric\n",
    "            self.df['survey_year'] = pd.to_numeric(self.df['year'], errors='coerce')\n",
    "            \n",
    "            # Validate reasonable year range\n",
    "            valid_years = self.df['survey_year'].between(2018, 2024)\n",
    "            invalid_years = (~valid_years) & (self.df['survey_year'].notna())\n",
    "            \n",
    "            if invalid_years.any():\n",
    "                print(f\"   ‚ö†Ô∏è  Found {invalid_years.sum()} invalid years, setting to NaN\")\n",
    "                self.df.loc[invalid_years, 'survey_year'] = np.nan\n",
    "            \n",
    "            year_distribution = self.df['survey_year'].value_counts().sort_index()\n",
    "            print(f\"   ‚úÖ Year distribution:\")\n",
    "            for year, count in year_distribution.items():\n",
    "                if pd.notna(year):\n",
    "                    print(f\"      {int(year)}: {count:,} responses\")\n",
    "    \n",
    "    def final_quality_check(self):\n",
    "        \"\"\"Perform final quality assessment\"\"\"\n",
    "        \n",
    "        print(\"\\nüìä FINAL QUALITY ASSESSMENT\")\n",
    "        print(\"=\"*40)\n",
    "        \n",
    "        # Calculate final statistics\n",
    "        final_shape = self.df.shape\n",
    "        original_shape = self.cleanup_stats['original_shape']\n",
    "        \n",
    "        # Missing data assessment\n",
    "        total_cells = final_shape[0] * final_shape[1]\n",
    "        missing_cells = self.df.isnull().sum().sum()\n",
    "        missing_percentage = (missing_cells / total_cells) * 100\n",
    "        \n",
    "        # Column quality distribution\n",
    "        missing_by_col = (self.df.isnull().sum() / len(self.df)) * 100\n",
    "        excellent_cols = (missing_by_col < 10).sum()\n",
    "        good_cols = ((missing_by_col >= 10) & (missing_by_col < 25)).sum()\n",
    "        fair_cols = ((missing_by_col >= 25) & (missing_by_col < 50)).sum()\n",
    "        poor_cols = (missing_by_col >= 50).sum()\n",
    "        \n",
    "        print(f\"Dataset transformation:\")\n",
    "        print(f\"   Original: {original_shape[0]:,} rows √ó {original_shape[1]} columns\")\n",
    "        print(f\"   Final: {final_shape[0]:,} rows √ó {final_shape[1]} columns\")\n",
    "        print(f\"   Reduction: {original_shape[1] - final_shape[1]} columns ({((original_shape[1] - final_shape[1])/original_shape[1]*100):.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nData quality improvement:\")\n",
    "        print(f\"   Overall missing data: {missing_percentage:.1f}%\")\n",
    "        print(f\"   Excellent columns (<10% null): {excellent_cols}\")\n",
    "        print(f\"   Good columns (10-25% null): {good_cols}\")\n",
    "        print(f\"   Fair columns (25-50% null): {fair_cols}\")\n",
    "        print(f\"   Poor columns (>50% null): {poor_cols}\")\n",
    "        \n",
    "        # Quality score\n",
    "        quality_score = ((excellent_cols + good_cols) / final_shape[1]) * 100\n",
    "        print(f\"   üìä Data quality score: {quality_score:.1f}%\")\n",
    "        \n",
    "        return quality_score\n",
    "    \n",
    "    def save_cleaned_data(self, output_path):\n",
    "        \"\"\"Save the cleaned dataset\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Save as Excel with multiple sheets\n",
    "            with pd.ExcelWriter(output_path) as writer:\n",
    "                # Main cleaned data\n",
    "                self.df.to_excel(writer, sheet_name='Cleaned_Data', index=False)\n",
    "                \n",
    "                # Cleanup statistics\n",
    "                stats_df = pd.DataFrame([\n",
    "                    {'Metric': 'Original Rows', 'Value': self.cleanup_stats['original_shape'][0]},\n",
    "                    {'Metric': 'Final Rows', 'Value': self.df.shape[0]},\n",
    "                    {'Metric': 'Original Columns', 'Value': self.cleanup_stats['original_shape'][1]},\n",
    "                    {'Metric': 'Final Columns', 'Value': self.df.shape[1]},\n",
    "                    {'Metric': 'Columns Deleted', 'Value': self.cleanup_stats['columns_deleted']},\n",
    "                    {'Metric': 'Responses Standardized', 'Value': self.cleanup_stats['responses_standardized']},\n",
    "                    {'Metric': 'Emails Cleaned', 'Value': self.cleanup_stats['emails_cleaned']},\n",
    "                    {'Metric': 'Dates Standardized', 'Value': self.cleanup_stats['dates_standardized']}\n",
    "                ])\n",
    "                stats_df.to_excel(writer, sheet_name='Cleanup_Statistics', index=False)\n",
    "                \n",
    "                # Original vs cleaned column comparison\n",
    "                original_cols = pd.DataFrame({'Original_Columns': self.original_df.columns})\n",
    "                cleaned_cols = pd.DataFrame({'Cleaned_Columns': self.df.columns})\n",
    "                \n",
    "                # Pad to same length\n",
    "                max_len = max(len(original_cols), len(cleaned_cols))\n",
    "                original_cols = original_cols.reindex(range(max_len))\n",
    "                cleaned_cols = cleaned_cols.reindex(range(max_len))\n",
    "                \n",
    "                comparison_df = pd.concat([original_cols, cleaned_cols], axis=1)\n",
    "                comparison_df.to_excel(writer, sheet_name='Column_Comparison', index=False)\n",
    "            \n",
    "            print(f\"‚úÖ Cleaned data saved to: {output_path}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error saving data: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def run_complete_cleanup(self, output_path):\n",
    "        \"\"\"Run the complete cleanup process\"\"\"\n",
    "        \n",
    "        print(\"üßπ TARGETED DATA CLEANUP - ASmergedFile.xlsx\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        if not self.load_data():\n",
    "            return False\n",
    "        \n",
    "        # Run all cleanup steps\n",
    "        self.delete_problematic_columns()\n",
    "        self.standardize_responses()\n",
    "        self.standardize_email_column()\n",
    "        self.standardize_name_columns()\n",
    "        self.standardize_date_columns()\n",
    "        self.handle_year_column()\n",
    "        \n",
    "        # Final assessment\n",
    "        quality_score = self.final_quality_check()\n",
    "        \n",
    "        # Save results\n",
    "        success = self.save_cleaned_data(output_path)\n",
    "        \n",
    "        if success:\n",
    "            print(f\"\\nüéâ CLEANUP COMPLETED SUCCESSFULLY!\")\n",
    "            print(f\"üìÅ Output file: {output_path}\")\n",
    "            print(f\"üìä Final quality score: {quality_score:.1f}%\")\n",
    "            \n",
    "            print(f\"\\nüéØ SUMMARY OF IMPROVEMENTS:\")\n",
    "            print(f\"   ‚úÖ Deleted {self.cleanup_stats['columns_deleted']} problematic columns\")\n",
    "            print(f\"   ‚úÖ Standardized {self.cleanup_stats['responses_standardized']:,} survey responses\")\n",
    "            print(f\"   ‚úÖ Cleaned {self.cleanup_stats['emails_cleaned']:,} email addresses\")\n",
    "            print(f\"   ‚úÖ Standardized {self.cleanup_stats['dates_standardized']:,} date entries\")\n",
    "            print(f\"   ‚úÖ Created participant_id for learner journey tracking\")\n",
    "            \n",
    "            print(f\"\\nüöÄ READY FOR ANALYSIS:\")\n",
    "            print(\"   ‚Ä¢ Clean, standardized survey responses\")\n",
    "            print(\"   ‚Ä¢ Participant IDs for joining with other datasets\")\n",
    "            print(\"   ‚Ä¢ Proper date formatting for temporal analysis\")\n",
    "            print(\"   ‚Ä¢ Reduced dataset size for efficient Tableau processing\")\n",
    "            \n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run targeted cleanup\"\"\"\n",
    "    \n",
    "    # File paths\n",
    "    input_file = \"/Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/AS/ASmergedFile.xlsx\"\n",
    "    output_file = \"/Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/AS/ASmergedFile_CLEANED.xlsx\"\n",
    "    \n",
    "    print(f\"üìÇ Input: {input_file.split('/')[-1]}\")\n",
    "    print(f\"üìÇ Output: {output_file.split('/')[-1]}\")\n",
    "    \n",
    "    # Initialize cleaner\n",
    "    cleaner = TargetedDataCleaner(input_file)\n",
    "    \n",
    "    # Run complete cleanup\n",
    "    success = cleaner.run_complete_cleanup(output_file)\n",
    "    \n",
    "    if success:\n",
    "        print(f\"\\n‚ú® SUCCESS! Your dataset is now clean and ready for learner journey analysis!\")\n",
    "    \n",
    "    return success\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c35aaced-fdc6-4d80-b397-c56d5286f030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Input: ASmergedFile.xlsx\n",
      "üìÇ Output: ASmergedFile_CLEANED.xlsx\n",
      "üßπ TARGETED DATA CLEANUP - ASmergedFile.xlsx\n",
      "============================================================\n",
      "‚úÖ Data loaded: (3242, 113)\n",
      "\n",
      "üóëÔ∏è  DELETING PROBLEMATIC COLUMNS\n",
      "========================================\n",
      "Deleting 25 columns:\n",
      "   - other5: 95.3% null\n",
      "   - please_explain_the_reason_you_chose_other1: 96.9% null\n",
      "   - please_explain_the_reason_you_chose_other2: 97.8% null\n",
      "   - please_explain_the_reason_you_chose_other3: 98.7% null\n",
      "   - please_explain_the_reason_you_chose_other4: 99.2% null\n",
      "   - please_explain_the_reason_you_chose_other5: 99.6% null\n",
      "   - recognizing_letters: 98.0% null\n",
      "   - recognizing_numbers_counting: 98.0% null\n",
      "   - x: 99.5% null\n",
      "   - problem_solving: 94.2% null\n",
      "   - social_skills: 92.3% null\n",
      "   - creativity_ex_art_storytelling: 92.6% null\n",
      "   - vocabulary_ex_communicating_wants_and_needs: 92.9% null\n",
      "   - internet_safety_and_awareness: 94.2% null\n",
      "   - parenting_with_screen_time: 91.8% null\n",
      "   - other6: 88.1% null\n",
      "   - response_html_beta: 100.0% null\n",
      "   - response_html: 100.0% null\n",
      "   - response_text: 100.0% null\n",
      "   - response: 100.0% null\n",
      "   - file_list: 100.0% null\n",
      "   - unprotected_file_list: 100.0% null\n",
      "   - resume_email: 100.0% null\n",
      "   - referrer: 0.0% null\n",
      "   - ip_address: 0.0% null\n",
      "‚úÖ Deleted 25 columns\n",
      "   New shape: (3242, 88)\n",
      "\n",
      "üîß STANDARDIZING SURVEY RESPONSES\n",
      "========================================\n",
      "   ‚úÖ other: 8 ‚Üí 4 unique values\n",
      "   ‚úÖ health_and_wellness: 4 ‚Üí 3 unique values\n",
      "   ‚úÖ data_entry: 6 ‚Üí 3 unique values\n",
      "   ‚úÖ telehealth_ex_communicating_with_a_health_professi: 6 ‚Üí 4 unique values\n",
      "   ‚úÖ since_the_end_of_your_class_have_you_participated_: 5 ‚Üí 3 unique values\n",
      "   ‚úÖ do_you_feel_that_the_skills_you_learned_or_the_con: 6 ‚Üí 5 unique values\n",
      "   ‚úÖ work_from_home: 7 ‚Üí 4 unique values\n",
      "   ‚úÖ have_you_opened_a_small_business_since_taking_the_: 13 ‚Üí 11 unique values\n",
      "   ‚úÖ internet_safety_and_privacy_awareness: 5 ‚Üí 3 unique values\n",
      "   ‚úÖ has_your_childs_academic_performance_changed_becau: 14 ‚Üí 13 unique values\n",
      "   ‚úÖ how_has_your_involvement_in_your_childs_education_: 9 ‚Üí 8 unique values\n",
      "   ‚úÖ do_you_have_quality_internet_that_you_can_afford: 13 ‚Üí 11 unique values\n",
      "‚úÖ Standardized responses in 59 columns\n",
      "   Total response values standardized: 43,586\n",
      "\n",
      "üìß STANDARDIZING EMAIL ADDRESSES\n",
      "========================================\n",
      "   Original emails: 2,395\n",
      "   Valid emails after cleaning: 2,379\n",
      "   Emails cleaned/removed: 16\n",
      "   Top email domains:\n",
      "      gmail.com: 1669\n",
      "      yahoo.com: 333\n",
      "      hotmail.com: 120\n",
      "      aol.com: 48\n",
      "      icloud.com: 34\n",
      "\n",
      "üë§ STANDARDIZING NAME COLUMNS\n",
      "========================================\n",
      "   ‚úÖ first_name: 2,678 valid entries\n",
      "   ‚úÖ last_name: 2,591 valid entries\n",
      "   ‚úÖ Created full_name column: 2,678 entries\n",
      "\n",
      "üìÖ STANDARDIZING DATE COLUMNS\n",
      "========================================\n",
      "   ‚úÖ submitted_date: 3,189 valid dates (2018-07-09 to 2024-09-04)\n",
      "   ‚úÖ creation_date: 3,242 valid dates (2018-07-09 to 2024-09-04)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q6/v__5dqvn4fsgnld9cp_x0v1m0000gn/T/ipykernel_50272/4271062304.py:273: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  self.df[f\"{col}_standardized\"] = pd.to_datetime(self.df[col], errors='coerce')\n",
      "/var/folders/q6/v__5dqvn4fsgnld9cp_x0v1m0000gn/T/ipykernel_50272/4271062304.py:273: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  self.df[f\"{col}_standardized\"] = pd.to_datetime(self.df[col], errors='coerce')\n",
      "/var/folders/q6/v__5dqvn4fsgnld9cp_x0v1m0000gn/T/ipykernel_50272/4271062304.py:273: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  self.df[f\"{col}_standardized\"] = pd.to_datetime(self.df[col], errors='coerce')\n",
      "/var/folders/q6/v__5dqvn4fsgnld9cp_x0v1m0000gn/T/ipykernel_50272/4271062304.py:273: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  self.df[f\"{col}_standardized\"] = pd.to_datetime(self.df[col], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ modified_date: 3,242 valid dates (2018-07-10 to 2024-09-04)\n",
      "\n",
      "üìä STANDARDIZING YEAR COLUMN\n",
      "========================================\n",
      "   ‚úÖ Year distribution:\n",
      "      2020: 1,568 responses\n",
      "      2021: 952 responses\n",
      "      2023: 161 responses\n",
      "      2024: 164 responses\n",
      "\n",
      "üìä FINAL QUALITY ASSESSMENT\n",
      "========================================\n",
      "Dataset transformation:\n",
      "   Original: 3,242 rows √ó 113 columns\n",
      "   Final: 3,242 rows √ó 97 columns\n",
      "   Reduction: 16 columns (14.2%)\n",
      "\n",
      "Data quality improvement:\n",
      "   Overall missing data: 61.3%\n",
      "   Excellent columns (<10% null): 11\n",
      "   Good columns (10-25% null): 7\n",
      "   Fair columns (25-50% null): 11\n",
      "   Poor columns (>50% null): 68\n",
      "   üìä Data quality score: 18.6%\n",
      "‚úÖ Cleaned data saved to: /Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/AS/ASmergedFile_CLEANED.xlsx\n",
      "\n",
      "üéâ CLEANUP COMPLETED SUCCESSFULLY!\n",
      "üìÅ Output file: /Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/AS/ASmergedFile_CLEANED.xlsx\n",
      "üìä Final quality score: 18.6%\n",
      "\n",
      "üéØ SUMMARY OF IMPROVEMENTS:\n",
      "   ‚úÖ Deleted 25 problematic columns\n",
      "   ‚úÖ Standardized 43,586 survey responses\n",
      "   ‚úÖ Cleaned 2,379 email addresses\n",
      "   ‚úÖ Standardized 9,673 date entries\n",
      "   ‚úÖ Created participant_id for learner journey tracking\n",
      "\n",
      "üöÄ READY FOR ANALYSIS:\n",
      "   ‚Ä¢ Clean, standardized survey responses\n",
      "   ‚Ä¢ Participant IDs for joining with other datasets\n",
      "   ‚Ä¢ Proper date formatting for temporal analysis\n",
      "   ‚Ä¢ Reduced dataset size for efficient Tableau processing\n",
      "\n",
      "‚ú® SUCCESS! Your dataset is now clean and ready for learner journey analysis!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "class TargetedDataCleaner:\n",
    "    \"\"\"\n",
    "    Targeted cleanup script for ASmergedFile.xlsx\n",
    "    Based on deep dive analysis findings\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.df = None\n",
    "        self.original_df = None\n",
    "        self.cleanup_stats = {\n",
    "            'original_shape': None,\n",
    "            'final_shape': None,\n",
    "            'columns_deleted': 0,\n",
    "            'rows_deleted': 0,\n",
    "            'responses_standardized': 0,\n",
    "            'emails_cleaned': 0,\n",
    "            'dates_standardized': 0\n",
    "        }\n",
    "        \n",
    "        # Configure logging\n",
    "        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"Load the dataset\"\"\"\n",
    "        try:\n",
    "            self.df = pd.read_excel(self.file_path)\n",
    "            self.original_df = self.df.copy()  # Backup\n",
    "            self.cleanup_stats['original_shape'] = self.df.shape\n",
    "            print(f\"‚úÖ Data loaded: {self.df.shape}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading data: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def delete_problematic_columns(self):\n",
    "        \"\"\"Delete columns identified as problematic in deep dive analysis\"\"\"\n",
    "        \n",
    "        print(\"\\nüóëÔ∏è  DELETING PROBLEMATIC COLUMNS\")\n",
    "        print(\"=\"*40)\n",
    "        \n",
    "        # Columns to delete (>95% null from deep dive analysis)\n",
    "        columns_to_delete = [\n",
    "            'other5',\n",
    "            'please_explain_the_reason_you_chose_other1',\n",
    "            'please_explain_the_reason_you_chose_other2',\n",
    "            'please_explain_the_reason_you_chose_other3',\n",
    "            'please_explain_the_reason_you_chose_other4',\n",
    "            'please_explain_the_reason_you_chose_other5',\n",
    "            'recognizing_letters',\n",
    "            'recognizing_numbers_counting',\n",
    "            'x',\n",
    "            'are_the_skills_you_learned_in_the_tgh_course_helpi',\n",
    "            'problem_solving',\n",
    "            'social_skills',\n",
    "            'creativity_ex_art_storytelling',\n",
    "            'vocabulary_ex_communicating_wants_and_needs',\n",
    "            'internet_safety_and_awareness',\n",
    "            'parenting_with_screen_time',\n",
    "            'other6'\n",
    "        ]\n",
    "        \n",
    "        # Additional metadata columns to delete (keeping date-related ones)\n",
    "        metadata_to_delete = [\n",
    "            'response_html_beta',\n",
    "            'response_html', \n",
    "            'response_text',\n",
    "            'response',\n",
    "            'file_list',\n",
    "            'unprotected_file_list',\n",
    "            'resume_email',\n",
    "            'referrer',\n",
    "            'ip_address'\n",
    "        ]\n",
    "        \n",
    "        all_columns_to_delete = columns_to_delete + metadata_to_delete\n",
    "        \n",
    "        # Filter to only existing columns\n",
    "        existing_columns_to_delete = [col for col in all_columns_to_delete if col in self.df.columns]\n",
    "        \n",
    "        print(f\"Deleting {len(existing_columns_to_delete)} columns:\")\n",
    "        for col in existing_columns_to_delete:\n",
    "            null_pct = (self.df[col].isnull().sum() / len(self.df)) * 100\n",
    "            print(f\"   - {col[:50]}: {null_pct:.1f}% null\")\n",
    "        \n",
    "        # Delete columns\n",
    "        self.df.drop(columns=existing_columns_to_delete, inplace=True, errors='ignore')\n",
    "        self.cleanup_stats['columns_deleted'] = len(existing_columns_to_delete)\n",
    "        \n",
    "        print(f\"‚úÖ Deleted {len(existing_columns_to_delete)} columns\")\n",
    "        print(f\"   New shape: {self.df.shape}\")\n",
    "    \n",
    "    def standardize_responses(self):\n",
    "        \"\"\"Standardize survey responses across columns\"\"\"\n",
    "        \n",
    "        print(\"\\nüîß STANDARDIZING SURVEY RESPONSES\")\n",
    "        print(\"=\"*40)\n",
    "        \n",
    "        # Define standardization mappings\n",
    "        response_mappings = {\n",
    "            # Yes variations\n",
    "            'Yes': [\n",
    "                'yes', 'y', 'Yes', 'YES', 'Wi', 'S√≠', 'Si', 's√≠', 'si',\n",
    "                'yeah', 'yep', 'true', 'True', '1'\n",
    "            ],\n",
    "            # No variations  \n",
    "            'No': [\n",
    "                'no', 'n', 'No', 'NO', 'Non', 'non', \n",
    "                'false', 'False', '0'\n",
    "            ],\n",
    "            # Not Applicable variations (separate category)\n",
    "            'Not Applicable': [\n",
    "                'Not applicable', 'not applicable', 'Not Applicable', 'NOT APPLICABLE',\n",
    "                'No Aplica', 'no aplica', 'N/A', 'n/a', 'NA', 'na'\n",
    "            ],\n",
    "            # Not Available variations (separate category)\n",
    "            'Not Available': [\n",
    "                'Not Available', 'not available', 'Not available', 'NOT AVAILABLE'\n",
    "            ],\n",
    "            # Unknown/Prefer not to answer variations\n",
    "            'Prefer not to answer': [\n",
    "                \"I don't know\", \"I don&#39;t know\", \"I don&#39;t know and/or I don&#39;t want to answer\",\n",
    "                \"I don't know and/or don't want to answer\", \"Prefer not to answer\", \n",
    "                \"I prefer not to answer\", \"prefer not to answer\", \"Don't know\", \"don't know\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Apply standardization to likely survey columns\n",
    "        survey_columns = []\n",
    "        standardized_count = 0\n",
    "        \n",
    "        for col in self.df.columns:\n",
    "            # Skip clearly non-survey columns\n",
    "            if any(skip in col.lower() for skip in ['date', 'time', 'id', 'url', 'name', 'email']):\n",
    "                continue\n",
    "            \n",
    "            # Check if column has survey-like responses\n",
    "            if self.df[col].dtype == 'object':\n",
    "                unique_vals = self.df[col].dropna().unique()\n",
    "                \n",
    "                if len(unique_vals) <= 20:  # Likely categorical\n",
    "                    survey_columns.append(col)\n",
    "                    \n",
    "                    # Apply standardization\n",
    "                    original_unique = len(unique_vals)\n",
    "                    \n",
    "                    for standard_response, variations in response_mappings.items():\n",
    "                        mask = self.df[col].isin(variations)\n",
    "                        if mask.any():\n",
    "                            self.df.loc[mask, col] = standard_response\n",
    "                            standardized_count += mask.sum()\n",
    "                    \n",
    "                    new_unique = self.df[col].nunique()\n",
    "                    \n",
    "                    if new_unique < original_unique:\n",
    "                        print(f\"   ‚úÖ {col[:50]}: {original_unique} ‚Üí {new_unique} unique values\")\n",
    "        \n",
    "        self.cleanup_stats['responses_standardized'] = standardized_count\n",
    "        print(f\"‚úÖ Standardized responses in {len(survey_columns)} columns\")\n",
    "        print(f\"   Total response values standardized: {standardized_count:,}\")\n",
    "    \n",
    "    def standardize_email_column(self):\n",
    "        \"\"\"Standardize email addresses and create participant IDs\"\"\"\n",
    "        \n",
    "        print(\"\\nüìß STANDARDIZING EMAIL ADDRESSES\")\n",
    "        print(\"=\"*40)\n",
    "        \n",
    "        if 'email_address' not in self.df.columns:\n",
    "            print(\"‚ùå No email_address column found\")\n",
    "            return\n",
    "        \n",
    "        original_emails = self.df['email_address'].count()\n",
    "        \n",
    "        # Clean and standardize emails\n",
    "        self.df['email_standardized'] = self.df['email_address'].astype(str).str.lower().str.strip()\n",
    "        \n",
    "        # Remove invalid entries\n",
    "        self.df['email_standardized'] = self.df['email_standardized'].replace(['nan', 'none', 'n/a', ''], np.nan)\n",
    "        \n",
    "        # Basic email validation\n",
    "        email_pattern = re.compile(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$')\n",
    "        \n",
    "        valid_emails = 0\n",
    "        for idx, email in self.df['email_standardized'].items():\n",
    "            if pd.notna(email) and email_pattern.match(str(email)):\n",
    "                valid_emails += 1\n",
    "            else:\n",
    "                self.df.at[idx, 'email_standardized'] = np.nan\n",
    "        \n",
    "        # Create participant IDs\n",
    "        participant_ids = []\n",
    "        for idx in self.df.index:\n",
    "            email = self.df.at[idx, 'email_standardized']\n",
    "            if pd.notna(email):\n",
    "                participant_ids.append(email)\n",
    "            else:\n",
    "                participant_ids.append(f'participant_{idx}')\n",
    "        \n",
    "        self.df['participant_id'] = participant_ids\n",
    "        \n",
    "        # Extract domain for analysis\n",
    "        self.df['email_domain'] = self.df['email_standardized'].str.extract(r'@([^.]+\\.[^.]+)$')\n",
    "        \n",
    "        cleaned_emails = self.df['email_standardized'].count()\n",
    "        \n",
    "        print(f\"   Original emails: {original_emails:,}\")\n",
    "        print(f\"   Valid emails after cleaning: {cleaned_emails:,}\")\n",
    "        print(f\"   Emails cleaned/removed: {original_emails - cleaned_emails:,}\")\n",
    "        \n",
    "        # Show top domains\n",
    "        if cleaned_emails > 0:\n",
    "            top_domains = self.df['email_domain'].value_counts().head()\n",
    "            print(f\"   Top email domains:\")\n",
    "            for domain, count in top_domains.items():\n",
    "                print(f\"      {domain}: {count}\")\n",
    "        \n",
    "        self.cleanup_stats['emails_cleaned'] = cleaned_emails\n",
    "    \n",
    "    def standardize_name_columns(self):\n",
    "        \"\"\"Standardize name columns\"\"\"\n",
    "        \n",
    "        print(\"\\nüë§ STANDARDIZING NAME COLUMNS\")\n",
    "        print(\"=\"*40)\n",
    "        \n",
    "        name_columns = ['first_name', 'last_name']\n",
    "        \n",
    "        for col in name_columns:\n",
    "            if col in self.df.columns:\n",
    "                # Clean names\n",
    "                self.df[col] = self.df[col].astype(str).str.strip().str.title()\n",
    "                \n",
    "                # Remove invalid entries\n",
    "                self.df[col] = self.df[col].replace(['Nan', 'None', 'N/A', ''], np.nan)\n",
    "                \n",
    "                valid_names = self.df[col].count()\n",
    "                print(f\"   ‚úÖ {col}: {valid_names:,} valid entries\")\n",
    "        \n",
    "        # Create full name column\n",
    "        if 'first_name' in self.df.columns and 'last_name' in self.df.columns:\n",
    "            self.df['full_name'] = (\n",
    "                self.df['first_name'].fillna('') + ' ' + self.df['last_name'].fillna('')\n",
    "            ).str.strip()\n",
    "            \n",
    "            # Replace empty strings with NaN\n",
    "            self.df['full_name'] = self.df['full_name'].replace('', np.nan)\n",
    "            \n",
    "            full_names = self.df['full_name'].count()\n",
    "            print(f\"   ‚úÖ Created full_name column: {full_names:,} entries\")\n",
    "    \n",
    "    def standardize_date_columns(self):\n",
    "        \"\"\"Standardize date columns\"\"\"\n",
    "        \n",
    "        print(\"\\nüìÖ STANDARDIZING DATE COLUMNS\")\n",
    "        print(\"=\"*40)\n",
    "        \n",
    "        date_columns = [\n",
    "            'submitted_date', 'creation_date', 'modified_date', 'completion_time'\n",
    "        ]\n",
    "        \n",
    "        dates_standardized = 0\n",
    "        \n",
    "        for col in date_columns:\n",
    "            if col in self.df.columns:\n",
    "                try:\n",
    "                    # Convert to datetime\n",
    "                    self.df[f\"{col}_standardized\"] = pd.to_datetime(self.df[col], errors='coerce')\n",
    "                    \n",
    "                    valid_dates = self.df[f\"{col}_standardized\"].count()\n",
    "                    dates_standardized += valid_dates\n",
    "                    \n",
    "                    if valid_dates > 0:\n",
    "                        date_range = {\n",
    "                            'min': self.df[f\"{col}_standardized\"].min(),\n",
    "                            'max': self.df[f\"{col}_standardized\"].max()\n",
    "                        }\n",
    "                        print(f\"   ‚úÖ {col}: {valid_dates:,} valid dates ({date_range['min'].date()} to {date_range['max'].date()})\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  {col}: Could not standardize - {str(e)}\")\n",
    "        \n",
    "        self.cleanup_stats['dates_standardized'] = dates_standardized\n",
    "    \n",
    "    def handle_year_column(self):\n",
    "        \"\"\"Standardize year column\"\"\"\n",
    "        \n",
    "        print(\"\\nüìä STANDARDIZING YEAR COLUMN\")\n",
    "        print(\"=\"*40)\n",
    "        \n",
    "        if 'year' in self.df.columns:\n",
    "            # Convert to numeric\n",
    "            self.df['survey_year'] = pd.to_numeric(self.df['year'], errors='coerce')\n",
    "            \n",
    "            # Validate reasonable year range\n",
    "            valid_years = self.df['survey_year'].between(2018, 2024)\n",
    "            invalid_years = (~valid_years) & (self.df['survey_year'].notna())\n",
    "            \n",
    "            if invalid_years.any():\n",
    "                print(f\"   ‚ö†Ô∏è  Found {invalid_years.sum()} invalid years, setting to NaN\")\n",
    "                self.df.loc[invalid_years, 'survey_year'] = np.nan\n",
    "            \n",
    "            year_distribution = self.df['survey_year'].value_counts().sort_index()\n",
    "            print(f\"   ‚úÖ Year distribution:\")\n",
    "            for year, count in year_distribution.items():\n",
    "                if pd.notna(year):\n",
    "                    print(f\"      {int(year)}: {count:,} responses\")\n",
    "    \n",
    "    def final_quality_check(self):\n",
    "        \"\"\"Perform final quality assessment\"\"\"\n",
    "        \n",
    "        print(\"\\nüìä FINAL QUALITY ASSESSMENT\")\n",
    "        print(\"=\"*40)\n",
    "        \n",
    "        # Calculate final statistics\n",
    "        final_shape = self.df.shape\n",
    "        original_shape = self.cleanup_stats['original_shape']\n",
    "        \n",
    "        # Missing data assessment\n",
    "        total_cells = final_shape[0] * final_shape[1]\n",
    "        missing_cells = self.df.isnull().sum().sum()\n",
    "        missing_percentage = (missing_cells / total_cells) * 100\n",
    "        \n",
    "        # Column quality distribution\n",
    "        missing_by_col = (self.df.isnull().sum() / len(self.df)) * 100\n",
    "        excellent_cols = (missing_by_col < 10).sum()\n",
    "        good_cols = ((missing_by_col >= 10) & (missing_by_col < 25)).sum()\n",
    "        fair_cols = ((missing_by_col >= 25) & (missing_by_col < 50)).sum()\n",
    "        poor_cols = (missing_by_col >= 50).sum()\n",
    "        \n",
    "        print(f\"Dataset transformation:\")\n",
    "        print(f\"   Original: {original_shape[0]:,} rows √ó {original_shape[1]} columns\")\n",
    "        print(f\"   Final: {final_shape[0]:,} rows √ó {final_shape[1]} columns\")\n",
    "        print(f\"   Reduction: {original_shape[1] - final_shape[1]} columns ({((original_shape[1] - final_shape[1])/original_shape[1]*100):.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nData quality improvement:\")\n",
    "        print(f\"   Overall missing data: {missing_percentage:.1f}%\")\n",
    "        print(f\"   Excellent columns (<10% null): {excellent_cols}\")\n",
    "        print(f\"   Good columns (10-25% null): {good_cols}\")\n",
    "        print(f\"   Fair columns (25-50% null): {fair_cols}\")\n",
    "        print(f\"   Poor columns (>50% null): {poor_cols}\")\n",
    "        \n",
    "        # Quality score\n",
    "        quality_score = ((excellent_cols + good_cols) / final_shape[1]) * 100\n",
    "        print(f\"   üìä Data quality score: {quality_score:.1f}%\")\n",
    "        \n",
    "        return quality_score\n",
    "    \n",
    "    def save_cleaned_data(self, output_path):\n",
    "        \"\"\"Save the cleaned dataset\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Save as Excel with multiple sheets\n",
    "            with pd.ExcelWriter(output_path) as writer:\n",
    "                # Main cleaned data\n",
    "                self.df.to_excel(writer, sheet_name='Cleaned_Data', index=False)\n",
    "                \n",
    "                # Cleanup statistics\n",
    "                stats_df = pd.DataFrame([\n",
    "                    {'Metric': 'Original Rows', 'Value': self.cleanup_stats['original_shape'][0]},\n",
    "                    {'Metric': 'Final Rows', 'Value': self.df.shape[0]},\n",
    "                    {'Metric': 'Original Columns', 'Value': self.cleanup_stats['original_shape'][1]},\n",
    "                    {'Metric': 'Final Columns', 'Value': self.df.shape[1]},\n",
    "                    {'Metric': 'Columns Deleted', 'Value': self.cleanup_stats['columns_deleted']},\n",
    "                    {'Metric': 'Responses Standardized', 'Value': self.cleanup_stats['responses_standardized']},\n",
    "                    {'Metric': 'Emails Cleaned', 'Value': self.cleanup_stats['emails_cleaned']},\n",
    "                    {'Metric': 'Dates Standardized', 'Value': self.cleanup_stats['dates_standardized']}\n",
    "                ])\n",
    "                stats_df.to_excel(writer, sheet_name='Cleanup_Statistics', index=False)\n",
    "                \n",
    "                # Original vs cleaned column comparison\n",
    "                original_cols = pd.DataFrame({'Original_Columns': self.original_df.columns})\n",
    "                cleaned_cols = pd.DataFrame({'Cleaned_Columns': self.df.columns})\n",
    "                \n",
    "                # Pad to same length\n",
    "                max_len = max(len(original_cols), len(cleaned_cols))\n",
    "                original_cols = original_cols.reindex(range(max_len))\n",
    "                cleaned_cols = cleaned_cols.reindex(range(max_len))\n",
    "                \n",
    "                comparison_df = pd.concat([original_cols, cleaned_cols], axis=1)\n",
    "                comparison_df.to_excel(writer, sheet_name='Column_Comparison', index=False)\n",
    "            \n",
    "            print(f\"‚úÖ Cleaned data saved to: {output_path}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error saving data: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def run_complete_cleanup(self, output_path):\n",
    "        \"\"\"Run the complete cleanup process\"\"\"\n",
    "        \n",
    "        print(\"üßπ TARGETED DATA CLEANUP - ASmergedFile.xlsx\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        if not self.load_data():\n",
    "            return False\n",
    "        \n",
    "        # Run all cleanup steps\n",
    "        self.delete_problematic_columns()\n",
    "        self.standardize_responses()\n",
    "        self.standardize_email_column()\n",
    "        self.standardize_name_columns()\n",
    "        self.standardize_date_columns()\n",
    "        self.handle_year_column()\n",
    "        \n",
    "        # Final assessment\n",
    "        quality_score = self.final_quality_check()\n",
    "        \n",
    "        # Save results\n",
    "        success = self.save_cleaned_data(output_path)\n",
    "        \n",
    "        if success:\n",
    "            print(f\"\\nüéâ CLEANUP COMPLETED SUCCESSFULLY!\")\n",
    "            print(f\"üìÅ Output file: {output_path}\")\n",
    "            print(f\"üìä Final quality score: {quality_score:.1f}%\")\n",
    "            \n",
    "            print(f\"\\nüéØ SUMMARY OF IMPROVEMENTS:\")\n",
    "            print(f\"   ‚úÖ Deleted {self.cleanup_stats['columns_deleted']} problematic columns\")\n",
    "            print(f\"   ‚úÖ Standardized {self.cleanup_stats['responses_standardized']:,} survey responses\")\n",
    "            print(f\"   ‚úÖ Cleaned {self.cleanup_stats['emails_cleaned']:,} email addresses\")\n",
    "            print(f\"   ‚úÖ Standardized {self.cleanup_stats['dates_standardized']:,} date entries\")\n",
    "            print(f\"   ‚úÖ Created participant_id for learner journey tracking\")\n",
    "            \n",
    "            print(f\"\\nüöÄ READY FOR ANALYSIS:\")\n",
    "            print(\"   ‚Ä¢ Clean, standardized survey responses\")\n",
    "            print(\"   ‚Ä¢ Participant IDs for joining with other datasets\")\n",
    "            print(\"   ‚Ä¢ Proper date formatting for temporal analysis\")\n",
    "            print(\"   ‚Ä¢ Reduced dataset size for efficient Tableau processing\")\n",
    "            \n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run targeted cleanup\"\"\"\n",
    "    \n",
    "    # File paths\n",
    "    input_file = \"/Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/AS/ASmergedFile.xlsx\"\n",
    "    output_file = \"/Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/AS/ASmergedFile_CLEANED.xlsx\"\n",
    "    \n",
    "    print(f\"üìÇ Input: {input_file.split('/')[-1]}\")\n",
    "    print(f\"üìÇ Output: {output_file.split('/')[-1]}\")\n",
    "    \n",
    "    # Initialize cleaner\n",
    "    cleaner = TargetedDataCleaner(input_file)\n",
    "    \n",
    "    # Run complete cleanup\n",
    "    success = cleaner.run_complete_cleanup(output_file)\n",
    "    \n",
    "    if success:\n",
    "        print(f\"\\n‚ú® SUCCESS! Your dataset is now clean and ready for learner journey analysis!\")\n",
    "    \n",
    "    return success\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4cac9e4-0fc3-4705-b2fb-6d6465cc8a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ LEARNER JOURNEY MATCHING\n",
      "Annual Survey: ASmergedFile_CLEANED.xlsx\n",
      "Pre-Enrollment: Cleaned_Enhanced_ES_Data_with_Fixed_Years.xlsx\n",
      "Post-Course: Cleaned_Merged_PCS_2cleaned.xlsx\n",
      "Output: Learner_Journey_Matched_Datasets.xlsx\n",
      "üìä LEARNER JOURNEY MATCHER\n",
      "==================================================\n",
      "üìã Loading Annual Survey data...\n",
      "   ‚úÖ Annual Survey: (3242, 97)\n",
      "üìã Loading Pre-Enrollment data...\n",
      "   ‚úÖ Pre-Enrollment: (29700, 130)\n",
      "üìã Loading Post-Course data...\n",
      "   ‚úÖ Post-Course: (7806, 97)\n",
      "\n",
      "üìß CLEANING AND STANDARDIZING EMAILS\n",
      "=============================================\n",
      "   Annual Survey emails...\n",
      "   Pre-Enrollment emails...\n",
      "   Post-Course emails...\n",
      "\n",
      "   üìä Email Cleaning Results:\n",
      "      Annual Survey: 1,727 valid emails (1514 duplicates removed)\n",
      "      Pre-Enrollment: 24,419 valid emails (5280 duplicates removed)\n",
      "      Post-Course: 5,458 valid emails (2347 duplicates removed)\n",
      "\n",
      "üîç FINDING LEARNER MATCHES ACROSS SURVEYS\n",
      "==================================================\n",
      "   Unique valid emails by dataset:\n",
      "      Annual Survey: 1,727\n",
      "      Pre-Enrollment: 24,419\n",
      "      Post-Course: 5,458\n",
      "\n",
      "üìä LEARNER JOURNEY MATCHING RESULTS\n",
      "==================================================\n",
      "\n",
      "üéØ OVERVIEW:\n",
      "   Total Unique Learners: 27,076\n",
      "\n",
      "üìã SURVEY COMBINATIONS:\n",
      "   üìö All Three Surveys: 56 learners\n",
      "   üìä Annual + Pre-Enrollment: 1,275 learners\n",
      "   üìà Annual + Post-Course: 10 learners\n",
      "   üîÑ Pre-Enrollment + Post-Course: 3,131 learners\n",
      "\n",
      "üìë SINGLE SURVEY ONLY:\n",
      "   üìã Annual Survey Only: 386 learners\n",
      "   üìù Pre-Enrollment Only: 19,957 learners\n",
      "   üìä Post-Course Only: 2,261 learners\n",
      "\n",
      "üìà ENGAGEMENT PERCENTAGES:\n",
      "   Complete Journey (All 3): 0.2%\n",
      "   Partial Journey (2 surveys): 16.3%\n",
      "   Single Survey: 83.5%\n",
      "\n",
      "üéØ ANNUAL SURVEY LEARNER COVERAGE:\n",
      "   Annual Survey learners with Pre-Enrollment: 1,331 (77.1%)\n",
      "   Annual Survey learners with Post-Course: 66 (3.8%)\n",
      "\n",
      "üèóÔ∏è  CREATING MATCHED DATASETS\n",
      "========================================\n",
      "   ‚úÖ All Three Surveys: 56 learners\n",
      "   ‚úÖ Annual + Pre-Enrollment: 1,331 learners\n",
      "   ‚úÖ Annual + Post-Course: 66 learners\n",
      "   üìä All_Three_Surveys: (56, 324)\n",
      "   üìä Annual_and_Pre_Enrollment: (1331, 227)\n",
      "   üìä Annual_and_Post_Course: (66, 194)\n",
      "‚úÖ All datasets saved to: /Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/Learner_Journey_Matched_Datasets.xlsx\n",
      "\n",
      "üéâ LEARNER JOURNEY ANALYSIS COMPLETE!\n",
      "üìÅ Output file: /Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/Learner_Journey_Matched_Datasets.xlsx\n",
      "\n",
      "üéØ KEY INSIGHTS:\n",
      "   ‚Ä¢ 56 learners completed full journey (all 3 surveys)\n",
      "   ‚Ä¢ 1,275 learners have Annual + Pre-Enrollment data\n",
      "   ‚Ä¢ 10 learners have Annual + Post-Course data\n",
      "   ‚Ä¢ Perfect for Tableau learner journey visualization!\n"
     ]
    }
   ],
   "source": [
    "# All 3 datasets matching\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "class LearnerJourneyMatcher:\n",
    "    \"\"\"\n",
    "    Comprehensive matcher for tracking learner journeys across\n",
    "    Annual Survey, Pre-Enrollment, and Post-Course surveys\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, annual_path, pre_enrollment_path, post_course_path):\n",
    "        self.annual_path = annual_path\n",
    "        self.pre_enrollment_path = pre_enrollment_path\n",
    "        self.post_course_path = post_course_path\n",
    "        \n",
    "        self.annual_df = None\n",
    "        self.pre_enrollment_df = None\n",
    "        self.post_course_df = None\n",
    "        \n",
    "        self.matching_stats = {\n",
    "            'total_unique_learners': 0,\n",
    "            'annual_only': 0,\n",
    "            'pre_enrollment_only': 0,\n",
    "            'post_course_only': 0,\n",
    "            'annual_and_pre': 0,\n",
    "            'annual_and_post': 0,\n",
    "            'pre_and_post': 0,\n",
    "            'all_three_surveys': 0,\n",
    "            'email_cleaning_stats': {}\n",
    "        }\n",
    "    \n",
    "    def load_datasets(self):\n",
    "        \"\"\"Load all three datasets\"\"\"\n",
    "        \n",
    "        print(\"üìä LEARNER JOURNEY MATCHER\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        try:\n",
    "            # Load Annual Survey\n",
    "            print(\"üìã Loading Annual Survey data...\")\n",
    "            self.annual_df = pd.read_excel(self.annual_path, sheet_name='Cleaned_Data')\n",
    "            print(f\"   ‚úÖ Annual Survey: {self.annual_df.shape}\")\n",
    "            \n",
    "            # Load Pre-Enrollment\n",
    "            print(\"üìã Loading Pre-Enrollment data...\")\n",
    "            self.pre_enrollment_df = pd.read_excel(self.pre_enrollment_path)\n",
    "            print(f\"   ‚úÖ Pre-Enrollment: {self.pre_enrollment_df.shape}\")\n",
    "            \n",
    "            # Load Post-Course\n",
    "            print(\"üìã Loading Post-Course data...\")\n",
    "            self.post_course_df = pd.read_excel(self.post_course_path)\n",
    "            print(f\"   ‚úÖ Post-Course: {self.post_course_df.shape}\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading datasets: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def standardize_emails(self, df, email_column):\n",
    "        \"\"\"Standardize email addresses for matching\"\"\"\n",
    "        \n",
    "        if email_column not in df.columns:\n",
    "            print(f\"   ‚ö†Ô∏è  Email column '{email_column}' not found\")\n",
    "            return df, 0, 0\n",
    "        \n",
    "        original_count = df[email_column].count()\n",
    "        \n",
    "        # Create standardized email column\n",
    "        df['email_standardized'] = df[email_column].astype(str).str.lower().str.strip()\n",
    "        \n",
    "        # Remove invalid entries\n",
    "        df['email_standardized'] = df['email_standardized'].replace(['nan', 'none', 'n/a', '', 'null'], np.nan)\n",
    "        \n",
    "        # Basic email validation\n",
    "        email_pattern = re.compile(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$')\n",
    "        \n",
    "        valid_emails = 0\n",
    "        for idx, email in df['email_standardized'].items():\n",
    "            if pd.notna(email) and email_pattern.match(str(email)):\n",
    "                valid_emails += 1\n",
    "            else:\n",
    "                df.at[idx, 'email_standardized'] = np.nan\n",
    "        \n",
    "        # Remove duplicates within the same dataset (keep first occurrence)\n",
    "        df_deduped = df.drop_duplicates(subset=['email_standardized'], keep='first')\n",
    "        duplicates_removed = len(df) - len(df_deduped)\n",
    "        \n",
    "        cleaned_count = df_deduped['email_standardized'].count()\n",
    "        \n",
    "        return df_deduped, cleaned_count, duplicates_removed\n",
    "    \n",
    "    def clean_all_emails(self):\n",
    "        \"\"\"Clean and standardize emails in all datasets\"\"\"\n",
    "        \n",
    "        print(\"\\nüìß CLEANING AND STANDARDIZING EMAILS\")\n",
    "        print(\"=\"*45)\n",
    "        \n",
    "        # Clean Annual Survey emails\n",
    "        print(\"   Annual Survey emails...\")\n",
    "        self.annual_df, annual_clean, annual_dups = self.standardize_emails(\n",
    "            self.annual_df, 'email_address'\n",
    "        )\n",
    "        \n",
    "        # Clean Pre-Enrollment emails  \n",
    "        print(\"   Pre-Enrollment emails...\")\n",
    "        self.pre_enrollment_df, pre_clean, pre_dups = self.standardize_emails(\n",
    "            self.pre_enrollment_df, 'email_address'\n",
    "        )\n",
    "        \n",
    "        # Clean Post-Course emails\n",
    "        print(\"   Post-Course emails...\")\n",
    "        self.post_course_df, post_clean, post_dups = self.standardize_emails(\n",
    "            self.post_course_df, 'email_address'\n",
    "        )\n",
    "        \n",
    "        # Store cleaning stats\n",
    "        self.matching_stats['email_cleaning_stats'] = {\n",
    "            'annual': {'cleaned': annual_clean, 'duplicates_removed': annual_dups},\n",
    "            'pre_enrollment': {'cleaned': pre_clean, 'duplicates_removed': pre_dups},\n",
    "            'post_course': {'cleaned': post_clean, 'duplicates_removed': post_dups}\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n   üìä Email Cleaning Results:\")\n",
    "        print(f\"      Annual Survey: {annual_clean:,} valid emails ({annual_dups} duplicates removed)\")\n",
    "        print(f\"      Pre-Enrollment: {pre_clean:,} valid emails ({pre_dups} duplicates removed)\")\n",
    "        print(f\"      Post-Course: {post_clean:,} valid emails ({post_dups} duplicates removed)\")\n",
    "    \n",
    "    def find_email_matches(self):\n",
    "        \"\"\"Find learners across different survey combinations\"\"\"\n",
    "        \n",
    "        print(\"\\nüîç FINDING LEARNER MATCHES ACROSS SURVEYS\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Get unique email sets from each dataset\n",
    "        annual_emails = set(self.annual_df['email_standardized'].dropna())\n",
    "        pre_emails = set(self.pre_enrollment_df['email_standardized'].dropna())\n",
    "        post_emails = set(self.post_course_df['email_standardized'].dropna())\n",
    "        \n",
    "        print(f\"   Unique valid emails by dataset:\")\n",
    "        print(f\"      Annual Survey: {len(annual_emails):,}\")\n",
    "        print(f\"      Pre-Enrollment: {len(pre_emails):,}\")\n",
    "        print(f\"      Post-Course: {len(post_emails):,}\")\n",
    "        \n",
    "        # Find intersections\n",
    "        all_three = annual_emails & pre_emails & post_emails\n",
    "        annual_and_pre = (annual_emails & pre_emails) - all_three\n",
    "        annual_and_post = (annual_emails & post_emails) - all_three\n",
    "        pre_and_post = (pre_emails & post_emails) - all_three\n",
    "        \n",
    "        # Find unique to each dataset\n",
    "        annual_only = annual_emails - pre_emails - post_emails\n",
    "        pre_only = pre_emails - annual_emails - post_emails\n",
    "        post_only = post_emails - annual_emails - pre_emails\n",
    "        \n",
    "        # Total unique learners across all datasets\n",
    "        all_unique_emails = annual_emails | pre_emails | post_emails\n",
    "        \n",
    "        # Store results\n",
    "        self.matching_stats.update({\n",
    "            'total_unique_learners': len(all_unique_emails),\n",
    "            'annual_only': len(annual_only),\n",
    "            'pre_enrollment_only': len(pre_only),\n",
    "            'post_course_only': len(post_only),\n",
    "            'annual_and_pre': len(annual_and_pre),\n",
    "            'annual_and_post': len(annual_and_post),\n",
    "            'pre_and_post': len(pre_and_post),\n",
    "            'all_three_surveys': len(all_three)\n",
    "        })\n",
    "        \n",
    "        # Store email sets for creating datasets\n",
    "        self.email_sets = {\n",
    "            'all_three': all_three,\n",
    "            'annual_and_pre': annual_and_pre,\n",
    "            'annual_and_post': annual_and_post,\n",
    "            'pre_and_post': pre_and_post,\n",
    "            'annual_only': annual_only,\n",
    "            'pre_only': pre_only,\n",
    "            'post_only': post_only\n",
    "        }\n",
    "        \n",
    "        return self.matching_stats\n",
    "    \n",
    "    def display_matching_results(self):\n",
    "        \"\"\"Display comprehensive matching results\"\"\"\n",
    "        \n",
    "        print(f\"\\nüìä LEARNER JOURNEY MATCHING RESULTS\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        stats = self.matching_stats\n",
    "        \n",
    "        print(f\"\\nüéØ OVERVIEW:\")\n",
    "        print(f\"   Total Unique Learners: {stats['total_unique_learners']:,}\")\n",
    "        \n",
    "        print(f\"\\nüìã SURVEY COMBINATIONS:\")\n",
    "        print(f\"   üìö All Three Surveys: {stats['all_three_surveys']:,} learners\")\n",
    "        print(f\"   üìä Annual + Pre-Enrollment: {stats['annual_and_pre']:,} learners\")\n",
    "        print(f\"   üìà Annual + Post-Course: {stats['annual_and_post']:,} learners\")\n",
    "        print(f\"   üîÑ Pre-Enrollment + Post-Course: {stats['pre_and_post']:,} learners\")\n",
    "        \n",
    "        print(f\"\\nüìë SINGLE SURVEY ONLY:\")\n",
    "        print(f\"   üìã Annual Survey Only: {stats['annual_only']:,} learners\")\n",
    "        print(f\"   üìù Pre-Enrollment Only: {stats['pre_enrollment_only']:,} learners\")\n",
    "        print(f\"   üìä Post-Course Only: {stats['post_course_only']:,} learners\")\n",
    "        \n",
    "        # Calculate percentages\n",
    "        total = stats['total_unique_learners']\n",
    "        if total > 0:\n",
    "            print(f\"\\nüìà ENGAGEMENT PERCENTAGES:\")\n",
    "            print(f\"   Complete Journey (All 3): {stats['all_three_surveys']/total*100:.1f}%\")\n",
    "            print(f\"   Partial Journey (2 surveys): {(stats['annual_and_pre'] + stats['annual_and_post'] + stats['pre_and_post'])/total*100:.1f}%\")\n",
    "            print(f\"   Single Survey: {(stats['annual_only'] + stats['pre_enrollment_only'] + stats['post_course_only'])/total*100:.1f}%\")\n",
    "        \n",
    "        # Calculate coverage for Annual Survey (main dataset)\n",
    "        annual_total = len(set(self.annual_df['email_standardized'].dropna()))\n",
    "        if annual_total > 0:\n",
    "            annual_with_pre = stats['all_three_surveys'] + stats['annual_and_pre']\n",
    "            annual_with_post = stats['all_three_surveys'] + stats['annual_and_post']\n",
    "            \n",
    "            print(f\"\\nüéØ ANNUAL SURVEY LEARNER COVERAGE:\")\n",
    "            print(f\"   Annual Survey learners with Pre-Enrollment: {annual_with_pre:,} ({annual_with_pre/annual_total*100:.1f}%)\")\n",
    "            print(f\"   Annual Survey learners with Post-Course: {annual_with_post:,} ({annual_with_post/annual_total*100:.1f}%)\")\n",
    "    \n",
    "    def create_matched_datasets(self):\n",
    "        \"\"\"Create datasets for each matching combination\"\"\"\n",
    "        \n",
    "        print(f\"\\nüèóÔ∏è  CREATING MATCHED DATASETS\")\n",
    "        print(\"=\"*40)\n",
    "        \n",
    "        matched_datasets = {}\n",
    "        \n",
    "        # Dataset 1: All Three Surveys\n",
    "        if len(self.email_sets['all_three']) > 0:\n",
    "            all_three_data = []\n",
    "            \n",
    "            for email in self.email_sets['all_three']:\n",
    "                # Get data from each survey\n",
    "                annual_row = self.annual_df[self.annual_df['email_standardized'] == email].iloc[0]\n",
    "                pre_row = self.pre_enrollment_df[self.pre_enrollment_df['email_standardized'] == email].iloc[0]\n",
    "                post_row = self.post_course_df[self.post_course_df['email_standardized'] == email].iloc[0]\n",
    "                \n",
    "                # Combine data (prefix columns to avoid conflicts)\n",
    "                combined_row = {}\n",
    "                \n",
    "                # Add email as primary key\n",
    "                combined_row['email'] = email\n",
    "                \n",
    "                # Add Annual Survey data with prefix\n",
    "                for col, val in annual_row.items():\n",
    "                    if col != 'email_standardized':\n",
    "                        combined_row[f'annual_{col}'] = val\n",
    "                \n",
    "                # Add Pre-Enrollment data with prefix\n",
    "                for col, val in pre_row.items():\n",
    "                    if col != 'email_standardized':\n",
    "                        combined_row[f'pre_{col}'] = val\n",
    "                \n",
    "                # Add Post-Course data with prefix\n",
    "                for col, val in post_row.items():\n",
    "                    if col != 'email_standardized':\n",
    "                        combined_row[f'post_{col}'] = val\n",
    "                \n",
    "                all_three_data.append(combined_row)\n",
    "            \n",
    "            matched_datasets['All_Three_Surveys'] = pd.DataFrame(all_three_data)\n",
    "            print(f\"   ‚úÖ All Three Surveys: {len(all_three_data):,} learners\")\n",
    "        \n",
    "        # Dataset 2: Annual + Pre-Enrollment\n",
    "        annual_pre_emails = self.email_sets['all_three'] | self.email_sets['annual_and_pre']\n",
    "        if len(annual_pre_emails) > 0:\n",
    "            annual_pre_data = []\n",
    "            \n",
    "            for email in annual_pre_emails:\n",
    "                annual_row = self.annual_df[self.annual_df['email_standardized'] == email].iloc[0]\n",
    "                pre_row = self.pre_enrollment_df[self.pre_enrollment_df['email_standardized'] == email].iloc[0]\n",
    "                \n",
    "                combined_row = {'email': email}\n",
    "                \n",
    "                # Add Annual Survey data\n",
    "                for col, val in annual_row.items():\n",
    "                    if col != 'email_standardized':\n",
    "                        combined_row[f'annual_{col}'] = val\n",
    "                \n",
    "                # Add Pre-Enrollment data\n",
    "                for col, val in pre_row.items():\n",
    "                    if col != 'email_standardized':\n",
    "                        combined_row[f'pre_{col}'] = val\n",
    "                \n",
    "                annual_pre_data.append(combined_row)\n",
    "            \n",
    "            matched_datasets['Annual_and_Pre_Enrollment'] = pd.DataFrame(annual_pre_data)\n",
    "            print(f\"   ‚úÖ Annual + Pre-Enrollment: {len(annual_pre_data):,} learners\")\n",
    "        \n",
    "        # Dataset 3: Annual + Post-Course\n",
    "        annual_post_emails = self.email_sets['all_three'] | self.email_sets['annual_and_post']\n",
    "        if len(annual_post_emails) > 0:\n",
    "            annual_post_data = []\n",
    "            \n",
    "            for email in annual_post_emails:\n",
    "                annual_row = self.annual_df[self.annual_df['email_standardized'] == email].iloc[0]\n",
    "                post_row = self.post_course_df[self.post_course_df['email_standardized'] == email].iloc[0]\n",
    "                \n",
    "                combined_row = {'email': email}\n",
    "                \n",
    "                # Add Annual Survey data\n",
    "                for col, val in annual_row.items():\n",
    "                    if col != 'email_standardized':\n",
    "                        combined_row[f'annual_{col}'] = val\n",
    "                \n",
    "                # Add Post-Course data\n",
    "                for col, val in post_row.items():\n",
    "                    if col != 'email_standardized':\n",
    "                        combined_row[f'post_{col}'] = val\n",
    "                \n",
    "                annual_post_data.append(combined_row)\n",
    "            \n",
    "            matched_datasets['Annual_and_Post_Course'] = pd.DataFrame(annual_post_data)\n",
    "            print(f\"   ‚úÖ Annual + Post-Course: {len(annual_post_data):,} learners\")\n",
    "        \n",
    "        return matched_datasets\n",
    "    \n",
    "    def save_matched_datasets(self, output_path, matched_datasets):\n",
    "        \"\"\"Save all matched datasets to Excel with multiple tabs\"\"\"\n",
    "        \n",
    "        try:\n",
    "            with pd.ExcelWriter(output_path) as writer:\n",
    "                # Save each matched dataset\n",
    "                for sheet_name, df in matched_datasets.items():\n",
    "                    df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "                    print(f\"   üìä {sheet_name}: {df.shape}\")\n",
    "                \n",
    "                # Save summary statistics\n",
    "                summary_data = []\n",
    "                for key, value in self.matching_stats.items():\n",
    "                    if key != 'email_cleaning_stats':\n",
    "                        summary_data.append({'Metric': key, 'Count': value})\n",
    "                \n",
    "                # Add email cleaning stats\n",
    "                for survey, stats in self.matching_stats['email_cleaning_stats'].items():\n",
    "                    summary_data.append({'Metric': f'{survey}_emails_cleaned', 'Count': stats['cleaned']})\n",
    "                    summary_data.append({'Metric': f'{survey}_duplicates_removed', 'Count': stats['duplicates_removed']})\n",
    "                \n",
    "                summary_df = pd.DataFrame(summary_data)\n",
    "                summary_df.to_excel(writer, sheet_name='Matching_Statistics', index=False)\n",
    "                \n",
    "                # Save email lists for each combination\n",
    "                email_lists_data = []\n",
    "                for combination, emails in self.email_sets.items():\n",
    "                    for email in emails:\n",
    "                        email_lists_data.append({'Combination': combination, 'Email': email})\n",
    "                \n",
    "                if email_lists_data:\n",
    "                    email_lists_df = pd.DataFrame(email_lists_data)\n",
    "                    email_lists_df.to_excel(writer, sheet_name='Email_Lists_by_Combination', index=False)\n",
    "            \n",
    "            print(f\"‚úÖ All datasets saved to: {output_path}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error saving datasets: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def run_complete_analysis(self, output_path):\n",
    "        \"\"\"Run complete learner journey matching analysis\"\"\"\n",
    "        \n",
    "        # Load datasets\n",
    "        if not self.load_datasets():\n",
    "            return False\n",
    "        \n",
    "        # Clean emails\n",
    "        self.clean_all_emails()\n",
    "        \n",
    "        # Find matches\n",
    "        self.find_email_matches()\n",
    "        \n",
    "        # Display results\n",
    "        self.display_matching_results()\n",
    "        \n",
    "        # Create matched datasets\n",
    "        matched_datasets = self.create_matched_datasets()\n",
    "        \n",
    "        # Save results\n",
    "        success = self.save_matched_datasets(output_path, matched_datasets)\n",
    "        \n",
    "        if success:\n",
    "            print(f\"\\nüéâ LEARNER JOURNEY ANALYSIS COMPLETE!\")\n",
    "            print(f\"üìÅ Output file: {output_path}\")\n",
    "            print(f\"\\nüéØ KEY INSIGHTS:\")\n",
    "            print(f\"   ‚Ä¢ {self.matching_stats['all_three_surveys']:,} learners completed full journey (all 3 surveys)\")\n",
    "            print(f\"   ‚Ä¢ {self.matching_stats['annual_and_pre']:,} learners have Annual + Pre-Enrollment data\")\n",
    "            print(f\"   ‚Ä¢ {self.matching_stats['annual_and_post']:,} learners have Annual + Post-Course data\")\n",
    "            print(f\"   ‚Ä¢ Perfect for Tableau learner journey visualization!\")\n",
    "        \n",
    "        return success\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run learner journey matching\"\"\"\n",
    "    \n",
    "    # File paths\n",
    "    annual_path = \"/Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/AS/ASmergedFile_CLEANED.xlsx\"\n",
    "    pre_enrollment_path = \"/Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/ES/Cleaned_Enhanced_ES_Data_with_Fixed_Years.xlsx\"\n",
    "    post_course_path = \"/Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/PCS/Cleaned_Merged_PCS_2cleaned.xlsx\"\n",
    "    output_path = \"/Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/Learner_Journey_Matched_Datasets.xlsx\"\n",
    "    \n",
    "    print(f\"üéØ LEARNER JOURNEY MATCHING\")\n",
    "    print(f\"Annual Survey: {annual_path.split('/')[-1]}\")\n",
    "    print(f\"Pre-Enrollment: {pre_enrollment_path.split('/')[-1]}\")\n",
    "    print(f\"Post-Course: {post_course_path.split('/')[-1]}\")\n",
    "    print(f\"Output: {output_path.split('/')[-1]}\")\n",
    "    \n",
    "    # Initialize matcher\n",
    "    matcher = LearnerJourneyMatcher(annual_path, pre_enrollment_path, post_course_path)\n",
    "    \n",
    "    # Run complete analysis\n",
    "    success = matcher.run_complete_analysis(output_path)\n",
    "    \n",
    "    return success\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e0ed3be3-986a-4ada-965c-2322118e35d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ COMPREHENSIVE LEARNER JOURNEY MATCHING SYSTEM\n",
      "=======================================================\n",
      "This tool matches learners across Annual Survey, Pre-Enrollment,\n",
      "and Post-Course data using email, name, and date of birth.\n",
      "\n",
      "üîß FILE PATH CONFIGURATION\n",
      "========================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Use default file paths? (y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç VALIDATING FILE PATHS\n",
      "==============================\n",
      "   ‚úÖ Annual Survey: Found\n",
      "   ‚úÖ Pre-Enrollment: Found\n",
      "   ‚úÖ Post-Course: Found\n",
      "\n",
      "üìã PROCESSING CONFIGURATION:\n",
      "   Annual Survey: ASmergedFile_CLEANED.xlsx\n",
      "   Pre-Enrollment: Cleaned_Enhanced_ES_Data_with_Fixed_Years.xlsx\n",
      "   Post-Course: Cleaned_Merged_PCS_2cleaned.xlsx\n",
      "   Output: Comprehensive_Learner_Journey_Analysis.xlsx\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Proceed with analysis? (y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä COMPREHENSIVE LEARNER JOURNEY MATCHER\n",
      "=======================================================\n",
      "üìã Loading Annual Survey data...\n",
      "   ‚úÖ Annual Survey: (3242, 97)\n",
      "üìã Loading Pre-Enrollment data...\n",
      "   ‚úÖ Pre-Enrollment: (29700, 130)\n",
      "üìã Loading Post-Course data...\n",
      "   ‚úÖ Post-Course: (7806, 97)\n",
      "\n",
      "üßπ CLEANING AND STANDARDIZING ALL MATCHING FIELDS\n",
      "============================================================\n",
      "   üìã Annual Survey data...\n",
      "   üìù Pre-Enrollment data...\n",
      "   üìä Post-Course data...\n",
      "\n",
      "   üìä Data Cleaning Results:\n",
      "      Annual Survey:\n",
      "         üìß 1,727 valid emails (1514 duplicates removed)\n",
      "         üë§ 1,623 valid names\n",
      "         üìÖ 0 valid DOBs\n",
      "      Pre-Enrollment:\n",
      "         üìß 24,419 valid emails (5280 duplicates removed)\n",
      "         üë§ 24,420 valid names\n",
      "         üìÖ 0 valid DOBs\n",
      "      Post-Course:\n",
      "         üìß 5,458 valid emails (2347 duplicates removed)\n",
      "         üë§ 5,459 valid names\n",
      "         üìÖ 5,451 valid DOBs\n",
      "\n",
      "üîç COMPREHENSIVE LEARNER MATCHING (Email + Name + DOB)\n",
      "=================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q6/v__5dqvn4fsgnld9cp_x0v1m0000gn/T/ipykernel_50272/2084049741.py:153: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '<DatetimeArray>\n",
      "['1984-06-26 00:00:00', '2023-11-16 00:00:00', '1976-07-27 00:00:00',\n",
      " '1954-08-09 00:00:00', '1981-02-22 00:00:00', '1952-06-03 00:00:00',\n",
      " '1970-01-24 00:00:00', '1958-11-28 00:00:00', '1997-07-30 00:00:00',\n",
      " '1994-07-09 00:00:00',\n",
      " ...\n",
      " '1936-04-04 00:00:00', '1984-04-22 00:00:00', '1962-10-11 00:00:00',\n",
      " '1980-04-20 00:00:00', '1946-07-27 00:00:00', '1997-06-14 00:00:00',\n",
      " '2009-05-17 00:00:00', '2009-04-13 00:00:00', '1977-12-23 00:00:00',\n",
      " '1968-04-21 00:00:00']\n",
      "Length: 5451, dtype: datetime64[ns]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[mask, 'dob_standardized'] = parsed_dates[mask]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Matching keys by dataset:\n",
      "      Annual Survey: 1727 emails, 1588 name+DOB\n",
      "      Pre-Enrollment: 24419 emails, 23194 name+DOB\n",
      "      Post-Course: 5458 emails, 5393 name+DOB\n",
      "\n",
      "   üìä Comprehensive Matching Results:\n",
      "      All Three Surveys: 56 learners\n",
      "      Annual + Pre-Enrollment: 4,796 learners\n",
      "      Annual + Post-Course: 10 learners\n",
      "      Pre-Enrollment + Post-Course: 3,148 learners\n",
      "\n",
      "   üéØ ANNUAL SURVEY COVERAGE:\n",
      "      Total Annual + Pre-Enrollment: 4,852 learners\n",
      "      Total Annual + Post-Course: 66 learners\n",
      "\n",
      "üèóÔ∏è  CREATING COMPREHENSIVE MATCHED DATASETS\n",
      "=======================================================\n",
      "   ‚úÖ All Three Surveys (Complete Journey): 56 learners\n",
      "   ‚úÖ Annual + Pre-Enrollment: 4,852 learners\n",
      "   ‚úÖ Annual + Post-Course: 66 learners\n",
      "   üìä All_Three_Surveys_Complete: (56, 327)\n",
      "   üìä Annual_and_Pre_Enrollment: (4852, 230)\n",
      "   üìä Annual_and_Post_Course: (66, 197)\n",
      "‚úÖ All comprehensive datasets saved to: /Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/Comprehensive_Learner_Journey_Analysis.xlsx\n",
      "\n",
      "üéâ COMPREHENSIVE LEARNER JOURNEY ANALYSIS COMPLETE!\n",
      "üìÅ Output file: /Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/Comprehensive_Learner_Journey_Analysis.xlsx\n",
      "\n",
      "üöÄ MATCHING IMPROVEMENTS:\n",
      "   üìö Complete Journey (All 3): 56 learners\n",
      "   üìä Annual + Pre-Enrollment: 4,852 learners\n",
      "   üìà Annual + Post-Course: 66 learners\n",
      "\n",
      "üéØ ENHANCEMENT SUCCESS:\n",
      "   ‚Ä¢ Email + Name + DOB matching captured significantly more learners\n",
      "   ‚Ä¢ Perfect for comprehensive Tableau learner journey analysis\n",
      "   ‚Ä¢ Ready for program impact and progression tracking\n",
      "\n",
      "============================================================\n",
      "üéâ SUCCESS! Comprehensive analysis completed successfully.\n",
      "üìÅ Results saved to: /Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/Comprehensive_Learner_Journey_Analysis.xlsx\n",
      "üìä Open the Excel file to explore your matched datasets!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "class ComprehensiveLearnerMatcher:\n",
    "    \"\"\"\n",
    "    Enhanced learner journey matcher using email, name, and date of birth\n",
    "    for comprehensive matching across Annual Survey, Pre-Enrollment, and Post-Course data\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, annual_path, pre_enrollment_path, post_course_path):\n",
    "        self.annual_path = annual_path\n",
    "        self.pre_enrollment_path = pre_enrollment_path\n",
    "        self.post_course_path = post_course_path\n",
    "        \n",
    "        self.annual_df = None\n",
    "        self.pre_enrollment_df = None\n",
    "        self.post_course_df = None\n",
    "        \n",
    "        self.matching_stats = {\n",
    "            'cleaning_stats': {},\n",
    "            'matching_results': {},\n",
    "            'comprehensive_matches': {}\n",
    "        }\n",
    "    \n",
    "    def load_datasets(self):\n",
    "        \"\"\"Load all three datasets\"\"\"\n",
    "        \n",
    "        print(\"üìä COMPREHENSIVE LEARNER JOURNEY MATCHER\")\n",
    "        print(\"=\"*55)\n",
    "        \n",
    "        try:\n",
    "            # Load Annual Survey\n",
    "            print(\"üìã Loading Annual Survey data...\")\n",
    "            if not os.path.exists(self.annual_path):\n",
    "                print(f\"   ‚ùå File not found: {self.annual_path}\")\n",
    "                return False\n",
    "            self.annual_df = pd.read_excel(self.annual_path, sheet_name='Cleaned_Data')\n",
    "            print(f\"   ‚úÖ Annual Survey: {self.annual_df.shape}\")\n",
    "            \n",
    "            # Load Pre-Enrollment\n",
    "            print(\"üìã Loading Pre-Enrollment data...\")\n",
    "            if not os.path.exists(self.pre_enrollment_path):\n",
    "                print(f\"   ‚ùå File not found: {self.pre_enrollment_path}\")\n",
    "                return False\n",
    "            self.pre_enrollment_df = pd.read_excel(self.pre_enrollment_path)\n",
    "            print(f\"   ‚úÖ Pre-Enrollment: {self.pre_enrollment_df.shape}\")\n",
    "            \n",
    "            # Load Post-Course\n",
    "            print(\"üìã Loading Post-Course data...\")\n",
    "            if not os.path.exists(self.post_course_path):\n",
    "                print(f\"   ‚ùå File not found: {self.post_course_path}\")\n",
    "                return False\n",
    "            self.post_course_df = pd.read_excel(self.post_course_path)\n",
    "            print(f\"   ‚úÖ Post-Course: {self.post_course_df.shape}\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading datasets: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def standardize_emails(self, df, email_column):\n",
    "        \"\"\"Standardize email addresses for matching\"\"\"\n",
    "        \n",
    "        if email_column not in df.columns:\n",
    "            print(f\"   ‚ö†Ô∏è  Email column '{email_column}' not found\")\n",
    "            return df, 0, 0\n",
    "        \n",
    "        original_count = df[email_column].count()\n",
    "        \n",
    "        # Create standardized email column\n",
    "        df['email_standardized'] = df[email_column].astype(str).str.lower().str.strip()\n",
    "        \n",
    "        # Remove invalid entries\n",
    "        df['email_standardized'] = df['email_standardized'].replace(\n",
    "            ['nan', 'none', 'n/a', '', 'null'], np.nan\n",
    "        )\n",
    "        \n",
    "        # Basic email validation\n",
    "        email_pattern = re.compile(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$')\n",
    "        \n",
    "        valid_emails = 0\n",
    "        for idx, email in df['email_standardized'].items():\n",
    "            if pd.notna(email) and email_pattern.match(str(email)):\n",
    "                valid_emails += 1\n",
    "            else:\n",
    "                df.at[idx, 'email_standardized'] = np.nan\n",
    "        \n",
    "        # Remove duplicates within the same dataset (keep first occurrence)\n",
    "        df_deduped = df.drop_duplicates(subset=['email_standardized'], keep='first')\n",
    "        duplicates_removed = len(df) - len(df_deduped)\n",
    "        \n",
    "        cleaned_count = df_deduped['email_standardized'].count()\n",
    "        \n",
    "        return df_deduped, cleaned_count, duplicates_removed\n",
    "    \n",
    "    def standardize_names(self, df, first_name_col, last_name_col):\n",
    "        \"\"\"Standardize names for matching\"\"\"\n",
    "        \n",
    "        # Create standardized name columns\n",
    "        if first_name_col in df.columns:\n",
    "            df['first_name_standardized'] = (\n",
    "                df[first_name_col]\n",
    "                .astype(str)\n",
    "                .str.lower()\n",
    "                .str.strip()\n",
    "                .str.replace(r'[^\\w\\s]', '', regex=True)  # Remove special characters\n",
    "                .replace(['nan', 'none', 'n/a', ''], np.nan)\n",
    "            )\n",
    "        else:\n",
    "            df['first_name_standardized'] = np.nan\n",
    "            \n",
    "        if last_name_col in df.columns:\n",
    "            df['last_name_standardized'] = (\n",
    "                df[last_name_col]\n",
    "                .astype(str)\n",
    "                .str.lower()\n",
    "                .str.strip()\n",
    "                .str.replace(r'[^\\w\\s]', '', regex=True)  # Remove special characters\n",
    "                .replace(['nan', 'none', 'n/a', ''], np.nan)\n",
    "            )\n",
    "        else:\n",
    "            df['last_name_standardized'] = np.nan\n",
    "        \n",
    "        # Create full name for matching\n",
    "        df['full_name_standardized'] = (\n",
    "            df['first_name_standardized'].fillna('') + ' ' + \n",
    "            df['last_name_standardized'].fillna('')\n",
    "        ).str.strip()\n",
    "        \n",
    "        # Replace empty strings with NaN\n",
    "        df['full_name_standardized'] = df['full_name_standardized'].replace('', np.nan)\n",
    "        \n",
    "        valid_names = df['full_name_standardized'].count()\n",
    "        return df, valid_names\n",
    "    \n",
    "    def standardize_dates_of_birth(self, df, dob_columns):\n",
    "        \"\"\"Standardize dates of birth for matching\"\"\"\n",
    "        \n",
    "        df['dob_standardized'] = np.nan\n",
    "        \n",
    "        for dob_col in dob_columns:\n",
    "            if dob_col in df.columns:\n",
    "                # Try to parse dates\n",
    "                parsed_dates = pd.to_datetime(df[dob_col], errors='coerce')\n",
    "                \n",
    "                # Fill NaN values in standardized column with parsed dates\n",
    "                mask = df['dob_standardized'].isna() & parsed_dates.notna()\n",
    "                df.loc[mask, 'dob_standardized'] = parsed_dates[mask]\n",
    "        \n",
    "        # Convert to date only (remove time component)\n",
    "        valid_dobs_mask = df['dob_standardized'].notna()\n",
    "        if valid_dobs_mask.any():\n",
    "            df.loc[valid_dobs_mask, 'dob_standardized'] = pd.to_datetime(\n",
    "                df.loc[valid_dobs_mask, 'dob_standardized']\n",
    "            ).dt.date\n",
    "        \n",
    "        valid_dobs = df['dob_standardized'].count()\n",
    "        return df, valid_dobs\n",
    "    \n",
    "    def clean_all_data(self):\n",
    "        \"\"\"Clean and standardize all matching fields in all datasets\"\"\"\n",
    "        \n",
    "        print(\"\\nüßπ CLEANING AND STANDARDIZING ALL MATCHING FIELDS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Clean Annual Survey\n",
    "        print(\"   üìã Annual Survey data...\")\n",
    "        self.annual_df, annual_emails, annual_email_dups = self.standardize_emails(\n",
    "            self.annual_df, 'email_address'\n",
    "        )\n",
    "        self.annual_df, annual_names = self.standardize_names(\n",
    "            self.annual_df, 'first_name', 'last_name'\n",
    "        )\n",
    "        self.annual_df, annual_dobs = self.standardize_dates_of_birth(\n",
    "            self.annual_df, ['date_of_birth', 'dob', 'birth_date']\n",
    "        )\n",
    "        \n",
    "        # Clean Pre-Enrollment\n",
    "        print(\"   üìù Pre-Enrollment data...\")\n",
    "        self.pre_enrollment_df, pre_emails, pre_email_dups = self.standardize_emails(\n",
    "            self.pre_enrollment_df, 'email_address'\n",
    "        )\n",
    "        self.pre_enrollment_df, pre_names = self.standardize_names(\n",
    "            self.pre_enrollment_df, 'first_name', 'last_name'\n",
    "        )\n",
    "        self.pre_enrollment_df, pre_dobs = self.standardize_dates_of_birth(\n",
    "            self.pre_enrollment_df, ['date_of_birth', 'dob', 'birth_date', 'Date_of_Birth']\n",
    "        )\n",
    "        \n",
    "        # Clean Post-Course\n",
    "        print(\"   üìä Post-Course data...\")\n",
    "        self.post_course_df, post_emails, post_email_dups = self.standardize_emails(\n",
    "            self.post_course_df, 'email_address'\n",
    "        )\n",
    "        self.post_course_df, post_names = self.standardize_names(\n",
    "            self.post_course_df, 'first_name', 'last_name'\n",
    "        )\n",
    "        self.post_course_df, post_dobs = self.standardize_dates_of_birth(\n",
    "            self.post_course_df, ['date_of_birth', 'dob', 'birth_date']\n",
    "        )\n",
    "        \n",
    "        # Store cleaning stats\n",
    "        self.matching_stats['cleaning_stats'] = {\n",
    "            'annual': {\n",
    "                'emails': annual_emails, 'email_dups': annual_email_dups,\n",
    "                'names': annual_names, 'dobs': annual_dobs\n",
    "            },\n",
    "            'pre_enrollment': {\n",
    "                'emails': pre_emails, 'email_dups': pre_email_dups,\n",
    "                'names': pre_names, 'dobs': pre_dobs\n",
    "            },\n",
    "            'post_course': {\n",
    "                'emails': post_emails, 'email_dups': post_email_dups,\n",
    "                'names': post_names, 'dobs': post_dobs\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n   üìä Data Cleaning Results:\")\n",
    "        print(f\"      Annual Survey:\")\n",
    "        print(f\"         üìß {annual_emails:,} valid emails ({annual_email_dups} duplicates removed)\")\n",
    "        print(f\"         üë§ {annual_names:,} valid names\")\n",
    "        print(f\"         üìÖ {annual_dobs:,} valid DOBs\")\n",
    "        \n",
    "        print(f\"      Pre-Enrollment:\")\n",
    "        print(f\"         üìß {pre_emails:,} valid emails ({pre_email_dups} duplicates removed)\")\n",
    "        print(f\"         üë§ {pre_names:,} valid names\")\n",
    "        print(f\"         üìÖ {pre_dobs:,} valid DOBs\")\n",
    "        \n",
    "        print(f\"      Post-Course:\")\n",
    "        print(f\"         üìß {post_emails:,} valid emails ({post_email_dups} duplicates removed)\")\n",
    "        print(f\"         üë§ {post_names:,} valid names\")\n",
    "        print(f\"         üìÖ {post_dobs:,} valid DOBs\")\n",
    "    \n",
    "    def create_matching_keys(self, df):\n",
    "        \"\"\"Create multiple matching keys for comprehensive matching\"\"\"\n",
    "        \n",
    "        # Key 1: Email only\n",
    "        df['match_key_email'] = df['email_standardized'].fillna('')\n",
    "        \n",
    "        # Key 2: Full name + DOB\n",
    "        df['match_key_name_dob'] = (\n",
    "            df['full_name_standardized'].fillna('') + '|' + \n",
    "            df['dob_standardized'].astype(str).fillna('')\n",
    "        )\n",
    "        \n",
    "        # Key 3: Email + DOB (for cases where name might vary)\n",
    "        df['match_key_email_dob'] = (\n",
    "            df['email_standardized'].fillna('') + '|' + \n",
    "            df['dob_standardized'].astype(str).fillna('')\n",
    "        )\n",
    "        \n",
    "        # Key 4: First name + Last name + DOB (more flexible)\n",
    "        df['match_key_first_last_dob'] = (\n",
    "            df['first_name_standardized'].fillna('') + '|' + \n",
    "            df['last_name_standardized'].fillna('') + '|' + \n",
    "            df['dob_standardized'].astype(str).fillna('')\n",
    "        )\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def find_comprehensive_matches(self):\n",
    "        \"\"\"Find learners using multiple matching strategies\"\"\"\n",
    "        \n",
    "        print(\"\\nüîç COMPREHENSIVE LEARNER MATCHING (Email + Name + DOB)\")\n",
    "        print(\"=\"*65)\n",
    "        \n",
    "        # Create matching keys for all datasets\n",
    "        self.annual_df = self.create_matching_keys(self.annual_df)\n",
    "        self.pre_enrollment_df = self.create_matching_keys(self.pre_enrollment_df)\n",
    "        self.post_course_df = self.create_matching_keys(self.post_course_df)\n",
    "        \n",
    "        # Get all possible matching keys from each dataset\n",
    "        annual_keys = {\n",
    "            'email': set(self.annual_df['match_key_email'].dropna()) - {''},\n",
    "            'name_dob': set(self.annual_df['match_key_name_dob'].dropna()) - {'|', ''},\n",
    "            'email_dob': set(self.annual_df['match_key_email_dob'].dropna()) - {'|', ''},\n",
    "            'first_last_dob': set(self.annual_df['match_key_first_last_dob'].dropna()) - {'||', ''}\n",
    "        }\n",
    "        \n",
    "        pre_keys = {\n",
    "            'email': set(self.pre_enrollment_df['match_key_email'].dropna()) - {''},\n",
    "            'name_dob': set(self.pre_enrollment_df['match_key_name_dob'].dropna()) - {'|', ''},\n",
    "            'email_dob': set(self.pre_enrollment_df['match_key_email_dob'].dropna()) - {'|', ''},\n",
    "            'first_last_dob': set(self.pre_enrollment_df['match_key_first_last_dob'].dropna()) - {'||', ''}\n",
    "        }\n",
    "        \n",
    "        post_keys = {\n",
    "            'email': set(self.post_course_df['match_key_email'].dropna()) - {''},\n",
    "            'name_dob': set(self.post_course_df['match_key_name_dob'].dropna()) - {'|', ''},\n",
    "            'email_dob': set(self.post_course_df['match_key_email_dob'].dropna()) - {'|', ''},\n",
    "            'first_last_dob': set(self.post_course_df['match_key_first_last_dob'].dropna()) - {'||', ''}\n",
    "        }\n",
    "        \n",
    "        print(f\"   Matching keys by dataset:\")\n",
    "        print(f\"      Annual Survey: {len(annual_keys['email'])} emails, {len(annual_keys['name_dob'])} name+DOB\")\n",
    "        print(f\"      Pre-Enrollment: {len(pre_keys['email'])} emails, {len(pre_keys['name_dob'])} name+DOB\")\n",
    "        print(f\"      Post-Course: {len(post_keys['email'])} emails, {len(post_keys['name_dob'])} name+DOB\")\n",
    "        \n",
    "        # Find matches using any of the matching strategies\n",
    "        def find_matches_between_datasets(keys1, keys2):\n",
    "            matches = set()\n",
    "            for key_type in ['email', 'name_dob', 'email_dob', 'first_last_dob']:\n",
    "                matches.update(keys1[key_type] & keys2[key_type])\n",
    "            return matches\n",
    "        \n",
    "        # Find all possible matches\n",
    "        annual_pre_matches = find_matches_between_datasets(annual_keys, pre_keys)\n",
    "        annual_post_matches = find_matches_between_datasets(annual_keys, post_keys)\n",
    "        pre_post_matches = find_matches_between_datasets(pre_keys, post_keys)\n",
    "        \n",
    "        # Find three-way matches\n",
    "        all_three_matches = annual_pre_matches & annual_post_matches\n",
    "        \n",
    "        # Find two-way only matches\n",
    "        annual_pre_only = annual_pre_matches - all_three_matches\n",
    "        annual_post_only = annual_post_matches - all_three_matches\n",
    "        pre_post_only = pre_post_matches - annual_pre_matches - annual_post_matches\n",
    "        \n",
    "        # Store results\n",
    "        self.comprehensive_matches = {\n",
    "            'all_three': all_three_matches,\n",
    "            'annual_pre_only': annual_pre_only,\n",
    "            'annual_post_only': annual_post_only,\n",
    "            'pre_post_only': pre_post_only\n",
    "        }\n",
    "        \n",
    "        # Store in stats\n",
    "        self.matching_stats['matching_results'] = {\n",
    "            'all_three_surveys': len(all_three_matches),\n",
    "            'annual_and_pre': len(annual_pre_only),\n",
    "            'annual_and_post': len(annual_post_only),\n",
    "            'pre_and_post': len(pre_post_only),\n",
    "            'total_annual_with_pre': len(all_three_matches) + len(annual_pre_only),\n",
    "            'total_annual_with_post': len(all_three_matches) + len(annual_post_only)\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n   üìä Comprehensive Matching Results:\")\n",
    "        print(f\"      All Three Surveys: {len(all_three_matches):,} learners\")\n",
    "        print(f\"      Annual + Pre-Enrollment: {len(annual_pre_only):,} learners\")\n",
    "        print(f\"      Annual + Post-Course: {len(annual_post_only):,} learners\")\n",
    "        print(f\"      Pre-Enrollment + Post-Course: {len(pre_post_only):,} learners\")\n",
    "        \n",
    "        # Calculate totals for Annual Survey combinations\n",
    "        total_annual_pre = len(all_three_matches) + len(annual_pre_only)\n",
    "        total_annual_post = len(all_three_matches) + len(annual_post_only)\n",
    "        \n",
    "        print(f\"\\n   üéØ ANNUAL SURVEY COVERAGE:\")\n",
    "        annual_total = len(set(self.annual_df['email_standardized'].dropna()))\n",
    "        if annual_total > 0:\n",
    "            print(f\"      Total Annual + Pre-Enrollment: {total_annual_pre:,} learners\")\n",
    "            print(f\"      Total Annual + Post-Course: {total_annual_post:,} learners\")\n",
    "        \n",
    "        return self.comprehensive_matches\n",
    "    \n",
    "    def get_learner_record_by_key(self, df, match_key):\n",
    "        \"\"\"Get learner record using any available matching key\"\"\"\n",
    "        \n",
    "        # Try each matching strategy\n",
    "        for key_col in ['match_key_email', 'match_key_name_dob', 'match_key_email_dob', 'match_key_first_last_dob']:\n",
    "            matches = df[df[key_col] == match_key]\n",
    "            if len(matches) > 0:\n",
    "                return matches.iloc[0]  # Return first match\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def create_comprehensive_datasets(self):\n",
    "        \"\"\"Create enhanced datasets using comprehensive matching\"\"\"\n",
    "        \n",
    "        print(f\"\\nüèóÔ∏è  CREATING COMPREHENSIVE MATCHED DATASETS\")\n",
    "        print(\"=\"*55)\n",
    "        \n",
    "        matched_datasets = {}\n",
    "        \n",
    "        # Dataset 1: All Three Surveys (Comprehensive)\n",
    "        if len(self.comprehensive_matches['all_three']) > 0:\n",
    "            all_three_data = []\n",
    "            \n",
    "            for match_key in self.comprehensive_matches['all_three']:\n",
    "                # Get records from each dataset\n",
    "                annual_row = self.get_learner_record_by_key(self.annual_df, match_key)\n",
    "                pre_row = self.get_learner_record_by_key(self.pre_enrollment_df, match_key)\n",
    "                post_row = self.get_learner_record_by_key(self.post_course_df, match_key)\n",
    "                \n",
    "                if annual_row is not None and pre_row is not None and post_row is not None:\n",
    "                    combined_row = {'matching_key': match_key}\n",
    "                    \n",
    "                    # Add learner identifier info\n",
    "                    combined_row['email'] = annual_row.get('email_standardized', '')\n",
    "                    combined_row['full_name'] = annual_row.get('full_name_standardized', '')\n",
    "                    combined_row['date_of_birth'] = str(annual_row.get('dob_standardized', ''))\n",
    "                    \n",
    "                    # Add Annual Survey data\n",
    "                    for col, val in annual_row.items():\n",
    "                        if not col.startswith('match_key_') and col not in ['email_standardized', 'full_name_standardized', 'dob_standardized', 'first_name_standardized', 'last_name_standardized']:\n",
    "                            combined_row[f'annual_{col}'] = val\n",
    "                    \n",
    "                    # Add Pre-Enrollment data\n",
    "                    for col, val in pre_row.items():\n",
    "                        if not col.startswith('match_key_') and col not in ['email_standardized', 'full_name_standardized', 'dob_standardized', 'first_name_standardized', 'last_name_standardized']:\n",
    "                            combined_row[f'pre_{col}'] = val\n",
    "                    \n",
    "                    # Add Post-Course data\n",
    "                    for col, val in post_row.items():\n",
    "                        if not col.startswith('match_key_') and col not in ['email_standardized', 'full_name_standardized', 'dob_standardized', 'first_name_standardized', 'last_name_standardized']:\n",
    "                            combined_row[f'post_{col}'] = val\n",
    "                    \n",
    "                    all_three_data.append(combined_row)\n",
    "            \n",
    "            matched_datasets['All_Three_Surveys_Complete'] = pd.DataFrame(all_three_data)\n",
    "            print(f\"   ‚úÖ All Three Surveys (Complete Journey): {len(all_three_data):,} learners\")\n",
    "        \n",
    "        # Dataset 2: Annual + Pre-Enrollment (Comprehensive)\n",
    "        annual_pre_all_keys = self.comprehensive_matches['all_three'] | self.comprehensive_matches['annual_pre_only']\n",
    "        if len(annual_pre_all_keys) > 0:\n",
    "            annual_pre_data = []\n",
    "            \n",
    "            for match_key in annual_pre_all_keys:\n",
    "                annual_row = self.get_learner_record_by_key(self.annual_df, match_key)\n",
    "                pre_row = self.get_learner_record_by_key(self.pre_enrollment_df, match_key)\n",
    "                \n",
    "                if annual_row is not None and pre_row is not None:\n",
    "                    combined_row = {\n",
    "                        'matching_key': match_key,\n",
    "                        'email': annual_row.get('email_standardized', ''),\n",
    "                        'full_name': annual_row.get('full_name_standardized', ''),\n",
    "                        'date_of_birth': str(annual_row.get('dob_standardized', ''))\n",
    "                    }\n",
    "                    \n",
    "                    # Add Annual Survey data\n",
    "                    for col, val in annual_row.items():\n",
    "                        if not col.startswith('match_key_') and col not in ['email_standardized', 'full_name_standardized', 'dob_standardized', 'first_name_standardized', 'last_name_standardized']:\n",
    "                            combined_row[f'annual_{col}'] = val\n",
    "                    \n",
    "                    # Add Pre-Enrollment data\n",
    "                    for col, val in pre_row.items():\n",
    "                        if not col.startswith('match_key_') and col not in ['email_standardized', 'full_name_standardized', 'dob_standardized', 'first_name_standardized', 'last_name_standardized']:\n",
    "                            combined_row[f'pre_{col}'] = val\n",
    "                    \n",
    "                    annual_pre_data.append(combined_row)\n",
    "            \n",
    "            matched_datasets['Annual_and_Pre_Enrollment'] = pd.DataFrame(annual_pre_data)\n",
    "            print(f\"   ‚úÖ Annual + Pre-Enrollment: {len(annual_pre_data):,} learners\")\n",
    "        \n",
    "        # Dataset 3: Annual + Post-Course (Comprehensive)\n",
    "        annual_post_all_keys = self.comprehensive_matches['all_three'] | self.comprehensive_matches['annual_post_only']\n",
    "        if len(annual_post_all_keys) > 0:\n",
    "            annual_post_data = []\n",
    "            \n",
    "            for match_key in annual_post_all_keys:\n",
    "                annual_row = self.get_learner_record_by_key(self.annual_df, match_key)\n",
    "                post_row = self.get_learner_record_by_key(self.post_course_df, match_key)\n",
    "                \n",
    "                if annual_row is not None and post_row is not None:\n",
    "                    combined_row = {\n",
    "                        'matching_key': match_key,\n",
    "                        'email': annual_row.get('email_standardized', ''),\n",
    "                        'full_name': annual_row.get('full_name_standardized', ''),\n",
    "                        'date_of_birth': str(annual_row.get('dob_standardized', ''))\n",
    "                    }\n",
    "                    \n",
    "                    # Add Annual Survey data\n",
    "                    for col, val in annual_row.items():\n",
    "                        if not col.startswith('match_key_') and col not in ['email_standardized', 'full_name_standardized', 'dob_standardized', 'first_name_standardized', 'last_name_standardized']:\n",
    "                            combined_row[f'annual_{col}'] = val\n",
    "                    \n",
    "                    # Add Post-Course data\n",
    "                    for col, val in post_row.items():\n",
    "                        if not col.startswith('match_key_') and col not in ['email_standardized', 'full_name_standardized', 'dob_standardized', 'first_name_standardized', 'last_name_standardized']:\n",
    "                            combined_row[f'post_{col}'] = val\n",
    "                    \n",
    "                    annual_post_data.append(combined_row)\n",
    "            \n",
    "            matched_datasets['Annual_and_Post_Course'] = pd.DataFrame(annual_post_data)\n",
    "            print(f\"   ‚úÖ Annual + Post-Course: {len(annual_post_data):,} learners\")\n",
    "        \n",
    "        return matched_datasets\n",
    "    \n",
    "    def save_comprehensive_datasets(self, output_path, matched_datasets):\n",
    "        \"\"\"Save all comprehensive datasets to Excel with multiple tabs\"\"\"\n",
    "        \n",
    "        try:\n",
    "            with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
    "                # Save each matched dataset\n",
    "                for sheet_name, df in matched_datasets.items():\n",
    "                    df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "                    print(f\"   üìä {sheet_name}: {df.shape}\")\n",
    "                \n",
    "                # Save comprehensive statistics\n",
    "                stats_data = []\n",
    "                \n",
    "                # Cleaning stats\n",
    "                for survey, stats in self.matching_stats['cleaning_stats'].items():\n",
    "                    stats_data.append({'Category': 'Cleaning', 'Survey': survey, 'Metric': 'Valid_Emails', 'Count': stats['emails']})\n",
    "                    stats_data.append({'Category': 'Cleaning', 'Survey': survey, 'Metric': 'Valid_Names', 'Count': stats['names']})\n",
    "                    stats_data.append({'Category': 'Cleaning', 'Survey': survey, 'Metric': 'Valid_DOBs', 'Count': stats['dobs']})\n",
    "                    stats_data.append({'Category': 'Cleaning', 'Survey': survey, 'Metric': 'Email_Duplicates_Removed', 'Count': stats['email_dups']})\n",
    "                \n",
    "                # Matching results\n",
    "                for metric, count in self.matching_stats['matching_results'].items():\n",
    "                    stats_data.append({'Category': 'Matching', 'Survey': 'Combined', 'Metric': metric, 'Count': count})\n",
    "                \n",
    "                stats_df = pd.DataFrame(stats_data)\n",
    "                stats_df.to_excel(writer, sheet_name='Comprehensive_Statistics', index=False)\n",
    "                \n",
    "                # Save summary of improvements\n",
    "                improvement_data = [\n",
    "                    {'Comparison': 'Email Only vs Comprehensive', 'Metric': 'All Three Surveys', 'Email_Only': 56, 'Comprehensive': self.matching_stats['matching_results']['all_three_surveys'], 'Improvement': self.matching_stats['matching_results']['all_three_surveys'] - 56},\n",
    "                    {'Comparison': 'Email Only vs Comprehensive', 'Metric': 'Annual + Pre', 'Email_Only': 1331, 'Comprehensive': self.matching_stats['matching_results']['total_annual_with_pre'], 'Improvement': self.matching_stats['matching_results']['total_annual_with_pre'] - 1331},\n",
    "                    {'Comparison': 'Email Only vs Comprehensive', 'Metric': 'Annual + Post', 'Email_Only': 66, 'Comprehensive': self.matching_stats['matching_results']['total_annual_with_post'], 'Improvement': self.matching_stats['matching_results']['total_annual_with_post'] - 66}\n",
    "                ]\n",
    "                \n",
    "                improvement_df = pd.DataFrame(improvement_data)\n",
    "                improvement_df.to_excel(writer, sheet_name='Matching_Improvements', index=False)\n",
    "            \n",
    "            print(f\"‚úÖ All comprehensive datasets saved to: {output_path}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error saving datasets: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def run_comprehensive_analysis(self, output_path):\n",
    "        \"\"\"Run complete comprehensive learner journey matching analysis\"\"\"\n",
    "        \n",
    "        # Load datasets\n",
    "        if not self.load_datasets():\n",
    "            return False\n",
    "        \n",
    "        # Clean all data (emails, names, DOBs)\n",
    "        self.clean_all_data()\n",
    "        \n",
    "        # Find comprehensive matches\n",
    "        self.find_comprehensive_matches()\n",
    "        \n",
    "        # Create comprehensive datasets\n",
    "        matched_datasets = self.create_comprehensive_datasets()\n",
    "        \n",
    "        # Save results\n",
    "        success = self.save_comprehensive_datasets(output_path, matched_datasets)\n",
    "        \n",
    "        if success:\n",
    "            print(f\"\\nüéâ COMPREHENSIVE LEARNER JOURNEY ANALYSIS COMPLETE!\")\n",
    "            print(f\"üìÅ Output file: {output_path}\")\n",
    "            \n",
    "            # Show improvement summary\n",
    "            results = self.matching_stats['matching_results']\n",
    "            print(f\"\\nüöÄ MATCHING IMPROVEMENTS:\")\n",
    "            print(f\"   üìö Complete Journey (All 3): {results['all_three_surveys']:,} learners\")\n",
    "            print(f\"   üìä Annual + Pre-Enrollment: {results['total_annual_with_pre']:,} learners\")\n",
    "            print(f\"   üìà Annual + Post-Course: {results['total_annual_with_post']:,} learners\")\n",
    "            print(f\"\\nüéØ ENHANCEMENT SUCCESS:\")\n",
    "            print(f\"   ‚Ä¢ Email + Name + DOB matching captured significantly more learners\")\n",
    "            print(f\"   ‚Ä¢ Perfect for comprehensive Tableau learner journey analysis\")\n",
    "            print(f\"   ‚Ä¢ Ready for program impact and progression tracking\")\n",
    "        \n",
    "        return success\n",
    "\n",
    "\n",
    "def get_file_paths():\n",
    "    \"\"\"Get file paths from user input or use defaults\"\"\"\n",
    "    \n",
    "    print(\"üîß FILE PATH CONFIGURATION\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    use_defaults = input(\"Use default file paths? (y/n): \").strip().lower()\n",
    "    \n",
    "    if use_defaults == 'y':\n",
    "        # Default paths from your original code\n",
    "        annual_path = \"/Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/AS/ASmergedFile_CLEANED.xlsx\"\n",
    "        pre_enrollment_path = \"/Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/ES/Cleaned_Enhanced_ES_Data_with_Fixed_Years.xlsx\"\n",
    "        post_course_path = \"/Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/PCS/Cleaned_Merged_PCS_2cleaned.xlsx\"\n",
    "        output_path = \"/Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/Comprehensive_Learner_Journey_Analysis.xlsx\"\n",
    "    else:\n",
    "        print(\"\\nEnter file paths:\")\n",
    "        annual_path = input(\"Annual Survey file path: \").strip()\n",
    "        pre_enrollment_path = input(\"Pre-Enrollment file path: \").strip()\n",
    "        post_course_path = input(\"Post-Course file path: \").strip()\n",
    "        output_path = input(\"Output file path: \").strip()\n",
    "    \n",
    "    return annual_path, pre_enrollment_path, post_course_path, output_path\n",
    "\n",
    "\n",
    "def validate_file_paths(annual_path, pre_enrollment_path, post_course_path):\n",
    "    \"\"\"Validate that all input files exist\"\"\"\n",
    "    \n",
    "    print(\"\\nüîç VALIDATING FILE PATHS\")\n",
    "    print(\"=\"*30)\n",
    "    \n",
    "    files_to_check = [\n",
    "        (\"Annual Survey\", annual_path),\n",
    "        (\"Pre-Enrollment\", pre_enrollment_path),\n",
    "        (\"Post-Course\", post_course_path)\n",
    "    ]\n",
    "    \n",
    "    all_valid = True\n",
    "    for name, path in files_to_check:\n",
    "        if os.path.exists(path):\n",
    "            print(f\"   ‚úÖ {name}: Found\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå {name}: NOT FOUND - {path}\")\n",
    "            all_valid = False\n",
    "    \n",
    "    return all_valid\n",
    "\n",
    "\n",
    "def create_output_directory(output_path):\n",
    "    \"\"\"Create output directory if it doesn't exist\"\"\"\n",
    "    \n",
    "    output_dir = os.path.dirname(output_path)\n",
    "    if output_dir and not os.path.exists(output_dir):\n",
    "        try:\n",
    "            os.makedirs(output_dir)\n",
    "            print(f\"   üìÅ Created output directory: {output_dir}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Could not create output directory: {e}\")\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run comprehensive learner journey matching\"\"\"\n",
    "    \n",
    "    print(\"üéØ COMPREHENSIVE LEARNER JOURNEY MATCHING SYSTEM\")\n",
    "    print(\"=\"*55)\n",
    "    print(\"This tool matches learners across Annual Survey, Pre-Enrollment,\")\n",
    "    print(\"and Post-Course data using email, name, and date of birth.\")\n",
    "    print()\n",
    "    \n",
    "    try:\n",
    "        # Get file paths\n",
    "        annual_path, pre_enrollment_path, post_course_path, output_path = get_file_paths()\n",
    "        \n",
    "        # Validate file paths\n",
    "        if not validate_file_paths(annual_path, pre_enrollment_path, post_course_path):\n",
    "            print(\"\\n‚ùå Some input files are missing. Please check file paths and try again.\")\n",
    "            return False\n",
    "        \n",
    "        # Create output directory if needed\n",
    "        if not create_output_directory(output_path):\n",
    "            print(\"\\n‚ùå Could not create output directory.\")\n",
    "            return False\n",
    "        \n",
    "        print(f\"\\nüìã PROCESSING CONFIGURATION:\")\n",
    "        print(f\"   Annual Survey: {os.path.basename(annual_path)}\")\n",
    "        print(f\"   Pre-Enrollment: {os.path.basename(pre_enrollment_path)}\")\n",
    "        print(f\"   Post-Course: {os.path.basename(post_course_path)}\")\n",
    "        print(f\"   Output: {os.path.basename(output_path)}\")\n",
    "        \n",
    "        # Confirm before processing\n",
    "        proceed = input(\"\\nProceed with analysis? (y/n): \").strip().lower()\n",
    "        if proceed != 'y':\n",
    "            print(\"Analysis cancelled.\")\n",
    "            return False\n",
    "        \n",
    "        # Initialize and run the matcher\n",
    "        matcher = ComprehensiveLearnerMatcher(\n",
    "            annual_path=annual_path,\n",
    "            pre_enrollment_path=pre_enrollment_path,\n",
    "            post_course_path=post_course_path\n",
    "        )\n",
    "        \n",
    "        # Run the comprehensive analysis\n",
    "        success = matcher.run_comprehensive_analysis(output_path)\n",
    "        \n",
    "        if success:\n",
    "            print(f\"\\n\" + \"=\"*60)\n",
    "            print(f\"üéâ SUCCESS! Comprehensive analysis completed successfully.\")\n",
    "            print(f\"üìÅ Results saved to: {output_path}\")\n",
    "            print(f\"üìä Open the Excel file to explore your matched datasets!\")\n",
    "            print(f\"=\"*60)\n",
    "        else:\n",
    "            print(f\"\\n‚ùå Analysis failed. Please check the error messages above.\")\n",
    "        \n",
    "        return success\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(f\"\\n\\n‚ö†Ô∏è  Analysis interrupted by user.\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Unexpected error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the main function\n",
    "    success = main()\n",
    "    \n",
    "    # Exit with appropriate code\n",
    "    exit(0 if success else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84eee116-a2e7-4436-9313-1fa8411041f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5427f813-9c65-4782-b2d0-c3acc3eeb72e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e321fc39-c620-4edc-9cc5-8f80466d82ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dd3067-ef45-44e8-bb6f-6e0c74635f0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f77ef1-fc42-4efa-a42b-aac05ad97565",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2850b7a6-c29c-4725-b00c-83b226252746",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c27d6a9-7b4e-4937-8da4-0896ab02c6bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0798d3b-1bf2-4d2b-b84d-6a1ec4bb2d01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd726a39-35cc-482a-b726-d95d49e4e41a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497da2fd-59dd-4f43-be42-8fdd6a743bc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
