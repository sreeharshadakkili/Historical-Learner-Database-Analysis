{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2944936e-e097-4c13-9b28-0b5c5e418e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data cleaning process at 2025-05-07 17:20:36\n",
      "Loading Excel file: /Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/ES/Merged_ES_Cleaned.xlsx\n",
      "Successfully loaded Excel file with 29700 rows and 151 columns.\n",
      "Data shape: (29700, 151)\n",
      "First few column names: ['name_of_siteschool', 'course_type', 'course_name', 'first_name', 'last_name']\n",
      "\n",
      "STEP 1: Extracting course date and year from course name\n",
      "Extracting course_date from course_name...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2d6ba60a6684387b21c6d8a93c2ad10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting dates:   0%|          | 0/29700 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted 19373 dates from 29700 course names (65.2%)\n",
      "Converting extracted dates to datetime objects...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20c5717793c3474688a672c5d9c04ad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting dates:   0%|          | 0/29700 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted 19363 years from 19373 dates (99.9%)\n",
      "\n",
      "STEP 2: Creating full names from first and last names\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2ed6dfadf0b4bafa086ecc95724f3d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating full names:   0%|          | 0/29700 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 29614 full names from 29700 records (99.7%)\n",
      "\n",
      "STEP 3: Standardizing dates of birth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed60f6864c2e4d22b32bc554c3cf14ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Standardizing DOBs:   0%|          | 0/29700 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized 29611 dates of birth from 29700 records (99.7%)\n",
      "\n",
      "Sample of date standardization:\n",
      "  Original: 4/9/88 -> Standardized: 04-09-1988\n",
      "  Original: 9/22/66 -> Standardized: 09-22-1966\n",
      "  Original: 3/5/81 -> Standardized: 03-05-1981\n",
      "  Original: 2/7/81 -> Standardized: 02-07-1981\n",
      "  Original: 12/18/92 -> Standardized: 12-18-1992\n",
      "\n",
      "STEP 4: Calculating ages from dates of birth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df01d7d5fb8841ae993eae3d7462e5b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating ages:   0%|          | 0/29700 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated 29559 ages from 29611 standardized dates (99.8%)\n",
      "Date format errors: 52, Age range errors: 42\n",
      "\n",
      "Age statistics:\n",
      "  Count: 29559\n",
      "  Mean: 46.5 years\n",
      "  Min: 0 years\n",
      "  Max: 99 years\n",
      "\n",
      "STEP 5: Fixing zip codes to 5 digits\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b249507ad14b47aba7398e2bad2edd39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fixing zip codes:   0%|          | 0/29700 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized 29610 zip codes from 29700 records (99.7%)\n",
      "Zip code errors: 0\n",
      "\n",
      "Preview of processed data:\n",
      "                                         course_name course_date  course_year  first_name        last_name                 full_name date_of_birth date_of_birth_standardized   age  zip_code zip_code_fixed\n",
      "0  The Latino Support Network Inc- Hugo Carvajal ...     1/12/22       2022.0       Flori           Robles              Flori Robles        4/9/88                 04-09-1988  37.0    1906.0          01906\n",
      "1  Codman Square NDC- Prince Charles   SB 4/6/21 ...      4/6/21       2021.0    Penelope  Ragland- Godwin  Penelope Ragland- Godwin       9/22/66                 09-22-1966  58.0    2114.0          02114\n",
      "2  MakeIT Haverhill- Tim Haynes Frank Vasquez SB ...     4/13/22       2022.0     Johanna          Hidalgo           Johanna Hidalgo        3/5/81                 03-05-1981  44.0    1835.0          01835\n",
      "3  YMCA International Learning Ce- Sarah Poole SB...     4/12/21       2021.0         Luz          Ramirez               Luz Ramirez        2/7/81                 02-07-1981  44.0    2131.0          02131\n",
      "4  UMass Institute for Early Educ- Lynne Mendes N...     10/5/21       2021.0  Altagracia           Valdez         Altagracia Valdez      12/18/92                 12-18-1992  32.0    2136.0          02136\n",
      "\n",
      "Data cleaning summary:\n",
      "Total records processed: 29700\n",
      "Course dates extracted: 19373 (65.2%)\n",
      "Course years extracted: 19363 (65.2%)\n",
      "Full names created: 29614 (99.7%)\n",
      "Dates of birth standardized: 29611 (99.7%)\n",
      "Ages calculated: 29559 (99.5%)\n",
      "Zip codes fixed: 29610 (99.7%)\n",
      "\n",
      "Saving data to Excel file: /Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/ES/2nd_cleaned_data_excel.xlsx\n",
      "Successfully saved cleaned data to '/Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/ES/2nd_cleaned_data_excel.xlsx'\n",
      "\n",
      "Data cleaning process completed at 2025-05-07 17:21:21\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "# Improve pandas display for debugging\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Function to create log directory if it doesn't exist\n",
    "def ensure_log_dir():\n",
    "    log_dir = 'logs'\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    return log_dir\n",
    "\n",
    "# Set up logging\n",
    "log_dir = ensure_log_dir()\n",
    "log_file = os.path.join(log_dir, f'data_cleaning_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log')\n",
    "\n",
    "def log_message(message):\n",
    "    \"\"\"Log a message to both console and file\"\"\"\n",
    "    print(message)\n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(message + '\\n')\n",
    "\n",
    "log_message(f\"Starting data cleaning process at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Load the data - use read_excel for .xlsx files\n",
    "file_path = '/Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/ES/Merged_ES_Cleaned.xlsx'\n",
    "try:\n",
    "    # Try reading the Excel file\n",
    "    log_message(f\"Loading Excel file: {file_path}\")\n",
    "    df = pd.read_excel(file_path)\n",
    "    log_message(f\"Successfully loaded Excel file with {len(df)} rows and {len(df.columns)} columns.\")\n",
    "except Exception as e:\n",
    "    log_message(f\"Error loading Excel file: {e}\")\n",
    "    # Try with different encoding as fallback\n",
    "    try:\n",
    "        # If it's actually a CSV disguised as .xlsx\n",
    "        log_message(\"Attempting to load as CSV with latin1 encoding...\")\n",
    "        df = pd.read_csv(file_path, encoding='latin1')\n",
    "        log_message(\"Loaded as CSV with latin1 encoding.\")\n",
    "    except Exception as e2:\n",
    "        log_message(f\"Second attempt failed: {e2}\")\n",
    "        raise Exception(\"Could not load the file. Please check the file format and path.\")\n",
    "\n",
    "# Print basic info to verify data was loaded\n",
    "log_message(f\"Data shape: {df.shape}\")\n",
    "log_message(f\"First few column names: {df.columns[:5].tolist()}\")\n",
    "\n",
    "# 1. Extract course_date and course_year from course_name\n",
    "log_message(\"\\nSTEP 1: Extracting course date and year from course name\")\n",
    "\n",
    "def extract_date_from_course_name(course_name):\n",
    "    if pd.isna(course_name):\n",
    "        return None\n",
    "    \n",
    "    # Match date patterns like 1/12/22, 4/6/21, etc.\n",
    "    date_pattern = r'(\\d{1,2}/\\d{1,2}/\\d{2})'\n",
    "    match = re.search(date_pattern, str(course_name))\n",
    "    \n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n",
    "# Apply the function to create course_date with a progress bar\n",
    "log_message(\"Extracting course_date from course_name...\")\n",
    "tqdm.pandas(desc=\"Extracting dates\")\n",
    "df['course_date'] = df['course_name'].progress_apply(extract_date_from_course_name)\n",
    "\n",
    "# Count successful extractions\n",
    "date_count = df['course_date'].notna().sum()\n",
    "log_message(f\"Successfully extracted {date_count} dates from {len(df)} course names ({date_count/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Convert course_date to datetime and extract year\n",
    "def convert_date(date_str):\n",
    "    if pd.isna(date_str):\n",
    "        return None\n",
    "    try:\n",
    "        date_obj = datetime.strptime(str(date_str), '%m/%d/%y')\n",
    "        return date_obj\n",
    "    except Exception:\n",
    "        # Try other formats if the first one fails\n",
    "        try:\n",
    "            date_obj = datetime.strptime(str(date_str), '%m/%d/%Y')\n",
    "            return date_obj\n",
    "        except:\n",
    "            pass\n",
    "        return None\n",
    "\n",
    "# Apply with error handling and progress bar\n",
    "log_message(\"Converting extracted dates to datetime objects...\")\n",
    "tqdm.pandas(desc=\"Converting dates\")\n",
    "df['course_date_dt'] = df['course_date'].progress_apply(convert_date)\n",
    "df['course_year'] = df['course_date_dt'].apply(lambda x: x.year if pd.notnull(x) else None)\n",
    "\n",
    "# Count successful conversions\n",
    "year_count = df['course_year'].notna().sum()\n",
    "log_message(f\"Successfully extracted {year_count} years from {date_count} dates ({year_count/date_count*100:.1f}%)\")\n",
    "\n",
    "# 2. Create full_name column by combining first_name and last_name\n",
    "log_message(\"\\nSTEP 2: Creating full names from first and last names\")\n",
    "\n",
    "# Add safety checks to handle missing values\n",
    "tqdm.pandas(desc=\"Creating full names\")\n",
    "df['full_name'] = df.progress_apply(\n",
    "    lambda row: (str(row['first_name']) + ' ' + str(row['last_name'])) \n",
    "    if pd.notnull(row['first_name']) and pd.notnull(row['last_name']) \n",
    "    else row['first_name'] if pd.notnull(row['first_name']) \n",
    "    else row['last_name'] if pd.notnull(row['last_name']) \n",
    "    else None, \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Count successful full name creations\n",
    "name_count = df['full_name'].notna().sum()\n",
    "log_message(f\"Created {name_count} full names from {len(df)} records ({name_count/len(df)*100:.1f}%)\")\n",
    "\n",
    "# 3. Standardize date_of_birth to MM-DD-YYYY format\n",
    "log_message(\"\\nSTEP 3: Standardizing dates of birth\")\n",
    "\n",
    "def standardize_dob(dob_str):\n",
    "    if pd.isna(dob_str):\n",
    "        return None\n",
    "    \n",
    "    # Handle datetime objects that include time components (from Excel)\n",
    "    if isinstance(dob_str, pd.Timestamp) or isinstance(dob_str, np.datetime64):\n",
    "        return dob_str.strftime('%m-%d-%Y')\n",
    "    \n",
    "    # Convert to string for other processing\n",
    "    dob_str = str(dob_str)\n",
    "    \n",
    "    # Check if the string contains a timestamp part (like '1947-09-14 00:00:00')\n",
    "    if ' 00:00:00' in dob_str:\n",
    "        dob_str = dob_str.split(' ')[0]  # Extract just the date part\n",
    "    \n",
    "    # Try different date formats\n",
    "    formats = [\n",
    "        '%m/%d/%y',   # 4/9/88\n",
    "        '%m/%d/%Y',   # 4/9/1988\n",
    "        '%d/%m/%y',   # 9/4/88\n",
    "        '%d/%m/%Y',   # 9/4/1988\n",
    "        '%Y-%m-%d',   # 1988-04-09\n",
    "        '%Y/%m/%d',   # 1988/04/09\n",
    "        '%m-%d-%Y',   # 04-09-1988\n",
    "        '%d-%m-%Y'    # 09-04-1988\n",
    "    ]\n",
    "    \n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            date_obj = datetime.strptime(dob_str, fmt)\n",
    "            \n",
    "            # Fix for 2-digit years: if year > current year, assume it's 19xx not 20xx\n",
    "            if fmt in ['%m/%d/%y', '%d/%m/%y'] and date_obj.year > datetime.now().year:\n",
    "                date_obj = date_obj.replace(year=date_obj.year - 100)\n",
    "                \n",
    "            # Convert to MM-DD-YYYY format\n",
    "            return date_obj.strftime('%m-%d-%Y')\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # Last resort: check if it's just YYYY-MM-DD format and try to extract\n",
    "    yyyy_mm_dd_pattern = r'(\\d{4})-(\\d{1,2})-(\\d{1,2})'\n",
    "    match = re.match(yyyy_mm_dd_pattern, dob_str)\n",
    "    if match:\n",
    "        year, month, day = match.groups()\n",
    "        try:\n",
    "            return f\"{int(month):02d}-{int(day):02d}-{year}\"\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return dob_str  # Return original string if unable to convert\n",
    "\n",
    "# Apply with progress bar\n",
    "tqdm.pandas(desc=\"Standardizing DOBs\")\n",
    "df['date_of_birth_standardized'] = df['date_of_birth'].progress_apply(standardize_dob)\n",
    "\n",
    "# Count standardized dates\n",
    "std_date_count = df['date_of_birth_standardized'].notna().sum()\n",
    "log_message(f\"Standardized {std_date_count} dates of birth from {len(df)} records ({std_date_count/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Log a sample of original vs. standardized dates for verification\n",
    "sample_size = min(5, len(df))\n",
    "log_message(\"\\nSample of date standardization:\")\n",
    "for i in range(sample_size):\n",
    "    if pd.notna(df.iloc[i]['date_of_birth']) and pd.notna(df.iloc[i]['date_of_birth_standardized']):\n",
    "        log_message(f\"  Original: {df.iloc[i]['date_of_birth']} -> Standardized: {df.iloc[i]['date_of_birth_standardized']}\")\n",
    "\n",
    "# 4. Calculate age from date_of_birth\n",
    "log_message(\"\\nSTEP 4: Calculating ages from dates of birth\")\n",
    "\n",
    "date_format_errors = 0\n",
    "age_range_errors = 0\n",
    "\n",
    "def calculate_age(dob_str):\n",
    "    global date_format_errors, age_range_errors\n",
    "    if pd.isna(dob_str):\n",
    "        return None\n",
    "        \n",
    "    # First, check if it's already a date object (from direct datetime conversion)\n",
    "    if isinstance(dob_str, (datetime, pd.Timestamp, np.datetime64)):\n",
    "        dob = pd.Timestamp(dob_str).to_pydatetime()\n",
    "        today = datetime.now()\n",
    "        age = today.year - dob.year - ((today.month, today.day) < (dob.month, dob.day))\n",
    "        \n",
    "        # Sanity check\n",
    "        if 0 <= age <= 120:\n",
    "            return age\n",
    "        else:\n",
    "            age_range_errors += 1\n",
    "            return None\n",
    "    \n",
    "    # If not a date object, process the string\n",
    "    try:\n",
    "        # Try various formats for standardized date string\n",
    "        for fmt in ['%m-%d-%Y', '%Y-%m-%d', '%m/%d/%Y']:\n",
    "            try:\n",
    "                dob = datetime.strptime(str(dob_str), fmt)\n",
    "                today = datetime.now()\n",
    "                age = today.year - dob.year - ((today.month, today.day) < (dob.month, dob.day))\n",
    "                \n",
    "                # Sanity check: ages should be reasonable (0-120)\n",
    "                if 0 <= age <= 120:\n",
    "                    return age\n",
    "                else:\n",
    "                    age_range_errors += 1\n",
    "                    continue\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "        # If we get here, none of the formats worked or gave reasonable ages\n",
    "        # Try direct parsing for timestamp-like strings\n",
    "        if isinstance(dob_str, str) and ' 00:00:00' in dob_str:\n",
    "            date_part = dob_str.split(' ')[0]\n",
    "            try:\n",
    "                dob = datetime.strptime(date_part, '%Y-%m-%d')\n",
    "                today = datetime.now()\n",
    "                age = today.year - dob.year - ((today.month, today.day) < (dob.month, dob.day))\n",
    "                if 0 <= age <= 120:\n",
    "                    return age\n",
    "                else:\n",
    "                    age_range_errors += 1\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        date_format_errors += 1\n",
    "        return None\n",
    "    except Exception:\n",
    "        date_format_errors += 1\n",
    "        return None\n",
    "\n",
    "# Apply with progress bar\n",
    "tqdm.pandas(desc=\"Calculating ages\")\n",
    "df['age'] = df['date_of_birth_standardized'].progress_apply(calculate_age)\n",
    "\n",
    "# Count successful age calculations\n",
    "age_count = df['age'].notna().sum()\n",
    "log_message(f\"Calculated {age_count} ages from {std_date_count} standardized dates ({age_count/std_date_count*100:.1f}%)\")\n",
    "log_message(f\"Date format errors: {date_format_errors}, Age range errors: {age_range_errors}\")\n",
    "\n",
    "# Log age distribution for verification\n",
    "age_stats = df['age'].describe()\n",
    "log_message(\"\\nAge statistics:\")\n",
    "log_message(f\"  Count: {age_stats['count']:.0f}\")\n",
    "log_message(f\"  Mean: {age_stats['mean']:.1f} years\")\n",
    "log_message(f\"  Min: {age_stats['min']:.0f} years\")\n",
    "log_message(f\"  Max: {age_stats['max']:.0f} years\")\n",
    "\n",
    "# 5. Fix zip codes (ensure they are 5 digits)\n",
    "log_message(\"\\nSTEP 5: Fixing zip codes to 5 digits\")\n",
    "\n",
    "zip_errors = 0\n",
    "\n",
    "def fix_zip_code(zip_code):\n",
    "    global zip_errors\n",
    "    if pd.isna(zip_code):\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Handle various types properly\n",
    "        if isinstance(zip_code, (int, float)):\n",
    "            zip_str = str(int(zip_code))\n",
    "        else:\n",
    "            # Remove any non-numeric characters\n",
    "            zip_str = re.sub(r'[^0-9]', '', str(zip_code))\n",
    "        \n",
    "        # If zip code is less than 5 digits, pad with leading zeros\n",
    "        if len(zip_str) < 5:\n",
    "            return zip_str.zfill(5)\n",
    "        \n",
    "        # If zip code is more than 5 digits, truncate to first 5\n",
    "        if len(zip_str) > 5:\n",
    "            return zip_str[:5]\n",
    "            \n",
    "        return zip_str\n",
    "    except Exception:\n",
    "        zip_errors += 1\n",
    "        return str(zip_code)  # Return original as string if conversion fails\n",
    "\n",
    "# Apply with progress bar\n",
    "tqdm.pandas(desc=\"Fixing zip codes\")\n",
    "df['zip_code_fixed'] = df['zip_code'].progress_apply(fix_zip_code)\n",
    "\n",
    "# Count fixed zip codes\n",
    "zip_count = df['zip_code_fixed'].notna().sum()\n",
    "log_message(f\"Standardized {zip_count} zip codes from {len(df)} records ({zip_count/len(df)*100:.1f}%)\")\n",
    "log_message(f\"Zip code errors: {zip_errors}\")\n",
    "\n",
    "# Preview the data to verify changes\n",
    "log_message(\"\\nPreview of processed data:\")\n",
    "try:\n",
    "    preview_columns = ['course_name', 'course_date', 'course_year', 'first_name', 'last_name', \n",
    "                      'full_name', 'date_of_birth', 'date_of_birth_standardized', 'age', \n",
    "                      'zip_code', 'zip_code_fixed']\n",
    "    \n",
    "    # Filter to only include columns that actually exist\n",
    "    existing_columns = [col for col in preview_columns if col in df.columns]\n",
    "    preview_df = df[existing_columns].head()\n",
    "    log_message(str(preview_df))\n",
    "except Exception as e:\n",
    "    log_message(f\"Error generating preview: {e}\")\n",
    "    # Show what columns are actually available\n",
    "    log_message(f\"Available columns: {df.columns.tolist()}\")\n",
    "\n",
    "# Generate summary statistics\n",
    "log_message(\"\\nData cleaning summary:\")\n",
    "log_message(f\"Total records processed: {len(df)}\")\n",
    "log_message(f\"Course dates extracted: {date_count} ({date_count/len(df)*100:.1f}%)\")\n",
    "log_message(f\"Course years extracted: {year_count} ({year_count/len(df)*100:.1f}%)\")\n",
    "log_message(f\"Full names created: {name_count} ({name_count/len(df)*100:.1f}%)\")\n",
    "log_message(f\"Dates of birth standardized: {std_date_count} ({std_date_count/len(df)*100:.1f}%)\")\n",
    "log_message(f\"Ages calculated: {age_count} ({age_count/len(df)*100:.1f}%)\")\n",
    "log_message(f\"Zip codes fixed: {zip_count} ({zip_count/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Save the cleaned data as Excel\n",
    "output_path = '/Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/ES/2nd_cleaned_data_excel.xlsx'\n",
    "try:\n",
    "    log_message(f\"\\nSaving data to Excel file: {output_path}\")\n",
    "    \n",
    "    # Create a writer object for Excel\n",
    "    with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
    "        # Write the DataFrame to Excel\n",
    "        df.to_excel(writer, index=False, sheet_name='Cleaned Data')\n",
    "        \n",
    "    log_message(f\"Successfully saved cleaned data to '{output_path}'\")\n",
    "except Exception as e:\n",
    "    log_message(f\"Error saving Excel file: {e}\")\n",
    "    \n",
    "    # Try with different path as fallback\n",
    "    try:\n",
    "        fallback_path = 'cleaned_data_excel.xlsx'\n",
    "        log_message(f\"Trying to save to alternate path: {fallback_path}\")\n",
    "        \n",
    "        with pd.ExcelWriter(fallback_path, engine='openpyxl') as writer:\n",
    "            df.to_excel(writer, index=False, sheet_name='Cleaned Data')\n",
    "            \n",
    "        log_message(f\"Saved to alternate path: {fallback_path}\")\n",
    "    except Exception as e2:\n",
    "        log_message(f\"Could not save file: {e2}\")\n",
    "\n",
    "log_message(f\"\\nData cleaning process completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01b0dd2-75b6-4e06-8c7f-e62641376a6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1590dfb-eaf7-41f7-9aca-c7db4ded3b6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3560cf-bbda-4ae0-933e-6386fad7a2c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9dd1b3-ae0a-4159-b6cb-38320239fefe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b67266-e8cf-46d9-8af0-61eff54e8060",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35eb8f6a-b8a0-4c48-bc9e-2798b5ae7d17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71d05843-83e9-4adf-84de-7378c09c7cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from /Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/PCS/Cleaned_Merged_PCS.xlsx...\n",
      "Found 7806 rows of data.\n",
      "Extracting course date and year from course names...\n",
      "Creating full names...\n",
      "Standardizing date of birth formats...\n",
      "Standardizing email addresses...\n",
      "Calculating age at course...\n",
      "Calculating response counts for each column...\n",
      "Saving cleaned data to /Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/PCS/Cleaned_Merged_PCS_2cleaned.xlsx...\n",
      "Data cleaning complete!\n",
      "\n",
      "Column Response Summary:\n",
      "submitted_date: 7647 responses (97.96%)\n",
      "course_name: 7686 responses (98.46%)\n",
      "course_type: 7700 responses (98.64%)\n",
      "first_name: 7806 responses (100.0%)\n",
      "middle_initial: 3165 responses (40.55%)\n",
      "last_name: 7806 responses (100.0%)\n",
      "date_of_birth: 7748 responses (99.26%)\n",
      "email_address: 7746 responses (99.23%)\n",
      "please_describe_your_confidence_level_using_the_internet_to_find_information_you_need: 7748 responses (99.26%)\n",
      "creating_and_sending_emails: 7742 responses (99.18%)\n",
      "opening_and_replying_to_emails: 7744 responses (99.21%)\n",
      "downloading_attachments_i_receive_in_emails_like_documents_or_pictures: 7737 responses (99.12%)\n",
      "adding_an_attachment_to_an_email_i_am_sending: 7736 responses (99.1%)\n",
      "turning_the_computer_on_and_off: 7273 responses (93.17%)\n",
      "adjusting_the_computers_settings_like_the_size_of_the_text_or_the_background_picture: 7266 responses (93.08%)\n",
      "connecting_the_computer_to_wifi: 7272 responses (93.16%)\n",
      "installing_applications_from_the_chromebook_store: 7266 responses (93.08%)\n",
      "saving_files_to_the_computer_and_finding_them_later: 7263 responses (93.04%)\n",
      "using_a_mouse_headset_andor_other_accessories: 7272 responses (93.16%)\n",
      "turning_the_tablet_on_and_off: 481 responses (6.16%)\n",
      "adjusting_the_tablets_settings_like_the_size_of_the_text_or_the_background_picture: 480 responses (6.15%)\n",
      "connecting_the_tablet_to_wifi: 480 responses (6.15%)\n",
      "installing_applications_apps: 480 responses (6.15%)\n",
      "saving_files_to_the_tablet_and_finding_them_later: 481 responses (6.16%)\n",
      "findingresearching_general_information_online_eg_google: 7748 responses (99.26%)\n",
      "finding_housing_eg_shelter_apartment_huntingrentals_sublets: 7748 responses (99.26%)\n",
      "accessing_health_and_wellness_information_and_resources_eg_telehealth_health_portals_managing_insurance_online: 7748 responses (99.26%)\n",
      "managing_finances_online_eg_create_budgets_online_banking_paying_bills: 7748 responses (99.26%)\n",
      "accessing_localcity_resources_eg_food_assistance: 7748 responses (99.26%)\n",
      "civic_engagement_eg_access_voting_information_filling_out_the_census_attending_virtual_towncity_meetings: 7748 responses (99.26%)\n",
      "education_for_myself_eg_schoolwork_applying_to_school: 7748 responses (99.26%)\n",
      "helping_my_child_with_school: 7748 responses (99.26%)\n",
      "using_job_resources_eg_indeed_linkedin_resume_building_applying_for_jobs_online: 7748 responses (99.26%)\n",
      "using_digital_skills_for_workcurrent_job: 7747 responses (99.24%)\n",
      "data_entry_eg_google_sheets_microsoft_excel: 7486 responses (95.9%)\n",
      "word_processing_andor_presentation_skills_eg_google_docs_google_slides: 7578 responses (97.08%)\n",
      "communicating_with_others_by_email: 7635 responses (97.81%)\n",
      "using_social_media_eg_facebook_instagram_twitter: 7538 responses (96.57%)\n",
      "video_chat_and_or_other_visual_communication_tools_eg_zoom_skype_google_hangoutchat: 7555 responses (96.78%)\n",
      "what_was_the_best_part_of_your_tgh_course_please_describe_your_experience: 5940 responses (76.1%)\n",
      "do_you_plan_to_participate_in_future_activities_at_the_site_where_you_took_tgh: 7748 responses (99.26%)\n",
      "i_learned_skills_during_my_tgh_courses_that_can_help_me_improve_my_life: 7590 responses (97.23%)\n",
      "how_do_you_think_your_life_will_improve: 6637 responses (85.02%)\n",
      "please_share_why_you_selected_strongly_disagree_or_disagree_for_the_statement_above: 240 responses (3.07%)\n",
      "what_other_digital_skills_would_you_like_to_learn: 5163 responses (66.14%)\n",
      "overall_how_satisfied_were_you_with_your_tgh_experience: 7747 responses (99.24%)\n",
      "how_satisfied_were_you_with_your_tgh_instructors: 7733 responses (99.06%)\n",
      "what_would_you_tell_a_friend_about_the_tgh_course: 5902 responses (75.61%)\n",
      "anything_else_you_would_like_to_add_about_your_tgh_course_or_instructor: 4854 responses (62.18%)\n",
      "internet_access: 7741 responses (99.17%)\n",
      "why_did_you_request_a_hotspot_instead_of_home_internet_service_through_comcast_internet_essentials: 965 responses (12.36%)\n",
      "did_you_request_a_tghsponsored_comcast_internet_essentials_promo_code: 7747 responses (99.24%)\n",
      "was_comcast_internet_essentials_successfully_set_up_in_your_home: 1050 responses (13.45%)\n",
      "how_could_your_internet_setup_experience_have_been_improved: 276 responses (3.54%)\n",
      "why_wasnt_internet_service_set_up_in_your_home: 231 responses (2.96%)\n",
      "are_you_aware_of_the_affordable_connectivity_program_acp_an_fcc_federal_communications_commission_program_to_help_families_and_households_afford_internet_service_more_info_here_httpswwwfccgovacp: 5999 responses (76.85%)\n",
      "are_you_enrolled_in_the_acp_program: 5983 responses (76.65%)\n",
      "i_feel_the_course_content_was_presented_for: 883 responses (11.31%)\n",
      "do_you_do_educational_activities_with_your_child_more_often_than_you_did_before_taking_tgh: 883 responses (11.31%)\n",
      "i_am_aware_of_what_my_skills_are_to_be_employed_in_a_good_job_eg_digital_skills_specific_trade_skills_etc: 2434 responses (31.18%)\n",
      "i_am_aware_of_what_my_resources_are_to_be_employed_in_a_good_job_eg_resources_for_job_searching_child_care_resources_etc: 2420 responses (31.0%)\n",
      "i_am_able_to_utilize_my_skills_to_move_toward_career_goals: 2417 responses (30.96%)\n",
      "i_am_able_to_utilize_my_resources_to_move_toward_career_goals: 2399 responses (30.73%)\n",
      "even_if_i_am_not_able_to_achieve_my_financial_goals_right_away_i_will_find_a_way_to_get_there: 2389 responses (30.6%)\n",
      "because_of_my_money_situation_i_feel_like_i_will_never_have_the_things_i_want_in_life: 2405 responses (30.81%)\n",
      "i_am_just_getting_by_financially: 2392 responses (30.64%)\n",
      "i_am_concerned_that_the_money_i_have_or_will_save_wont_last: 2378 responses (30.46%)\n",
      "since_participating_in_the_tech_goes_home_have_you_check_all_that_apply: 1144 responses (14.66%)\n",
      "did_tgh_make_it_more_likely_that_you_will_use_the_internet_for_any_of_the_following_select_all_that_apply: 223 responses (2.86%)\n",
      "other_TGH_Internet_use: 5 responses (0.06%)\n",
      "did_tgh_help_you_create_a_website_for_your_business_or_entrepreneurial_project: 246 responses (3.15%)\n",
      "what_is_the_web_address_url_of_the_website: 6 responses (0.08%)\n",
      "did_tgh_help_you_use_social_media_for_your_business_or_entrepreneurial_project: 245 responses (3.14%)\n",
      "please_share_anyall_of_your_social_media_handles_separated_by_a_comma_eg_techgoeshome_facebookcomyourbusinessname: 180 responses (2.31%)\n",
      "course_id: 7553 responses (96.76%)\n",
      "form_name: 5524 responses (70.77%)\n",
      "creation_date: 7806 responses (100.0%)\n",
      "modified_date: 7806 responses (100.0%)\n",
      "completion_time: 7806 responses (100.0%)\n",
      "response_html_beta: 0 responses (0.0%)\n",
      "response_html: 0 responses (0.0%)\n",
      "response_text: 0 responses (0.0%)\n",
      "response: 0 responses (0.0%)\n",
      "response_url: 7806 responses (100.0%)\n",
      "resume_email: 0 responses (0.0%)\n",
      "file_list: 0 responses (0.0%)\n",
      "unprotected_file_list: 0 responses (0.0%)\n",
      "response_id: 7806 responses (100.0%)\n",
      "referrer: 7804 responses (99.97%)\n",
      "ip_address: 7806 responses (100.0%)\n",
      "Unnamed: 90: 0 responses (0.0%)\n",
      "Unnamed: 91: 0 responses (0.0%)\n",
      "# of responses: 7806 responses (100.0%)\n",
      "course_date: 6922 responses (88.68%)\n",
      "course_year: 6922 responses (88.68%)\n",
      "full_name: 7806 responses (100.0%)\n",
      "age_at_course: 6839 responses (87.61%)\n",
      "\n",
      "Cleaned data has been saved successfully!\n"
     ]
    }
   ],
   "source": [
    "#POST course data cleaning\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "def clean_excel_data(input_file):\n",
    "    \"\"\"\n",
    "    Clean and transform Excel data according to the specified requirements.\n",
    "    \n",
    "    Parameters:\n",
    "    input_file (str): Path to the input Excel file\n",
    "    \"\"\"\n",
    "    print(f\"Reading data from {input_file}...\")\n",
    "    \n",
    "    # Read the Excel file\n",
    "    df = pd.read_excel(input_file)\n",
    "    original_row_count = len(df)\n",
    "    print(f\"Found {original_row_count} rows of data.\")\n",
    "    \n",
    "    # Create output file path in the same directory\n",
    "    input_dir = os.path.dirname(input_file)\n",
    "    input_filename = os.path.basename(input_file)\n",
    "    base_name = os.path.splitext(input_filename)[0]\n",
    "    output_file = os.path.join(input_dir, f\"{base_name}_2cleaned.xlsx\")\n",
    "    \n",
    "    # 1. Extract course_date and course_year from course_name\n",
    "    print(\"Extracting course date and year from course names...\")\n",
    "    \n",
    "    def extract_date_from_course_name(course_name):\n",
    "        if pd.isna(course_name):\n",
    "            return None\n",
    "            \n",
    "        course_str = str(course_name)\n",
    "        \n",
    "        # Look for standard date patterns like M/D/YY or MM/DD/YY\n",
    "        date_pattern = r'\\b(\\d{1,2}/\\d{1,2}/\\d{2})\\b'\n",
    "        match = re.search(date_pattern, course_str)\n",
    "        \n",
    "        if match:\n",
    "            date_str = match.group(1)\n",
    "            try:\n",
    "                date_obj = pd.to_datetime(date_str, format='%m/%d/%y')\n",
    "                return date_obj.strftime('%m/%d/%Y')\n",
    "            except:\n",
    "                return date_str\n",
    "        \n",
    "        # Look for dates without separators like 21524 (2/15/24)\n",
    "        no_sep_pattern = r'\\b(\\d{5,6})\\b'\n",
    "        matches = re.findall(no_sep_pattern, course_str)\n",
    "        \n",
    "        for match in matches:\n",
    "            if len(match) == 5:  # Format: MDDYY\n",
    "                try:\n",
    "                    month = int(match[0:1])\n",
    "                    day = int(match[1:3])\n",
    "                    year = int(match[3:5])\n",
    "                    \n",
    "                    if 1 <= month <= 12 and 1 <= day <= 31:\n",
    "                        date_str = f\"{month}/{day}/{year}\"\n",
    "                        date_obj = pd.to_datetime(date_str, format='%m/%d/%y')\n",
    "                        return date_obj.strftime('%m/%d/%Y')\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            elif len(match) == 6:  # Format: MMDDYY\n",
    "                try:\n",
    "                    month = int(match[0:2])\n",
    "                    day = int(match[2:4])\n",
    "                    year = int(match[4:6])\n",
    "                    \n",
    "                    if 1 <= month <= 12 and 1 <= day <= 31:\n",
    "                        date_str = f\"{month}/{day}/{year}\"\n",
    "                        date_obj = pd.to_datetime(date_str, format='%m/%d/%y')\n",
    "                        return date_obj.strftime('%m/%d/%Y')\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def extract_year_from_date(date_str):\n",
    "        if pd.isna(date_str):\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            # Extract the year from the standardized date (MM/DD/YYYY)\n",
    "            date_parts = date_str.split('/')\n",
    "            if len(date_parts) == 3:\n",
    "                return int(date_parts[2])\n",
    "            return None\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    # Apply the date extraction and conversion\n",
    "    df['course_date'] = df['course_name'].apply(extract_date_from_course_name)\n",
    "    df['course_year'] = df['course_date'].apply(extract_year_from_date)\n",
    "    \n",
    "    # 2. Create full_name by joining first_name and last_name\n",
    "    print(\"Creating full names...\")\n",
    "    \n",
    "    # Handle potential missing values\n",
    "    df['first_name'] = df['first_name'].fillna('')\n",
    "    df['last_name'] = df['last_name'].fillna('')\n",
    "    df['full_name'] = df.apply(lambda row: (row['first_name'] + ' ' + row['last_name']).strip(), axis=1)\n",
    "    \n",
    "    # 3. Standardize date of birth column\n",
    "    print(\"Standardizing date of birth formats...\")\n",
    "    \n",
    "    def standardize_dob(dob):\n",
    "        if pd.isna(dob):\n",
    "            return None\n",
    "            \n",
    "        dob_str = str(dob).strip().replace('-', '/').replace('.', '/').replace(' ', '')\n",
    "        \n",
    "        # Try to handle various formats\n",
    "        \n",
    "        # Check if it's already in a standard format\n",
    "        try:\n",
    "            date_obj = pd.to_datetime(dob_str, errors='raise')\n",
    "            # For two-digit years, ensure proper century (1900s not 2000s for older dates)\n",
    "            if date_obj.year > datetime.now().year:\n",
    "                date_obj = date_obj.replace(year=date_obj.year - 100)\n",
    "            return date_obj.strftime('%m-%d-%Y')\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Handle numeric-only formats without separators\n",
    "        if dob_str.isdigit():\n",
    "            # MMDDYYYY format (8 digits)\n",
    "            if len(dob_str) == 8:\n",
    "                month = int(dob_str[0:2])\n",
    "                day = int(dob_str[2:4])\n",
    "                year = int(dob_str[4:8])\n",
    "                \n",
    "                if 1 <= month <= 12 and 1 <= day <= 31:\n",
    "                    try:\n",
    "                        return f\"{month:02d}-{day:02d}-{year}\"\n",
    "                    except:\n",
    "                        pass\n",
    "            \n",
    "            # MMDDYY format (6 digits)\n",
    "            elif len(dob_str) == 6:\n",
    "                month = int(dob_str[0:2])\n",
    "                day = int(dob_str[2:4])\n",
    "                year = int(dob_str[4:6])\n",
    "                \n",
    "                if 1 <= month <= 12 and 1 <= day <= 31:\n",
    "                    # Adjust years to 19XX or 20XX\n",
    "                    full_year = 1900 + year if year >= 50 else 2000 + year\n",
    "                    try:\n",
    "                        return f\"{month:02d}-{day:02d}-{full_year}\"\n",
    "                    except:\n",
    "                        pass\n",
    "            \n",
    "            # MDDYYYY format (7 digits)\n",
    "            elif len(dob_str) == 7:\n",
    "                month = int(dob_str[0:1])\n",
    "                day = int(dob_str[1:3])\n",
    "                year = int(dob_str[3:7])\n",
    "                \n",
    "                if 1 <= month <= 12 and 1 <= day <= 31:\n",
    "                    try:\n",
    "                        return f\"{month:02d}-{day:02d}-{year}\"\n",
    "                    except:\n",
    "                        pass\n",
    "        \n",
    "        # Formats with separators (M/D/YY, MM/DD/YYYY, etc.)\n",
    "        if '/' in dob_str:\n",
    "            parts = dob_str.split('/')\n",
    "            if len(parts) == 3:\n",
    "                month, day, year = parts\n",
    "                \n",
    "                # Handle 2-digit years\n",
    "                if len(year) == 2:\n",
    "                    year_int = int(year)\n",
    "                    full_year = 1900 + year_int if year_int >= 50 else 2000 + year_int\n",
    "                else:\n",
    "                    full_year = int(year)\n",
    "                \n",
    "                try:\n",
    "                    month_int = int(month)\n",
    "                    day_int = int(day)\n",
    "                    if 1 <= month_int <= 12 and 1 <= day_int <= 31:\n",
    "                        return f\"{month_int:02d}-{day_int:02d}-{full_year}\"\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        # If we couldn't parse it properly, return the original string\n",
    "        return dob_str\n",
    "    \n",
    "    df['date_of_birth'] = df['date_of_birth'].apply(standardize_dob)\n",
    "    \n",
    "    # 4. Standardize email addresses\n",
    "    print(\"Standardizing email addresses...\")\n",
    "    \n",
    "    def standardize_email(email):\n",
    "        if pd.isna(email):\n",
    "            return None\n",
    "        return str(email).lower().strip()\n",
    "    \n",
    "    # Ensure we're using the correct column name for email\n",
    "    if 'email_address' in df.columns:\n",
    "        df['email_address'] = df['email_address'].apply(standardize_email)\n",
    "    \n",
    "    # 5. Calculate age at course\n",
    "    print(\"Calculating age at course...\")\n",
    "    \n",
    "    def calculate_age(dob, course_year):\n",
    "        if pd.isna(dob) or pd.isna(course_year):\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Parse the DOB\n",
    "            dob_obj = pd.to_datetime(dob, format='%m-%d-%Y')\n",
    "            \n",
    "            # Calculate age based on the course year\n",
    "            age = course_year - dob_obj.year\n",
    "            \n",
    "            # Adjust age if the birthday hasn't occurred yet in the course year\n",
    "            # We'll consider the person's age at December 31st of the course year\n",
    "            # as we don't have specific course dates for all entries\n",
    "            return age\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    df['age_at_course'] = df.apply(lambda row: calculate_age(row['date_of_birth'], row['course_year']), axis=1)\n",
    "    \n",
    "    # 6. Count responses for each column\n",
    "    print(\"Calculating response counts for each column...\")\n",
    "    response_counts = df.count()\n",
    "    \n",
    "    # Save the processed data\n",
    "    print(f\"Saving cleaned data to {output_file}...\")\n",
    "    \n",
    "    # Create a new Excel writer\n",
    "    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "        # Write the main data\n",
    "        df.to_excel(writer, sheet_name='Cleaned Data', index=False)\n",
    "        \n",
    "        # Create a summary DataFrame for the counts\n",
    "        summary_df = pd.DataFrame({\n",
    "            'Column': response_counts.index,\n",
    "            'Response Count': response_counts.values,\n",
    "            'Percentage': (response_counts.values / original_row_count * 100).round(2),\n",
    "            'Missing Values': original_row_count - response_counts.values\n",
    "        })\n",
    "        \n",
    "        # Write the summary to a new sheet\n",
    "        summary_df.to_excel(writer, sheet_name='Response Summary', index=False)\n",
    "    \n",
    "    print(\"Data cleaning complete!\")\n",
    "    print(\"\\nColumn Response Summary:\")\n",
    "    for col, count, pct in zip(summary_df['Column'], summary_df['Response Count'], summary_df['Percentage']):\n",
    "        print(f\"{col}: {count} responses ({pct}%)\")\n",
    "    \n",
    "    return df, summary_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = '/Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/PCS/Cleaned_Merged_PCS.xlsx'\n",
    "    df, summary = clean_excel_data(input_file)\n",
    "    print(f\"\\nCleaned data has been saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20b0e915-5f18-4f52-883a-b36dd526e371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-enrollment survey data...\n",
      "Loading post-course survey data...\n",
      "Pre-enrollment survey: 29700 entries\n",
      "Post-course survey: 7806 entries\n",
      "Pre-enrollment survey: 24446 entries with valid emails\n",
      "Post-course survey: 7746 entries with valid emails\n",
      "Pre-enrollment survey: 24446 unique emails\n",
      "Post-course survey: 7746 unique emails\n",
      "\n",
      "Found 3189 matching emails between datasets\n",
      "Match rate: 13.05% of pre-enrollment emails\n",
      "Match rate: 41.17% of post-course emails\n",
      "\n",
      "Sample of matched emails (first 5):\n",
      "1. milliee38@gmail.com\n",
      "2. shanakawap@yahoo.com\n",
      "3. griseldacruz49@hotmail.com\n",
      "4. johnrossfrancis9@gmail.com\n",
      "5. zpt00200@gmail.com\n",
      "\n",
      "Saved match analysis to: /Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/ES/survey_match_analysis.xlsx\n"
     ]
    }
   ],
   "source": [
    "#pre and post merge stats\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def analyze_survey_matches(pre_course_path, post_course_path):\n",
    "    \"\"\"\n",
    "    Analyze how many learners completed both pre-enrollment and post-course surveys\n",
    "    by matching email addresses between the two datasets.\n",
    "    \n",
    "    Parameters:\n",
    "    pre_course_path (str): Path to the pre-enrollment survey Excel file\n",
    "    post_course_path (str): Path to the post-course survey Excel file\n",
    "    \"\"\"\n",
    "    print(\"Loading pre-enrollment survey data...\")\n",
    "    pre_df = pd.read_excel(pre_course_path)\n",
    "    \n",
    "    print(\"Loading post-course survey data...\")\n",
    "    post_df = pd.read_excel(post_course_path)\n",
    "    \n",
    "    # Get total counts\n",
    "    pre_total = len(pre_df)\n",
    "    post_total = len(post_df)\n",
    "    \n",
    "    print(f\"Pre-enrollment survey: {pre_total} entries\")\n",
    "    print(f\"Post-course survey: {post_total} entries\")\n",
    "    \n",
    "    # Check if email_address column exists in both datasets\n",
    "    if 'email_address' not in pre_df.columns:\n",
    "        print(\"Error: 'email_address' column not found in pre-enrollment dataset\")\n",
    "        print(f\"Available columns: {pre_df.columns.tolist()}\")\n",
    "        return\n",
    "    \n",
    "    if 'email_address' not in post_df.columns:\n",
    "        print(\"Error: 'email_address' column not found in post-course dataset\")\n",
    "        print(f\"Available columns: {post_df.columns.tolist()}\")\n",
    "        return\n",
    "    \n",
    "    # Clean email addresses to ensure fair comparison\n",
    "    def clean_email(email):\n",
    "        if pd.isna(email):\n",
    "            return None\n",
    "        return str(email).lower().strip()\n",
    "    \n",
    "    pre_df['email_clean'] = pre_df['email_address'].apply(clean_email)\n",
    "    post_df['email_clean'] = post_df['email_address'].apply(clean_email)\n",
    "    \n",
    "    # Remove null emails\n",
    "    pre_df_valid = pre_df.dropna(subset=['email_clean'])\n",
    "    post_df_valid = post_df.dropna(subset=['email_clean'])\n",
    "    \n",
    "    pre_valid_count = len(pre_df_valid)\n",
    "    post_valid_count = len(post_df_valid)\n",
    "    \n",
    "    print(f\"Pre-enrollment survey: {pre_valid_count} entries with valid emails\")\n",
    "    print(f\"Post-course survey: {post_valid_count} entries with valid emails\")\n",
    "    \n",
    "    # Get unique emails\n",
    "    pre_emails = set(pre_df_valid['email_clean'])\n",
    "    post_emails = set(post_df_valid['email_clean'])\n",
    "    \n",
    "    pre_unique_count = len(pre_emails)\n",
    "    post_unique_count = len(post_emails)\n",
    "    \n",
    "    print(f\"Pre-enrollment survey: {pre_unique_count} unique emails\")\n",
    "    print(f\"Post-course survey: {post_unique_count} unique emails\")\n",
    "    \n",
    "    # Find matching emails\n",
    "    matching_emails = pre_emails.intersection(post_emails)\n",
    "    matching_count = len(matching_emails)\n",
    "    \n",
    "    print(f\"\\nFound {matching_count} matching emails between datasets\")\n",
    "    print(f\"Match rate: {(matching_count / pre_unique_count * 100):.2f}% of pre-enrollment emails\")\n",
    "    print(f\"Match rate: {(matching_count / post_unique_count * 100):.2f}% of post-course emails\")\n",
    "    \n",
    "    # Create a merged dataset with only the matching records\n",
    "    pre_matched = pre_df_valid[pre_df_valid['email_clean'].isin(matching_emails)]\n",
    "    post_matched = post_df_valid[post_df_valid['email_clean'].isin(matching_emails)]\n",
    "    \n",
    "    # Get a sample of matched emails for verification\n",
    "    sample_size = min(5, matching_count)\n",
    "    sample_emails = list(matching_emails)[:sample_size]\n",
    "    \n",
    "    print(f\"\\nSample of matched emails (first {sample_size}):\")\n",
    "    for i, email in enumerate(sample_emails, 1):\n",
    "        print(f\"{i}. {email}\")\n",
    "    \n",
    "    # Save match report\n",
    "    output_dir = os.path.dirname(pre_course_path)\n",
    "    output_file = os.path.join(output_dir, \"survey_match_analysis.xlsx\")\n",
    "    \n",
    "    # Create a summary dataframe\n",
    "    summary_data = {\n",
    "        'Metric': [\n",
    "            'Total Pre-Enrollment Records',\n",
    "            'Total Post-Course Records',\n",
    "            'Valid Pre-Enrollment Emails',\n",
    "            'Valid Post-Course Emails',\n",
    "            'Unique Pre-Enrollment Emails',\n",
    "            'Unique Post-Course Emails',\n",
    "            'Matching Emails',\n",
    "            'Pre-Enrollment Match Rate',\n",
    "            'Post-Course Match Rate'\n",
    "        ],\n",
    "        'Value': [\n",
    "            pre_total,\n",
    "            post_total,\n",
    "            pre_valid_count,\n",
    "            post_valid_count,\n",
    "            pre_unique_count,\n",
    "            post_unique_count,\n",
    "            matching_count,\n",
    "            f\"{(matching_count / pre_unique_count * 100):.2f}%\",\n",
    "            f\"{(matching_count / post_unique_count * 100):.2f}%\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    # Create a DataFrame with the list of matching emails\n",
    "    matches_df = pd.DataFrame({'Matching Emails': list(matching_emails)})\n",
    "    \n",
    "    # Save to Excel\n",
    "    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "        summary_df.to_excel(writer, sheet_name='Match Summary', index=False)\n",
    "        matches_df.to_excel(writer, sheet_name='Matching Emails', index=False)\n",
    "        \n",
    "        # Also save a sample of pre and post data for matched emails\n",
    "        if matching_count > 0:\n",
    "            sample_email = sample_emails[0]\n",
    "            pre_sample = pre_df_valid[pre_df_valid['email_clean'] == sample_email]\n",
    "            post_sample = post_df_valid[post_df_valid['email_clean'] == sample_email]\n",
    "            \n",
    "            sample_comparison = pd.DataFrame({\n",
    "                'Field': ['Email Address'],\n",
    "                'Pre-Enrollment Value': [sample_email],\n",
    "                'Post-Course Value': [sample_email]\n",
    "            })\n",
    "            \n",
    "            sample_comparison.to_excel(writer, sheet_name='Sample Comparison', index=False)\n",
    "    \n",
    "    print(f\"\\nSaved match analysis to: {output_file}\")\n",
    "    \n",
    "    return {\n",
    "        'pre_total': pre_total,\n",
    "        'post_total': post_total,\n",
    "        'matching_count': matching_count,\n",
    "        'matching_emails': matching_emails,\n",
    "        'pre_df': pre_df,\n",
    "        'post_df': post_df\n",
    "    }\n",
    "\n",
    "def merge_survey_data(results, output_path=None):\n",
    "    \"\"\"\n",
    "    Merge the pre-enrollment and post-course survey data based on matching emails.\n",
    "    \n",
    "    Parameters:\n",
    "    results (dict): Results from analyze_survey_matches\n",
    "    output_path (str, optional): Path to save the merged data\n",
    "    \"\"\"\n",
    "    if results['matching_count'] == 0:\n",
    "        print(\"No matching emails found. Cannot merge datasets.\")\n",
    "        return None\n",
    "    \n",
    "    pre_df = results['pre_df']\n",
    "    post_df = results['post_df']\n",
    "    \n",
    "    # Prepare for merge\n",
    "    pre_df_for_merge = pre_df.copy()\n",
    "    post_df_for_merge = post_df.copy()\n",
    "    \n",
    "    # Rename columns in post_df to avoid duplicates\n",
    "    post_columns = post_df_for_merge.columns\n",
    "    post_columns_renamed = [f\"post_{col}\" if col != 'email_clean' and col != 'email_address' else col for col in post_columns]\n",
    "    post_df_for_merge.columns = post_columns_renamed\n",
    "    \n",
    "    # Merge datasets on cleaned email\n",
    "    merged_df = pd.merge(\n",
    "        pre_df_for_merge, \n",
    "        post_df_for_merge,\n",
    "        on='email_clean',\n",
    "        how='inner',\n",
    "        suffixes=('_pre', '_post')\n",
    "    )\n",
    "    \n",
    "    # If there are duplicate email_address columns, keep only one\n",
    "    if 'email_address_pre' in merged_df.columns and 'email_address_post' in merged_df.columns:\n",
    "        merged_df.drop('email_address_post', axis=1, inplace=True)\n",
    "        merged_df.rename(columns={'email_address_pre': 'email_address'}, inplace=True)\n",
    "    \n",
    "    # Save to Excel if output path provided\n",
    "    if output_path:\n",
    "        merged_df.to_excel(output_path, index=False)\n",
    "        print(f\"Merged data saved to: {output_path}\")\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pre_course_path = '/Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/ES/Cleaned & Enhanced ES Data .xlsx'\n",
    "    post_course_path = '/Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/PCS/Cleaned_Merged_PCS_2cleaned.xlsx'\n",
    "    \n",
    "    # Analyze matches\n",
    "    results = analyze_survey_matches(pre_course_path, post_course_path)\n",
    "    \n",
    "    # If you want to merge the datasets, uncomment this section\n",
    "    # output_dir = os.path.dirname(pre_course_path)\n",
    "    # merged_output_path = os.path.join(output_dir, \"Merged_Surveys.xlsx\")\n",
    "    # merged_df = merge_survey_data(results, merged_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f6a0e59-4a94-4b2e-b57e-d9ae5ed3ca99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fuzzywuzzy in /opt/anaconda3/lib/python3.12/site-packages (0.18.0)\n",
      "Collecting python-Levenshtein\n",
      "  Downloading python_levenshtein-0.27.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting Levenshtein==0.27.1 (from python-Levenshtein)\n",
      "  Downloading levenshtein-0.27.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from Levenshtein==0.27.1->python-Levenshtein) (3.10.1)\n",
      "Downloading python_levenshtein-0.27.1-py3-none-any.whl (9.4 kB)\n",
      "Downloading levenshtein-0.27.1-cp312-cp312-macosx_11_0_arm64.whl (156 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.4/156.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: Levenshtein, python-Levenshtein\n",
      "Successfully installed Levenshtein-0.27.1 python-Levenshtein-0.27.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fuzzywuzzy python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "337fa781-2b58-400b-b060-b629715ef8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-enrollment survey data...\n",
      "Loading post-course survey data...\n",
      "Pre-enrollment survey: 29700 entries\n",
      "Post-course survey: 7806 entries\n",
      "Pre-enrollment survey: 24433 entries with valid emails\n",
      "Post-course survey: 5462 entries with valid emails\n",
      "Pre-enrollment survey: 24412 unique emails\n",
      "Post-course survey: 5462 unique emails\n",
      "\n",
      "Found 3208 exact matching emails between datasets\n",
      "\n",
      "Performing fuzzy matching to find additional potential matches...\n",
      "Warning: Too many emails for full fuzzy matching. Limiting comparison to 5000 emails.\n",
      "Found 4319 additional potential matches through fuzzy matching\n",
      "\n",
      "Total matching emails: 7527\n",
      "Exact match rate: 13.14% of pre-enrollment emails\n",
      "Exact match rate: 58.73% of post-course emails\n",
      "Total match rate: 30.83% of pre-enrollment emails\n",
      "Total match rate: 137.81% of post-course emails\n",
      "\n",
      "Analyzing unmatched emails for patterns...\n",
      "\n",
      "Top 5 domains in pre-enrollment emails:\n",
      "  gmail.com: 18997 emails\n",
      "  yahoo.com: 1742 emails\n",
      "  hotmail.com: 1173 emails\n",
      "  icloud.com: 432 emails\n",
      "  aol.com: 219 emails\n",
      "\n",
      "Top 5 domains in post-course emails:\n",
      "  gmail.com: 4699 emails\n",
      "  yahoo.com: 219 emails\n",
      "  hotmail.com: 135 emails\n",
      "  icloud.com: 59 emails\n",
      "  bpsadulted.com: 39 emails\n",
      "\n",
      "Saved enhanced match analysis to: /Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/ES/enhanced_survey_match_analysis.xlsx\n"
     ]
    }
   ],
   "source": [
    "#using fuzzy matching for further matching\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from fuzzywuzzy import fuzz, process  # For fuzzy matching\n",
    "\n",
    "def analyze_survey_matches(pre_course_path, post_course_path, fuzzy_match=True, similarity_threshold=85):\n",
    "    \"\"\"\n",
    "    Analyze how many learners completed both pre-enrollment and post-course surveys\n",
    "    by matching email addresses between the two datasets with enhanced normalization.\n",
    "    \n",
    "    Parameters:\n",
    "    pre_course_path (str): Path to the pre-enrollment survey Excel file\n",
    "    post_course_path (str): Path to the post-course survey Excel file\n",
    "    fuzzy_match (bool): Whether to use fuzzy matching for emails\n",
    "    similarity_threshold (int): Threshold for fuzzy matching (0-100)\n",
    "    \"\"\"\n",
    "    print(\"Loading pre-enrollment survey data...\")\n",
    "    pre_df = pd.read_excel(pre_course_path)\n",
    "    \n",
    "    print(\"Loading post-course survey data...\")\n",
    "    post_df = pd.read_excel(post_course_path)\n",
    "    \n",
    "    # Get total counts\n",
    "    pre_total = len(pre_df)\n",
    "    post_total = len(post_df)\n",
    "    \n",
    "    print(f\"Pre-enrollment survey: {pre_total} entries\")\n",
    "    print(f\"Post-course survey: {post_total} entries\")\n",
    "    \n",
    "    # Check if email_address column exists in both datasets\n",
    "    if 'email_address' not in pre_df.columns:\n",
    "        print(\"Error: 'email_address' column not found in pre-enrollment dataset\")\n",
    "        print(f\"Available columns: {pre_df.columns.tolist()}\")\n",
    "        return\n",
    "    \n",
    "    if 'email_address' not in post_df.columns:\n",
    "        print(\"Error: 'email_address' column not found in post-course dataset\")\n",
    "        print(f\"Available columns: {post_df.columns.tolist()}\")\n",
    "        return\n",
    "    \n",
    "    # Enhanced email normalization\n",
    "    def normalize_email(email):\n",
    "        if pd.isna(email):\n",
    "            return None\n",
    "        \n",
    "        # Convert to string and lowercase\n",
    "        email_str = str(email).lower().strip()\n",
    "        \n",
    "        # Remove any spaces\n",
    "        email_str = email_str.replace(' ', '')\n",
    "        \n",
    "        # Remove common typos or variations\n",
    "        email_str = email_str.replace('gmail.con', 'gmail.com')\n",
    "        email_str = email_str.replace('yaho.com', 'yahoo.com')\n",
    "        email_str = email_str.replace('yahooo.com', 'yahoo.com')\n",
    "        email_str = email_str.replace('hotmial.com', 'hotmail.com')\n",
    "        email_str = email_str.replace('hotmal.com', 'hotmail.com')\n",
    "        email_str = email_str.replace('gamil.com', 'gmail.com')\n",
    "        email_str = email_str.replace('gnail.com', 'gmail.com')\n",
    "        \n",
    "        # Remove any non-email characters that might have been added\n",
    "        email_str = re.sub(r'[^\\w@.-]', '', email_str)\n",
    "        \n",
    "        # If it doesn't look like an email at all, return None\n",
    "        if '@' not in email_str or '.' not in email_str:\n",
    "            return None\n",
    "            \n",
    "        return email_str\n",
    "    \n",
    "    pre_df['email_normalized'] = pre_df['email_address'].apply(normalize_email)\n",
    "    post_df['email_normalized'] = post_df['email_address'].apply(normalize_email)\n",
    "    \n",
    "    # Remove null emails\n",
    "    pre_df_valid = pre_df.dropna(subset=['email_normalized'])\n",
    "    post_df_valid = post_df.dropna(subset=['email_normalized'])\n",
    "    \n",
    "    pre_valid_count = len(pre_df_valid)\n",
    "    post_valid_count = len(post_df_valid)\n",
    "    \n",
    "    print(f\"Pre-enrollment survey: {pre_valid_count} entries with valid emails\")\n",
    "    print(f\"Post-course survey: {post_valid_count} entries with valid emails\")\n",
    "    \n",
    "    # Get unique emails\n",
    "    pre_emails = set(pre_df_valid['email_normalized'])\n",
    "    post_emails = set(post_df_valid['email_normalized'])\n",
    "    \n",
    "    pre_unique_count = len(pre_emails)\n",
    "    post_unique_count = len(post_emails)\n",
    "    \n",
    "    print(f\"Pre-enrollment survey: {pre_unique_count} unique emails\")\n",
    "    print(f\"Post-course survey: {post_unique_count} unique emails\")\n",
    "    \n",
    "    # First find exact matches\n",
    "    exact_matching_emails = pre_emails.intersection(post_emails)\n",
    "    exact_matching_count = len(exact_matching_emails)\n",
    "    \n",
    "    print(f\"\\nFound {exact_matching_count} exact matching emails between datasets\")\n",
    "    \n",
    "    # Additional matches through fuzzy matching if requested\n",
    "    fuzzy_matches = {}\n",
    "    additional_matches_count = 0\n",
    "    \n",
    "    if fuzzy_match:\n",
    "        print(\"\\nPerforming fuzzy matching to find additional potential matches...\")\n",
    "        \n",
    "        # Only try to fuzzy match emails that didn't get an exact match\n",
    "        pre_unmatched = pre_emails - exact_matching_emails\n",
    "        post_unmatched = post_emails - exact_matching_emails\n",
    "        \n",
    "        # Convert to lists for fuzzy matching\n",
    "        pre_unmatched_list = list(pre_unmatched)\n",
    "        post_unmatched_list = list(post_unmatched)\n",
    "        \n",
    "        # If either list is too large, limit the fuzzy matching\n",
    "        max_fuzzy_compare = 5000  # Adjust based on performance needs\n",
    "        \n",
    "        if len(pre_unmatched_list) > max_fuzzy_compare or len(post_unmatched_list) > max_fuzzy_compare:\n",
    "            print(f\"Warning: Too many emails for full fuzzy matching. Limiting comparison to {max_fuzzy_compare} emails.\")\n",
    "            if len(pre_unmatched_list) > max_fuzzy_compare:\n",
    "                pre_unmatched_list = pre_unmatched_list[:max_fuzzy_compare]\n",
    "            if len(post_unmatched_list) > max_fuzzy_compare:\n",
    "                post_unmatched_list = post_unmatched_list[:max_fuzzy_compare]\n",
    "        \n",
    "        for pre_email in pre_unmatched_list:\n",
    "            # Use domain-based grouping to reduce comparisons\n",
    "            pre_domain = pre_email.split('@')[-1] if '@' in pre_email else ''\n",
    "            post_domain_group = [email for email in post_unmatched_list if email.endswith(pre_domain)]\n",
    "            \n",
    "            # Only perform fuzzy matching within same domain group\n",
    "            if post_domain_group:\n",
    "                matches = process.extractBests(pre_email, post_domain_group, \n",
    "                                             score_cutoff=similarity_threshold, \n",
    "                                             limit=1)\n",
    "                if matches:\n",
    "                    best_match, score = matches[0]\n",
    "                    fuzzy_matches[pre_email] = (best_match, score)\n",
    "                    additional_matches_count += 1\n",
    "        \n",
    "        print(f\"Found {additional_matches_count} additional potential matches through fuzzy matching\")\n",
    "    \n",
    "    total_matches = exact_matching_count + additional_matches_count\n",
    "    \n",
    "    print(f\"\\nTotal matching emails: {total_matches}\")\n",
    "    print(f\"Exact match rate: {(exact_matching_count / pre_unique_count * 100):.2f}% of pre-enrollment emails\")\n",
    "    print(f\"Exact match rate: {(exact_matching_count / post_unique_count * 100):.2f}% of post-course emails\")\n",
    "    print(f\"Total match rate: {(total_matches / pre_unique_count * 100):.2f}% of pre-enrollment emails\")\n",
    "    print(f\"Total match rate: {(total_matches / post_unique_count * 100):.2f}% of post-course emails\")\n",
    "    \n",
    "    # Check for common patterns in unmatched emails\n",
    "    print(\"\\nAnalyzing unmatched emails for patterns...\")\n",
    "    \n",
    "    # Extract domains from pre and post emails\n",
    "    def get_domain(email):\n",
    "        parts = email.split('@')\n",
    "        return parts[1] if len(parts) > 1 else None\n",
    "    \n",
    "    pre_domains = [get_domain(email) for email in pre_emails if get_domain(email)]\n",
    "    post_domains = [get_domain(email) for email in post_emails if get_domain(email)]\n",
    "    \n",
    "    # Count domain frequencies\n",
    "    from collections import Counter\n",
    "    pre_domain_counts = Counter(pre_domains)\n",
    "    post_domain_counts = Counter(post_domains)\n",
    "    \n",
    "    # Get top 5 domains in each dataset\n",
    "    print(\"\\nTop 5 domains in pre-enrollment emails:\")\n",
    "    for domain, count in pre_domain_counts.most_common(5):\n",
    "        print(f\"  {domain}: {count} emails\")\n",
    "    \n",
    "    print(\"\\nTop 5 domains in post-course emails:\")\n",
    "    for domain, count in post_domain_counts.most_common(5):\n",
    "        print(f\"  {domain}: {count} emails\")\n",
    "    \n",
    "    # Create a merged dataset with matched records\n",
    "    # Combine exact and fuzzy matches\n",
    "    all_matched_pairs = {}\n",
    "    for email in exact_matching_emails:\n",
    "        all_matched_pairs[email] = email\n",
    "    \n",
    "    for pre_email, (post_email, score) in fuzzy_matches.items():\n",
    "        all_matched_pairs[pre_email] = post_email\n",
    "    \n",
    "    # Save match report\n",
    "    output_dir = os.path.dirname(pre_course_path)\n",
    "    output_file = os.path.join(output_dir, \"enhanced_survey_match_analysis.xlsx\")\n",
    "    \n",
    "    # Create a summary dataframe\n",
    "    summary_data = {\n",
    "        'Metric': [\n",
    "            'Total Pre-Enrollment Records',\n",
    "            'Total Post-Course Records',\n",
    "            'Valid Pre-Enrollment Emails',\n",
    "            'Valid Post-Course Emails',\n",
    "            'Unique Pre-Enrollment Emails',\n",
    "            'Unique Post-Course Emails',\n",
    "            'Exact Matching Emails',\n",
    "            'Fuzzy Matching Emails',\n",
    "            'Total Matching Emails',\n",
    "            'Pre-Enrollment Exact Match Rate',\n",
    "            'Post-Course Exact Match Rate',\n",
    "            'Pre-Enrollment Total Match Rate',\n",
    "            'Post-Course Total Match Rate'\n",
    "        ],\n",
    "        'Value': [\n",
    "            pre_total,\n",
    "            post_total,\n",
    "            pre_valid_count,\n",
    "            post_valid_count,\n",
    "            pre_unique_count,\n",
    "            post_unique_count,\n",
    "            exact_matching_count,\n",
    "            additional_matches_count,\n",
    "            total_matches,\n",
    "            f\"{(exact_matching_count / pre_unique_count * 100):.2f}%\",\n",
    "            f\"{(exact_matching_count / post_unique_count * 100):.2f}%\",\n",
    "            f\"{(total_matches / pre_unique_count * 100):.2f}%\",\n",
    "            f\"{(total_matches / post_unique_count * 100):.2f}%\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    # Create a DataFrame with exact matches\n",
    "    exact_matches_df = pd.DataFrame({'Email': list(exact_matching_emails)})\n",
    "    \n",
    "    # Create a DataFrame with fuzzy matches\n",
    "    if fuzzy_matches:\n",
    "        fuzzy_matches_data = []\n",
    "        for pre_email, (post_email, score) in fuzzy_matches.items():\n",
    "            fuzzy_matches_data.append({\n",
    "                'Pre-Enrollment Email': pre_email,\n",
    "                'Post-Course Email': post_email,\n",
    "                'Similarity Score': score\n",
    "            })\n",
    "        fuzzy_matches_df = pd.DataFrame(fuzzy_matches_data)\n",
    "    else:\n",
    "        fuzzy_matches_df = pd.DataFrame({'Pre-Enrollment Email': [], 'Post-Course Email': [], 'Similarity Score': []})\n",
    "    \n",
    "    # Save to Excel\n",
    "    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "        summary_df.to_excel(writer, sheet_name='Match Summary', index=False)\n",
    "        exact_matches_df.to_excel(writer, sheet_name='Exact Matches', index=False)\n",
    "        fuzzy_matches_df.to_excel(writer, sheet_name='Fuzzy Matches', index=False)\n",
    "        \n",
    "        # Also save domain frequency information\n",
    "        pre_domain_df = pd.DataFrame({\n",
    "            'Domain': list(pre_domain_counts.keys()),\n",
    "            'Count': list(pre_domain_counts.values()),\n",
    "            'Percentage': [(count/pre_unique_count*100) for count in pre_domain_counts.values()]\n",
    "        }).sort_values('Count', ascending=False)\n",
    "        \n",
    "        post_domain_df = pd.DataFrame({\n",
    "            'Domain': list(post_domain_counts.keys()),\n",
    "            'Count': list(post_domain_counts.values()),\n",
    "            'Percentage': [(count/post_unique_count*100) for count in post_domain_counts.values()]\n",
    "        }).sort_values('Count', ascending=False)\n",
    "        \n",
    "        pre_domain_df.to_excel(writer, sheet_name='Pre-Enrollment Domains', index=False)\n",
    "        post_domain_df.to_excel(writer, sheet_name='Post-Course Domains', index=False)\n",
    "    \n",
    "    print(f\"\\nSaved enhanced match analysis to: {output_file}\")\n",
    "    \n",
    "    return {\n",
    "        'pre_total': pre_total,\n",
    "        'post_total': post_total,\n",
    "        'exact_matching_count': exact_matching_count,\n",
    "        'additional_matches_count': additional_matches_count,\n",
    "        'total_matches': total_matches,\n",
    "        'exact_matching_emails': exact_matching_emails,\n",
    "        'fuzzy_matches': fuzzy_matches,\n",
    "        'all_matched_pairs': all_matched_pairs,\n",
    "        'pre_df': pre_df,\n",
    "        'post_df': post_df\n",
    "    }\n",
    "\n",
    "def merge_survey_data(results, output_path=None, include_fuzzy_matches=True):\n",
    "    \"\"\"\n",
    "    Merge the pre-enrollment and post-course survey data based on matching emails.\n",
    "    \n",
    "    Parameters:\n",
    "    results (dict): Results from analyze_survey_matches\n",
    "    output_path (str, optional): Path to save the merged data\n",
    "    include_fuzzy_matches (bool): Whether to include fuzzy matches in the merge\n",
    "    \"\"\"\n",
    "    if results['total_matches'] == 0:\n",
    "        print(\"No matching emails found. Cannot merge datasets.\")\n",
    "        return None\n",
    "    \n",
    "    pre_df = results['pre_df']\n",
    "    post_df = results['post_df']\n",
    "    all_matched_pairs = results['all_matched_pairs']\n",
    "    \n",
    "    print(f\"Merging datasets with {len(all_matched_pairs)} matched email pairs...\")\n",
    "    \n",
    "    # Create a mapping DataFrame\n",
    "    mapping_data = []\n",
    "    for pre_email, post_email in all_matched_pairs.items():\n",
    "        mapping_data.append({\n",
    "            'pre_email': pre_email,\n",
    "            'post_email': post_email\n",
    "        })\n",
    "    \n",
    "    mapping_df = pd.DataFrame(mapping_data)\n",
    "    \n",
    "    # Merge using the mapping\n",
    "    # First, merge mapping with pre_df\n",
    "    pre_df_norm = pre_df.dropna(subset=['email_normalized'])\n",
    "    post_df_norm = post_df.dropna(subset=['email_normalized'])\n",
    "    \n",
    "    merged_with_pre = pd.merge(\n",
    "        mapping_df,\n",
    "        pre_df_norm,\n",
    "        left_on='pre_email',\n",
    "        right_on='email_normalized',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Then merge with post_df\n",
    "    merged_full = pd.merge(\n",
    "        merged_with_pre,\n",
    "        post_df_norm,\n",
    "        left_on='post_email',\n",
    "        right_on='email_normalized',\n",
    "        how='left',\n",
    "        suffixes=('_pre', '_post')\n",
    "    )\n",
    "    \n",
    "    # Clean up the merged dataset\n",
    "    # Remove mapping columns\n",
    "    merged_full.drop(['pre_email', 'post_email'], axis=1, inplace=True)\n",
    "    \n",
    "    # Handle duplicate columns\n",
    "    # If there are duplicate email_address columns, keep only one\n",
    "    if 'email_address_pre' in merged_full.columns and 'email_address_post' in merged_full.columns:\n",
    "        # Create a new column that takes the pre value, but if it's missing, takes the post value\n",
    "        merged_full['email_address'] = merged_full['email_address_pre'].fillna(merged_full['email_address_post'])\n",
    "        merged_full.drop(['email_address_pre', 'email_address_post'], axis=1, inplace=True)\n",
    "    \n",
    "    # Also clean up the email_normalized columns\n",
    "    if 'email_normalized_pre' in merged_full.columns and 'email_normalized_post' in merged_full.columns:\n",
    "        merged_full.drop(['email_normalized_pre', 'email_normalized_post'], axis=1, inplace=True)\n",
    "    \n",
    "    print(f\"Merged dataset has {len(merged_full)} rows and {len(merged_full.columns)} columns\")\n",
    "    \n",
    "    # Save to Excel if output path provided\n",
    "    if output_path:\n",
    "        merged_full.to_excel(output_path, index=False)\n",
    "        print(f\"Merged data saved to: {output_path}\")\n",
    "    \n",
    "    return merged_full\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pre_course_path = '/Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/ES/Cleaned & Enhanced ES Data .xlsx'\n",
    "    post_course_path = '/Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/PCS/Cleaned_Merged_PCS_2cleaned.xlsx'\n",
    "    \n",
    "    # Analyze matches with enhanced normalization and fuzzy matching\n",
    "    results = analyze_survey_matches(pre_course_path, post_course_path, fuzzy_match=True, similarity_threshold=85)\n",
    "    \n",
    "    # If you want to merge the datasets, uncomment this section\n",
    "    # output_dir = os.path.dirname(pre_course_path)\n",
    "    # merged_output_path = os.path.join(output_dir, \"Enhanced_Merged_Surveys.xlsx\")\n",
    "    # merged_df = merge_survey_data(results, merged_output_path, include_fuzzy_matches=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "781f5c4f-66e5-483b-af20-0556db768b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-enrollment survey data...\n",
      "Loading post-course survey data...\n",
      "Pre-enrollment survey: 29700 entries\n",
      "Post-course survey: 7806 entries\n",
      "Pre-enrollment survey: 24433 entries with valid emails\n",
      "Post-course survey: 5462 entries with valid emails\n",
      "Pre-enrollment survey: 24412 unique emails\n",
      "Post-course survey: 5462 unique emails\n",
      "\n",
      "Found 3208 matching emails between datasets\n",
      "Match rate: 13.14% of pre-enrollment emails\n",
      "Match rate: 58.73% of post-course emails\n",
      "Found 1264 additional potential matches with minor differences\n",
      "\n",
      "Sample of matched emails (first 5):\n",
      "1. milliee38@gmail.com\n",
      "2. shanakawap@yahoo.com\n",
      "3. griseldacruz49@hotmail.com\n",
      "4. johnrossfrancis9@gmail.com\n",
      "5. zpt00200@gmail.com\n",
      "\n",
      "Top 5 domains in pre-enrollment emails:\n",
      "  gmail.com: 18997 emails\n",
      "  yahoo.com: 1742 emails\n",
      "  hotmail.com: 1173 emails\n",
      "  icloud.com: 432 emails\n",
      "  aol.com: 219 emails\n",
      "\n",
      "Top 5 domains in post-course emails:\n",
      "  gmail.com: 4699 emails\n",
      "  yahoo.com: 219 emails\n",
      "  hotmail.com: 135 emails\n",
      "  icloud.com: 59 emails\n",
      "  bpsadulted.com: 39 emails\n",
      "\n",
      "Saved match analysis to: /Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/ES/strict_match_analysis.xlsx\n"
     ]
    }
   ],
   "source": [
    "#Fuzzy Matching analysis\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "def analyze_survey_matches(pre_course_path, post_course_path):\n",
    "    \"\"\"\n",
    "    Analyze how many learners completed both pre-enrollment and post-course surveys\n",
    "    by matching email addresses between the two datasets with strict normalization.\n",
    "    \n",
    "    Parameters:\n",
    "    pre_course_path (str): Path to the pre-enrollment survey Excel file\n",
    "    post_course_path (str): Path to the post-course survey Excel file\n",
    "    \"\"\"\n",
    "    print(\"Loading pre-enrollment survey data...\")\n",
    "    pre_df = pd.read_excel(pre_course_path)\n",
    "    \n",
    "    print(\"Loading post-course survey data...\")\n",
    "    post_df = pd.read_excel(post_course_path)\n",
    "    \n",
    "    # Get total counts\n",
    "    pre_total = len(pre_df)\n",
    "    post_total = len(post_df)\n",
    "    \n",
    "    print(f\"Pre-enrollment survey: {pre_total} entries\")\n",
    "    print(f\"Post-course survey: {post_total} entries\")\n",
    "    \n",
    "    # Check if email_address column exists in both datasets\n",
    "    if 'email_address' not in pre_df.columns:\n",
    "        print(\"Error: 'email_address' column not found in pre-enrollment dataset\")\n",
    "        print(f\"Available columns: {pre_df.columns.tolist()}\")\n",
    "        return\n",
    "    \n",
    "    if 'email_address' not in post_df.columns:\n",
    "        print(\"Error: 'email_address' column not found in post-course dataset\")\n",
    "        print(f\"Available columns: {post_df.columns.tolist()}\")\n",
    "        return\n",
    "    \n",
    "    # Enhanced email normalization\n",
    "    def normalize_email(email):\n",
    "        if pd.isna(email):\n",
    "            return None\n",
    "        \n",
    "        # Convert to string and lowercase\n",
    "        email_str = str(email).lower().strip()\n",
    "        \n",
    "        # Remove any spaces\n",
    "        email_str = email_str.replace(' ', '')\n",
    "        \n",
    "        # Fix common typos in domains\n",
    "        email_str = email_str.replace('gmail.con', 'gmail.com')\n",
    "        email_str = email_str.replace('yaho.com', 'yahoo.com')\n",
    "        email_str = email_str.replace('yahooo.com', 'yahoo.com')\n",
    "        email_str = email_str.replace('hotmial.com', 'hotmail.com')\n",
    "        email_str = email_str.replace('hotmal.com', 'hotmail.com')\n",
    "        email_str = email_str.replace('gamil.com', 'gmail.com')\n",
    "        email_str = email_str.replace('gnail.com', 'gmail.com')\n",
    "        \n",
    "        # Remove any non-email characters that might have been added\n",
    "        email_str = re.sub(r'[^\\w@.-]', '', email_str)\n",
    "        \n",
    "        # If it doesn't look like an email at all, return None\n",
    "        if '@' not in email_str or '.' not in email_str:\n",
    "            return None\n",
    "            \n",
    "        return email_str\n",
    "    \n",
    "    # Apply normalization\n",
    "    pre_df['email_normalized'] = pre_df['email_address'].apply(normalize_email)\n",
    "    post_df['email_normalized'] = post_df['email_address'].apply(normalize_email)\n",
    "    \n",
    "    # Remove null emails\n",
    "    pre_df_valid = pre_df.dropna(subset=['email_normalized'])\n",
    "    post_df_valid = post_df.dropna(subset=['email_normalized'])\n",
    "    \n",
    "    pre_valid_count = len(pre_df_valid)\n",
    "    post_valid_count = len(post_df_valid)\n",
    "    \n",
    "    print(f\"Pre-enrollment survey: {pre_valid_count} entries with valid emails\")\n",
    "    print(f\"Post-course survey: {post_valid_count} entries with valid emails\")\n",
    "    \n",
    "    # Get unique emails\n",
    "    pre_emails = set(pre_df_valid['email_normalized'].unique())\n",
    "    post_emails = set(post_df_valid['email_normalized'].unique())\n",
    "    \n",
    "    pre_unique_count = len(pre_emails)\n",
    "    post_unique_count = len(post_emails)\n",
    "    \n",
    "    print(f\"Pre-enrollment survey: {pre_unique_count} unique emails\")\n",
    "    print(f\"Post-course survey: {post_unique_count} unique emails\")\n",
    "    \n",
    "    # Find exact matches only\n",
    "    matching_emails = pre_emails.intersection(post_emails)\n",
    "    matching_count = len(matching_emails)\n",
    "    \n",
    "    print(f\"\\nFound {matching_count} matching emails between datasets\")\n",
    "    print(f\"Match rate: {(matching_count / pre_unique_count * 100):.2f}% of pre-enrollment emails\")\n",
    "    print(f\"Match rate: {(matching_count / post_unique_count * 100):.2f}% of post-course emails\")\n",
    "    \n",
    "    # Find potential near-matches with very minimal differences\n",
    "    # These would be typo corrections that are extremely likely to be the same person\n",
    "    # This is much stricter than fuzzy matching\n",
    "    typo_corrections = {}\n",
    "    for pre_email in pre_emails:\n",
    "        username, domain = pre_email.split('@')\n",
    "        for post_email in post_emails:\n",
    "            if '@' in post_email:\n",
    "                post_username, post_domain = post_email.split('@')\n",
    "                \n",
    "                # Only consider if domains match exactly\n",
    "                if domain == post_domain:\n",
    "                    # Look for extremely similar usernames with just one character difference\n",
    "                    # This catches things like john.smith vs johnsmith or jsmith vs j.smith\n",
    "                    # But avoids matching completely different people\n",
    "                    \n",
    "                    # Check for dot differences\n",
    "                    pre_no_dots = username.replace('.', '')\n",
    "                    post_no_dots = post_username.replace('.', '')\n",
    "                    \n",
    "                    if pre_no_dots == post_no_dots and pre_email != post_email:\n",
    "                        typo_corrections[pre_email] = post_email\n",
    "                    \n",
    "                    # Check for single digit difference at the end (e.g., john1 vs john)\n",
    "                    if (username.rstrip('0123456789') == post_username or \n",
    "                        post_username.rstrip('0123456789') == username):\n",
    "                        typo_corrections[pre_email] = post_email\n",
    "    \n",
    "    print(f\"Found {len(typo_corrections)} additional potential matches with minor differences\")\n",
    "    \n",
    "    # Get a sample of matched emails for verification\n",
    "    sample_size = min(5, matching_count)\n",
    "    sample_emails = list(matching_emails)[:sample_size]\n",
    "    \n",
    "    print(f\"\\nSample of matched emails (first {sample_size}):\")\n",
    "    for i, email in enumerate(sample_emails, 1):\n",
    "        print(f\"{i}. {email}\")\n",
    "    \n",
    "    # Save match report\n",
    "    output_dir = os.path.dirname(pre_course_path)\n",
    "    output_file = os.path.join(output_dir, \"strict_match_analysis.xlsx\")\n",
    "    \n",
    "    # Create a summary dataframe\n",
    "    summary_data = {\n",
    "        'Metric': [\n",
    "            'Total Pre-Enrollment Records',\n",
    "            'Total Post-Course Records',\n",
    "            'Valid Pre-Enrollment Emails',\n",
    "            'Valid Post-Course Emails',\n",
    "            'Unique Pre-Enrollment Emails',\n",
    "            'Unique Post-Course Emails',\n",
    "            'Matching Emails',\n",
    "            'Potential Minor Typo Matches',\n",
    "            'Pre-Enrollment Match Rate',\n",
    "            'Post-Course Match Rate'\n",
    "        ],\n",
    "        'Value': [\n",
    "            pre_total,\n",
    "            post_total,\n",
    "            pre_valid_count,\n",
    "            post_valid_count,\n",
    "            pre_unique_count,\n",
    "            post_unique_count,\n",
    "            matching_count,\n",
    "            len(typo_corrections),\n",
    "            f\"{(matching_count / pre_unique_count * 100):.2f}%\",\n",
    "            f\"{(matching_count / post_unique_count * 100):.2f}%\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    # Create a DataFrame with the list of matching emails\n",
    "    matches_df = pd.DataFrame({'Exact Matching Emails': list(matching_emails)})\n",
    "    \n",
    "    # Create a DataFrame with potential typo corrections\n",
    "    if typo_corrections:\n",
    "        typo_data = []\n",
    "        for pre_email, post_email in typo_corrections.items():\n",
    "            typo_data.append({\n",
    "                'Pre-Enrollment Email': pre_email,\n",
    "                'Post-Course Email': post_email\n",
    "            })\n",
    "        typo_df = pd.DataFrame(typo_data)\n",
    "    else:\n",
    "        typo_df = pd.DataFrame({'Pre-Enrollment Email': [], 'Post-Course Email': []})\n",
    "    \n",
    "    # Extract domains for analysis\n",
    "    def get_domain(email):\n",
    "        parts = email.split('@')\n",
    "        return parts[1] if len(parts) > 1 else None\n",
    "    \n",
    "    pre_domains = [get_domain(email) for email in pre_emails if get_domain(email)]\n",
    "    post_domains = [get_domain(email) for email in post_emails if get_domain(email)]\n",
    "    \n",
    "    # Count domain frequencies\n",
    "    from collections import Counter\n",
    "    pre_domain_counts = Counter(pre_domains)\n",
    "    post_domain_counts = Counter(post_domains)\n",
    "    \n",
    "    # Get top domains in each dataset\n",
    "    print(\"\\nTop 5 domains in pre-enrollment emails:\")\n",
    "    for domain, count in pre_domain_counts.most_common(5):\n",
    "        print(f\"  {domain}: {count} emails\")\n",
    "    \n",
    "    print(\"\\nTop 5 domains in post-course emails:\")\n",
    "    for domain, count in post_domain_counts.most_common(5):\n",
    "        print(f\"  {domain}: {count} emails\")\n",
    "    \n",
    "    # Save to Excel\n",
    "    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "        summary_df.to_excel(writer, sheet_name='Match Summary', index=False)\n",
    "        matches_df.to_excel(writer, sheet_name='Exact Matches', index=False)\n",
    "        typo_df.to_excel(writer, sheet_name='Potential Typo Matches', index=False)\n",
    "        \n",
    "        # Also save domain frequency information\n",
    "        pre_domain_df = pd.DataFrame({\n",
    "            'Domain': list(pre_domain_counts.keys()),\n",
    "            'Count': list(pre_domain_counts.values()),\n",
    "            'Percentage': [(count/pre_unique_count*100) for count in pre_domain_counts.values()]\n",
    "        }).sort_values('Count', ascending=False)\n",
    "        \n",
    "        post_domain_df = pd.DataFrame({\n",
    "            'Domain': list(post_domain_counts.keys()),\n",
    "            'Count': list(post_domain_counts.values()),\n",
    "            'Percentage': [(count/post_unique_count*100) for count in post_domain_counts.values()]\n",
    "        }).sort_values('Count', ascending=False)\n",
    "        \n",
    "        pre_domain_df.to_excel(writer, sheet_name='Pre-Enrollment Domains', index=False)\n",
    "        post_domain_df.to_excel(writer, sheet_name='Post-Course Domains', index=False)\n",
    "    \n",
    "    print(f\"\\nSaved match analysis to: {output_file}\")\n",
    "    \n",
    "    return {\n",
    "        'pre_total': pre_total,\n",
    "        'post_total': post_total,\n",
    "        'matching_count': matching_count,\n",
    "        'matching_emails': matching_emails,\n",
    "        'typo_corrections': typo_corrections,\n",
    "        'pre_df': pre_df,\n",
    "        'post_df': post_df\n",
    "    }\n",
    "\n",
    "def merge_survey_data(results, output_path=None, include_typo_matches=False):\n",
    "    \"\"\"\n",
    "    Merge the pre-enrollment and post-course survey data based on matching emails.\n",
    "    \n",
    "    Parameters:\n",
    "    results (dict): Results from analyze_survey_matches\n",
    "    output_path (str, optional): Path to save the merged data\n",
    "    include_typo_matches (bool): Whether to include typo-corrected matches\n",
    "    \"\"\"\n",
    "    if results['matching_count'] == 0 and not (include_typo_matches and results['typo_corrections']):\n",
    "        print(\"No matching emails found. Cannot merge datasets.\")\n",
    "        return None\n",
    "    \n",
    "    pre_df = results['pre_df']\n",
    "    post_df = results['post_df']\n",
    "    matching_emails = results['matching_emails']\n",
    "    typo_corrections = results['typo_corrections'] if include_typo_matches else {}\n",
    "    \n",
    "    # Prepare for merge\n",
    "    pre_df_for_merge = pre_df.copy()\n",
    "    post_df_for_merge = post_df.copy()\n",
    "    \n",
    "    # Filter datasets to only include matched records\n",
    "    if include_typo_matches:\n",
    "        all_pre_emails = list(matching_emails) + list(typo_corrections.keys())\n",
    "        all_post_emails = list(matching_emails) + list(typo_corrections.values())\n",
    "    else:\n",
    "        all_pre_emails = list(matching_emails)\n",
    "        all_post_emails = list(matching_emails)\n",
    "    \n",
    "    pre_matched_df = pre_df_for_merge[pre_df_for_merge['email_normalized'].isin(all_pre_emails)]\n",
    "    post_matched_df = post_df_for_merge[post_df_for_merge['email_normalized'].isin(all_post_emails)]\n",
    "    \n",
    "    print(f\"Filtered to {len(pre_matched_df)} pre-enrollment records and {len(post_matched_df)} post-course records\")\n",
    "    \n",
    "    # Create a mapping for exact matches\n",
    "    mapping_data = []\n",
    "    \n",
    "    # Add exact matches\n",
    "    for email in matching_emails:\n",
    "        mapping_data.append({\n",
    "            'pre_email': email,\n",
    "            'post_email': email,\n",
    "            'match_type': 'exact'\n",
    "        })\n",
    "    \n",
    "    # Add typo corrections if requested\n",
    "    if include_typo_matches:\n",
    "        for pre_email, post_email in typo_corrections.items():\n",
    "            mapping_data.append({\n",
    "                'pre_email': pre_email,\n",
    "                'post_email': post_email,\n",
    "                'match_type': 'typo'\n",
    "            })\n",
    "    \n",
    "    mapping_df = pd.DataFrame(mapping_data)\n",
    "    \n",
    "    # Merge using the mapping\n",
    "    # First, merge mapping with pre_df\n",
    "    merged_with_pre = pd.merge(\n",
    "        mapping_df,\n",
    "        pre_matched_df,\n",
    "        left_on='pre_email',\n",
    "        right_on='email_normalized',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Then merge with post_df\n",
    "    merged_full = pd.merge(\n",
    "        merged_with_pre,\n",
    "        post_matched_df,\n",
    "        left_on='post_email',\n",
    "        right_on='email_normalized',\n",
    "        how='left',\n",
    "        suffixes=('_pre', '_post')\n",
    "    )\n",
    "    \n",
    "    # Clean up the merged dataset\n",
    "    # Remove mapping columns\n",
    "    merged_full.drop(['pre_email', 'post_email'], axis=1, inplace=True)\n",
    "    \n",
    "    # Handle duplicate columns\n",
    "    if 'email_address_pre' in merged_full.columns and 'email_address_post' in merged_full.columns:\n",
    "        # Create a new column that takes the pre value, but if it's missing, takes the post value\n",
    "        merged_full['email_address'] = merged_full['email_address_pre'].fillna(merged_full['email_address_post'])\n",
    "        merged_full.drop(['email_address_pre', 'email_address_post'], axis=1, inplace=True)\n",
    "    \n",
    "    # Also clean up the email_normalized columns\n",
    "    if 'email_normalized_pre' in merged_full.columns and 'email_normalized_post' in merged_full.columns:\n",
    "        merged_full.drop(['email_normalized_pre', 'email_normalized_post'], axis=1, inplace=True)\n",
    "    \n",
    "    print(f\"Merged dataset has {len(merged_full)} rows and {len(merged_full.columns)} columns\")\n",
    "    \n",
    "    # Save to Excel if output path provided\n",
    "    if output_path:\n",
    "        merged_full.to_excel(output_path, index=False)\n",
    "        print(f\"Merged data saved to: {output_path}\")\n",
    "    \n",
    "    return merged_full\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pre_course_path = '/Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/ES/Cleaned & Enhanced ES Data .xlsx'\n",
    "    post_course_path = '/Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/PCS/Cleaned_Merged_PCS_2cleaned.xlsx'\n",
    "    \n",
    "    # Analyze matches with enhanced normalization but no fuzzy matching\n",
    "    results = analyze_survey_matches(pre_course_path, post_course_path)\n",
    "    \n",
    "    # If you want to merge the datasets, uncomment this section\n",
    "    # output_dir = os.path.dirname(pre_course_path)\n",
    "    # merged_output_path = os.path.join(output_dir, \"Strict_Merged_Surveys.xlsx\")\n",
    "    # merged_df = merge_survey_data(results, merged_output_path, include_typo_matches=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7f43aa7-ba0b-425f-9e88-2a171e9bbff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-enrollment survey data...\n",
      "Loading post-course survey data...\n",
      "Pre-enrollment survey: 29700 entries\n",
      "Post-course survey: 7806 entries\n",
      "Aligning date of birth column names...\n",
      "Standardizing data for matching...\n",
      "Processing pre-enrollment emails...\n",
      "Pre-enrollment emails before normalization: 24446\n",
      "Pre-enrollment emails after normalization: 24433\n",
      "Processing pre-enrollment dates of birth...\n",
      "Processing post-course emails...\n",
      "Post-course emails before normalization: 7746\n",
      "Post-course emails after normalization: 5462\n",
      "Processing post-course dates of birth...\n",
      "\n",
      "Performing email matching...\n",
      "Pre-enrollment records with valid emails: 24433 (82.27%)\n",
      "Post-course records with valid emails: 5462 (69.97%)\n",
      "Unique emails in pre-enrollment: 24368\n",
      "Unique emails in post-course: 5456\n",
      "Found 3226 matches by email\n",
      "Email match rate: 13.24% of pre-enrollment emails\n",
      "Email match rate: 59.13% of post-course emails\n",
      "\n",
      "Domain corrections applied:\n",
      "Pre-enrollment: 334 domains corrected\n",
      "Post-course: 58 domains corrected\n",
      "\n",
      "Performing full name + DOB matching...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q6/v__5dqvn4fsgnld9cp_x0v1m0000gn/T/ipykernel_7198/52385201.py:401: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pre_unmatched_email['name_dob_key'] = pre_unmatched_email.apply(create_name_dob_key, axis=1)\n",
      "/var/folders/q6/v__5dqvn4fsgnld9cp_x0v1m0000gn/T/ipykernel_7198/52385201.py:402: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  post_unmatched_email['name_dob_key'] = post_unmatched_email.apply(create_name_dob_key, axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-enrollment records with valid name+DOB (unmatched by email): 26364\n",
      "Post-course records with valid name+DOB (unmatched by email): 4488\n",
      "Found 2324 additional matches by full name + DOB\n",
      "\n",
      "Performing first name + last name + birth year matching...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q6/v__5dqvn4fsgnld9cp_x0v1m0000gn/T/ipykernel_7198/52385201.py:449: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pre_unmatched['name_year_key'] = pre_unmatched.apply(create_name_year_key, axis=1)\n",
      "/var/folders/q6/v__5dqvn4fsgnld9cp_x0v1m0000gn/T/ipykernel_7198/52385201.py:450: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  post_unmatched['name_year_key'] = post_unmatched.apply(create_name_year_key, axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-enrollment records with valid name+year (still unmatched): 23885\n",
      "Post-course records with valid name+year (still unmatched): 2139\n",
      "Found 97 additional matches by first name + last name + birth year\n",
      "\n",
      "Total matched records: 5647\n",
      "Overall match rate: 23.17% of pre-enrollment emails\n",
      "Overall match rate: 103.50% of post-course emails\n",
      "\n",
      "Creating matching pairs...\n",
      "\n",
      "Matches by type:\n",
      "  email: 3247 match pairs\n",
      "  name_dob: 2506 match pairs\n",
      "  name_year: 100 match pairs\n",
      "\n",
      "Analyzing domain corrections that led to successful matches...\n",
      "Found 60 matches due to domain correction\n",
      "\n",
      "Saving match analysis...\n",
      "Saved comprehensive match analysis to: /Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/ES/comprehensive_match_analysis.xlsx\n",
      "Merging datasets using match types: ['email', 'name_dob', 'name_year']\n",
      "Using 5853 matching pairs for merging.\n",
      "Filtered to 5820 pre-enrollment records and 5676 post-course records\n",
      "Merged dataset has 5820 rows and 229 columns\n",
      "Merged data saved to: /Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/ES/Comprehensive_Merged_Surveys.xlsx\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'match_type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'match_type'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 857\u001b[0m\n\u001b[1;32m    855\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(pre_course_path)\n\u001b[1;32m    856\u001b[0m merged_output_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComprehensive_Merged_Surveys.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 857\u001b[0m merged_df \u001b[38;5;241m=\u001b[39m merge_survey_data(results, merged_output_path)\n",
      "Cell \u001b[0;32mIn[18], line 840\u001b[0m, in \u001b[0;36mmerge_survey_data\u001b[0;34m(results, output_path, include_match_types)\u001b[0m\n\u001b[1;32m    838\u001b[0m \u001b[38;5;66;03m# Split by match type\u001b[39;00m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m match_type \u001b[38;5;129;01min\u001b[39;00m include_match_types:\n\u001b[0;32m--> 840\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m match_type \u001b[38;5;129;01min\u001b[39;00m merged_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatch_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues:\n\u001b[1;32m    841\u001b[0m         type_df \u001b[38;5;241m=\u001b[39m merged_df[merged_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatch_type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m match_type]\n\u001b[1;32m    842\u001b[0m         type_df\u001b[38;5;241m.\u001b[39mto_excel(writer, sheet_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmatch_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_matches\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'match_type'"
     ]
    }
   ],
   "source": [
    "#matching name and dob to improing the matching\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "def analyze_survey_matches(pre_course_path, post_course_path):\n",
    "    \"\"\"\n",
    "    Analyze matches between pre-enrollment and post-course surveys\n",
    "    using multiple identifiers with comprehensive domain correction.\n",
    "    \n",
    "    Parameters:\n",
    "    pre_course_path (str): Path to the pre-enrollment survey Excel file\n",
    "    post_course_path (str): Path to the post-course survey Excel file\n",
    "    \"\"\"\n",
    "    # Store statistics for reporting\n",
    "    stats = {}\n",
    "    \n",
    "    print(\"Loading pre-enrollment survey data...\")\n",
    "    pre_df = pd.read_excel(pre_course_path)\n",
    "    \n",
    "    print(\"Loading post-course survey data...\")\n",
    "    post_df = pd.read_excel(post_course_path)\n",
    "    \n",
    "    # Get total counts\n",
    "    pre_total = len(pre_df)\n",
    "    post_total = len(post_df)\n",
    "    \n",
    "    stats['pre_total'] = pre_total\n",
    "    stats['post_total'] = post_total\n",
    "    \n",
    "    print(f\"Pre-enrollment survey: {pre_total} entries\")\n",
    "    print(f\"Post-course survey: {post_total} entries\")\n",
    "    \n",
    "    # Step 1: Standardize and prepare data for matching\n",
    "    \n",
    "    # Fix field name discrepancy for date of birth\n",
    "    if 'date_of_birth_standardized' in pre_df.columns and 'date_of_birth' in post_df.columns:\n",
    "        print(\"Aligning date of birth column names...\")\n",
    "        pre_df.rename(columns={'date_of_birth_standardized': 'date_of_birth'}, inplace=True)\n",
    "    \n",
    "    # Comprehensive domain correction dictionary\n",
    "    domain_corrections = {\n",
    "        # Gmail variations\n",
    "        'gmai.com': 'gmail.com',\n",
    "        'gmil.com': 'gmail.com',\n",
    "        'gmal.com': 'gmail.com',\n",
    "        'gamail.com': 'gmail.com',\n",
    "        'gimail.com': 'gmail.com',\n",
    "        'gmaill.com': 'gmail.com',\n",
    "        'gmail.comm': 'gmail.com',\n",
    "        'gmial.com': 'gmail.com',\n",
    "        'gmail.cm': 'gmail.com',\n",
    "        'gmail.org': 'gmail.com',\n",
    "        'gemail.com': 'gmail.com',\n",
    "        'gmail.com.com': 'gmail.com',\n",
    "        'gmai.coml': 'gmail.com',\n",
    "        'testgmail.com': 'gmail.com',\n",
    "        'gmail.coom': 'gmail.com',\n",
    "        'jmail.com': 'gmail.com',\n",
    "        'gmeil.com': 'gmail.com',\n",
    "        'gmall.com': 'gmail.com',\n",
    "        'gail.com': 'gmail.com',\n",
    "        'gmail.co': 'gmail.com',\n",
    "        'gmaiil.com': 'gmail.com',\n",
    "        'gmaim.com': 'gmail.com',\n",
    "        'gmail.om': 'gmail.com',\n",
    "        'gmail.ccom': 'gmail.com',\n",
    "        'g.com': 'gmail.com',\n",
    "        'gmail.vom': 'gmail.com',\n",
    "        'gmali.com': 'gmail.com',\n",
    "        'gmaio.com': 'gmail.com',\n",
    "        'g-mail.com': 'gmail.com',\n",
    "        'gamill.com': 'gmail.com',\n",
    "        'gmaol.com': 'gmail.com',\n",
    "        'fakegmail.com': 'gmail.com',\n",
    "        'gmsil.com': 'gmail.com',\n",
    "        'gmaii.com': 'gmail.com',\n",
    "        'gmail.oeg': 'gmail.com',\n",
    "        '27gmail.com': 'gmail.com',\n",
    "        'gmanil.com': 'gmail.com',\n",
    "        'gmaili.com': 'gmail.com',\n",
    "        'gmail.crom': 'gmail.com',\n",
    "        'gmail.com11': 'gmail.com',\n",
    "        'gmill.com': 'gmail.com',\n",
    "        'gmail.com3': 'gmail.com',\n",
    "        'gnmail.com': 'gmail.com',\n",
    "        '7gmail.com': 'gmail.com',\n",
    "        '08gmail.com': 'gmail.com',\n",
    "        'gmaij.com': 'gmail.com',\n",
    "        '729gmail.com': 'gmail.com',\n",
    "        'gmail.coml.com': 'gmail.com',\n",
    "        '23gmail.com': 'gmail.com',\n",
    "        \n",
    "        # Yahoo variations\n",
    "        'yahoo.con': 'yahoo.com',\n",
    "        'yaoo.com': 'yahoo.com',\n",
    "        'hayoo.com': 'yahoo.com',\n",
    "        'yhoo.com': 'yahoo.com',\n",
    "        'yahoo.comm': 'yahoo.com',\n",
    "        'myyahoo.com': 'yahoo.com',\n",
    "        'yahoo.comb': 'yahoo.com',\n",
    "        'yahool.com': 'yahoo.com',\n",
    "        'yaho.cm': 'yahoo.com',\n",
    "        'yahio.un': 'yahoo.com',\n",
    "        \n",
    "        # Hotmail variations\n",
    "        'hmail.com': 'hotmail.com',\n",
    "        'hotmai.com': 'hotmail.com',\n",
    "        'hotamil.com': 'hotmail.com',\n",
    "        'hotmail.con': 'hotmail.com',\n",
    "        'hoo.com': 'hotmail.com',\n",
    "        'hotmiail.com': 'hotmail.com',\n",
    "        'htomail.com': 'hotmail.com',\n",
    "        'hiotmail.com': 'hotmail.com',\n",
    "        'hitmail.com': 'hotmail.com',\n",
    "        \n",
    "        # iCloud variations\n",
    "        'icould.com': 'icloud.com',\n",
    "        'iclod.com': 'icloud.com',\n",
    "        'cloud.com': 'icloud.com',\n",
    "        'icoud.com': 'icloud.com',\n",
    "        'ichoud.com': 'icloud.com',\n",
    "        'iclou.com': 'icloud.com',\n",
    "        \n",
    "        # AOL variations\n",
    "        'ao.com': 'aol.com',\n",
    "        'alo.com': 'aol.com',\n",
    "        'aol.co': 'aol.com',\n",
    "        'aol.om': 'aol.com',\n",
    "        \n",
    "        # Boston Public Schools variations\n",
    "        'bostonk12.com': 'bostonk12.org',\n",
    "        'boston12.org': 'bostonk12.org',\n",
    "        'bostobpublicschools.org': 'bostonpublicschools.org',\n",
    "        'bostonnpublicschools.org': 'bostonpublicschools.org',\n",
    "        'bostonnk12.org': 'bostonk12.org',\n",
    "        'bostonpulicschools.org': 'bostonpublicschools.org',\n",
    "        'bostonpublicschool.org.org': 'bostonpublicschools.org',\n",
    "        \n",
    "        # BPS Adult Ed variations\n",
    "        'pbsadulted.com': 'bpsadulted.com',\n",
    "        'bpsadulred.com': 'bpsadulted.com',\n",
    "        'adulted.com': 'bpsadulted.com',\n",
    "        \n",
    "        # Other common corrections\n",
    "        'live.con': 'live.com',\n",
    "        'live.cm': 'live.com',\n",
    "        'outlook.pt': 'outlook.com',\n",
    "        'verizon.com': 'verizon.net',\n",
    "        'comcast.com': 'comcast.net',\n",
    "        'mail.com': 'gmail.com',  # Often a typo for gmail\n",
    "        'email.com': 'gmail.com', # Often a typo for gmail\n",
    "        'cmail.com': 'gmail.com', # Often a typo for gmail\n",
    "        'ail.com': 'gmail.com'    # Often a typo for gmail\n",
    "    }\n",
    "    \n",
    "    # Email standardization with domain correction\n",
    "    def normalize_email(email):\n",
    "        if pd.isna(email):\n",
    "            return None\n",
    "        \n",
    "        # Convert to string and lowercase\n",
    "        email_str = str(email).lower().strip()\n",
    "        \n",
    "        # Remove any spaces and special characters\n",
    "        email_str = email_str.replace(' ', '')\n",
    "        email_str = re.sub(r'[^\\w@.-]', '', email_str)\n",
    "        \n",
    "        # Basic validation\n",
    "        if '@' not in email_str or '.' not in email_str:\n",
    "            return None\n",
    "            \n",
    "        # Extract domain for correction\n",
    "        username, domain = email_str.split('@', 1)\n",
    "        \n",
    "        # Apply domain correction if needed\n",
    "        corrected_domain = domain_corrections.get(domain, domain)\n",
    "        \n",
    "        # Rebuild email with potentially corrected domain\n",
    "        corrected_email = f\"{username}@{corrected_domain}\"\n",
    "        \n",
    "        return corrected_email\n",
    "    \n",
    "    # Name standardization\n",
    "    def normalize_name(name):\n",
    "        if pd.isna(name):\n",
    "            return None\n",
    "        \n",
    "        # Convert to string, lowercase, and strip spaces\n",
    "        name_str = str(name).lower().strip()\n",
    "        \n",
    "        # Remove extra spaces\n",
    "        name_str = re.sub(r'\\s+', ' ', name_str)\n",
    "        \n",
    "        # Remove non-alphabetic characters except spaces\n",
    "        name_str = re.sub(r'[^a-z ]', '', name_str)\n",
    "        \n",
    "        return name_str\n",
    "    \n",
    "    # Date of birth standardization - extract year, month, and day components\n",
    "    def standardize_dob(dob):\n",
    "        if pd.isna(dob):\n",
    "            return None, None, None\n",
    "            \n",
    "        # Try to parse the date\n",
    "        try:\n",
    "            if isinstance(dob, str):\n",
    "                # Replace common separators with a standard one\n",
    "                dob_str = dob.replace('-', '/').replace('.', '/').replace(' ', '')\n",
    "                \n",
    "                # Try various date formats\n",
    "                formats = ['%m/%d/%Y', '%m/%d/%y', '%Y/%m/%d', '%d/%m/%Y', '%m-%d-%Y']\n",
    "                \n",
    "                for fmt in formats:\n",
    "                    try:\n",
    "                        date_obj = datetime.strptime(dob_str, fmt)\n",
    "                        # For two-digit years, adjust century if needed\n",
    "                        if '%y' in fmt and date_obj.year > datetime.now().year:\n",
    "                            date_obj = date_obj.replace(year=date_obj.year - 100)\n",
    "                        return date_obj.year, date_obj.month, date_obj.day\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                # If we couldn't parse with standard formats, try handling numeric-only strings\n",
    "                if dob_str.isdigit():\n",
    "                    if len(dob_str) == 8:  # MMDDYYYY\n",
    "                        month = int(dob_str[0:2])\n",
    "                        day = int(dob_str[2:4])\n",
    "                        year = int(dob_str[4:8])\n",
    "                        if 1 <= month <= 12 and 1 <= day <= 31:\n",
    "                            return year, month, day\n",
    "                    elif len(dob_str) == 6:  # MMDDYY\n",
    "                        month = int(dob_str[0:2])\n",
    "                        day = int(dob_str[2:4])\n",
    "                        year = int(dob_str[4:6])\n",
    "                        if 1 <= month <= 12 and 1 <= day <= 31:\n",
    "                            full_year = 1900 + year if year >= 50 else 2000 + year\n",
    "                            return full_year, month, day\n",
    "            else:\n",
    "                # If it's already a datetime or similar type\n",
    "                try:\n",
    "                    date_obj = pd.to_datetime(dob)\n",
    "                    return date_obj.year, date_obj.month, date_obj.day\n",
    "                except:\n",
    "                    pass\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        return None, None, None\n",
    "    \n",
    "    # Apply standardization to both datasets\n",
    "    print(\"Standardizing data for matching...\")\n",
    "    \n",
    "    # Process pre-enrollment data\n",
    "    print(\"Processing pre-enrollment emails...\")\n",
    "    pre_df['original_email'] = pre_df['email_address']\n",
    "    pre_df['email_normalized'] = pre_df['email_address'].apply(normalize_email)\n",
    "    \n",
    "    # Get email stats before domain correction\n",
    "    pre_valid_email_before = pre_df.dropna(subset=['email_address']).shape[0]\n",
    "    pre_valid_normalized_before = pre_df.dropna(subset=['email_normalized']).shape[0]\n",
    "    \n",
    "    stats['pre_valid_email_before'] = pre_valid_email_before\n",
    "    stats['pre_valid_normalized_before'] = pre_valid_normalized_before\n",
    "    \n",
    "    print(f\"Pre-enrollment emails before normalization: {pre_valid_email_before}\")\n",
    "    print(f\"Pre-enrollment emails after normalization: {pre_valid_normalized_before}\")\n",
    "    \n",
    "    # Check if 'full_name' exists, if not, try to create it\n",
    "    if 'full_name' not in pre_df.columns:\n",
    "        if 'first_name' in pre_df.columns and 'last_name' in pre_df.columns:\n",
    "            pre_df['first_name'] = pre_df['first_name'].fillna('')\n",
    "            pre_df['last_name'] = pre_df['last_name'].fillna('')\n",
    "            pre_df['full_name'] = pre_df['first_name'] + ' ' + pre_df['last_name']\n",
    "            pre_df['full_name'] = pre_df['full_name'].str.strip()\n",
    "    \n",
    "    pre_df['name_normalized'] = pre_df['full_name'].apply(normalize_name)\n",
    "    pre_df['first_name_norm'] = pre_df['first_name'].apply(normalize_name) if 'first_name' in pre_df.columns else None\n",
    "    pre_df['last_name_norm'] = pre_df['last_name'].apply(normalize_name) if 'last_name' in pre_df.columns else None\n",
    "    \n",
    "    # Extract DOB components\n",
    "    print(\"Processing pre-enrollment dates of birth...\")\n",
    "    dob_results = pre_df['date_of_birth'].apply(standardize_dob)\n",
    "    pre_df['dob_year'], pre_df['dob_month'], pre_df['dob_day'] = zip(*dob_results)\n",
    "    \n",
    "    # Process post-course data\n",
    "    print(\"Processing post-course emails...\")\n",
    "    post_df['original_email'] = post_df['email_address']\n",
    "    post_df['email_normalized'] = post_df['email_address'].apply(normalize_email)\n",
    "    \n",
    "    # Get email stats before domain correction\n",
    "    post_valid_email_before = post_df.dropna(subset=['email_address']).shape[0]\n",
    "    post_valid_normalized_before = post_df.dropna(subset=['email_normalized']).shape[0]\n",
    "    \n",
    "    stats['post_valid_email_before'] = post_valid_email_before\n",
    "    stats['post_valid_normalized_before'] = post_valid_normalized_before\n",
    "    \n",
    "    print(f\"Post-course emails before normalization: {post_valid_email_before}\")\n",
    "    print(f\"Post-course emails after normalization: {post_valid_normalized_before}\")\n",
    "    \n",
    "    if 'full_name' not in post_df.columns:\n",
    "        if 'first_name' in post_df.columns and 'last_name' in post_df.columns:\n",
    "            post_df['first_name'] = post_df['first_name'].fillna('')\n",
    "            post_df['last_name'] = post_df['last_name'].fillna('')\n",
    "            post_df['full_name'] = post_df['first_name'] + ' ' + post_df['last_name']\n",
    "            post_df['full_name'] = post_df['full_name'].str.strip()\n",
    "    \n",
    "    post_df['name_normalized'] = post_df['full_name'].apply(normalize_name)\n",
    "    post_df['first_name_norm'] = post_df['first_name'].apply(normalize_name) if 'first_name' in post_df.columns else None\n",
    "    post_df['last_name_norm'] = post_df['last_name'].apply(normalize_name) if 'last_name' in post_df.columns else None\n",
    "    \n",
    "    # Extract DOB components\n",
    "    print(\"Processing post-course dates of birth...\")\n",
    "    dob_results = post_df['date_of_birth'].apply(standardize_dob)\n",
    "    post_df['dob_year'], post_df['dob_month'], post_df['dob_day'] = zip(*dob_results)\n",
    "    \n",
    "    # Step 2: Perform matching across different identifiers\n",
    "    \n",
    "    # 2.1 Start with email matching\n",
    "    print(\"\\nPerforming email matching...\")\n",
    "    \n",
    "    # Remove invalid emails\n",
    "    pre_df_valid_email = pre_df.dropna(subset=['email_normalized'])\n",
    "    post_df_valid_email = post_df.dropna(subset=['email_normalized'])\n",
    "    \n",
    "    pre_valid_email_count = len(pre_df_valid_email)\n",
    "    post_valid_email_count = len(post_df_valid_email)\n",
    "    \n",
    "    stats['pre_valid_email_count'] = pre_valid_email_count\n",
    "    stats['post_valid_email_count'] = post_valid_email_count\n",
    "    \n",
    "    print(f\"Pre-enrollment records with valid emails: {pre_valid_email_count} ({pre_valid_email_count/pre_total*100:.2f}%)\")\n",
    "    print(f\"Post-course records with valid emails: {post_valid_email_count} ({post_valid_email_count/post_total*100:.2f}%)\")\n",
    "    \n",
    "    # Get unique normalized emails\n",
    "    pre_emails = set(pre_df_valid_email['email_normalized'].unique())\n",
    "    post_emails = set(post_df_valid_email['email_normalized'].unique())\n",
    "    \n",
    "    pre_unique_email_count = len(pre_emails)\n",
    "    post_unique_email_count = len(post_emails)\n",
    "    \n",
    "    stats['pre_unique_email_count'] = pre_unique_email_count\n",
    "    stats['post_unique_email_count'] = post_unique_email_count\n",
    "    \n",
    "    print(f\"Unique emails in pre-enrollment: {pre_unique_email_count}\")\n",
    "    print(f\"Unique emails in post-course: {post_unique_email_count}\")\n",
    "    \n",
    "    # Find matching emails\n",
    "    email_matches = pre_emails.intersection(post_emails)\n",
    "    email_match_count = len(email_matches)\n",
    "    \n",
    "    stats['email_match_count'] = email_match_count\n",
    "    \n",
    "    print(f\"Found {email_match_count} matches by email\")\n",
    "    print(f\"Email match rate: {email_match_count/pre_unique_email_count*100:.2f}% of pre-enrollment emails\")\n",
    "    print(f\"Email match rate: {email_match_count/post_unique_email_count*100:.2f}% of post-course emails\")\n",
    "    \n",
    "    # Extract domains before and after correction for analysis\n",
    "    def extract_domain(email):\n",
    "        if pd.isna(email):\n",
    "            return None\n",
    "        try:\n",
    "            return email.split('@')[1]\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    pre_df['original_domain'] = pre_df['original_email'].apply(extract_domain)\n",
    "    pre_df['corrected_domain'] = pre_df['email_normalized'].apply(extract_domain)\n",
    "    \n",
    "    post_df['original_domain'] = post_df['original_email'].apply(extract_domain)\n",
    "    post_df['corrected_domain'] = post_df['email_normalized'].apply(extract_domain)\n",
    "    \n",
    "    # Count domain corrections\n",
    "    pre_domain_changes = pre_df[pre_df['original_domain'] != pre_df['corrected_domain']].dropna(subset=['original_domain', 'corrected_domain'])\n",
    "    post_domain_changes = post_df[post_df['original_domain'] != post_df['corrected_domain']].dropna(subset=['original_domain', 'corrected_domain'])\n",
    "    \n",
    "    pre_domain_corrections_count = len(pre_domain_changes)\n",
    "    post_domain_corrections_count = len(post_domain_changes)\n",
    "    \n",
    "    stats['pre_domain_corrections_count'] = pre_domain_corrections_count\n",
    "    stats['post_domain_corrections_count'] = post_domain_corrections_count\n",
    "    \n",
    "    print(f\"\\nDomain corrections applied:\")\n",
    "    print(f\"Pre-enrollment: {pre_domain_corrections_count} domains corrected\")\n",
    "    print(f\"Post-course: {post_domain_corrections_count} domains corrected\")\n",
    "    \n",
    "    # 2.2 Next, try matching by full name + DOB for records without email matches\n",
    "    print(\"\\nPerforming full name + DOB matching...\")\n",
    "    \n",
    "    # Get records without email matches\n",
    "    pre_unmatched_email = pre_df[~pre_df['email_normalized'].isin(email_matches)]\n",
    "    post_unmatched_email = post_df[~post_df['email_normalized'].isin(email_matches)]\n",
    "    \n",
    "    # Create a composite key for name+DOB matching\n",
    "    def create_name_dob_key(row):\n",
    "        if pd.isna(row['name_normalized']) or pd.isna(row['dob_year']):\n",
    "            return None\n",
    "        return f\"{row['name_normalized']}_{row['dob_year']}_{row['dob_month']}_{row['dob_day']}\"\n",
    "    \n",
    "    pre_unmatched_email['name_dob_key'] = pre_unmatched_email.apply(create_name_dob_key, axis=1)\n",
    "    post_unmatched_email['name_dob_key'] = post_unmatched_email.apply(create_name_dob_key, axis=1)\n",
    "    \n",
    "    # Remove invalid keys\n",
    "    pre_valid_name_dob = pre_unmatched_email.dropna(subset=['name_dob_key'])\n",
    "    post_valid_name_dob = post_unmatched_email.dropna(subset=['name_dob_key'])\n",
    "    \n",
    "    pre_valid_name_dob_count = len(pre_valid_name_dob)\n",
    "    post_valid_name_dob_count = len(post_valid_name_dob)\n",
    "    \n",
    "    stats['pre_valid_name_dob_count'] = pre_valid_name_dob_count\n",
    "    stats['post_valid_name_dob_count'] = post_valid_name_dob_count\n",
    "    \n",
    "    print(f\"Pre-enrollment records with valid name+DOB (unmatched by email): {pre_valid_name_dob_count}\")\n",
    "    print(f\"Post-course records with valid name+DOB (unmatched by email): {post_valid_name_dob_count}\")\n",
    "    \n",
    "    # Get unique keys\n",
    "    pre_name_dob_keys = set(pre_valid_name_dob['name_dob_key'].unique())\n",
    "    post_name_dob_keys = set(post_valid_name_dob['name_dob_key'].unique())\n",
    "    \n",
    "    # Find matching name+DOB combinations\n",
    "    name_dob_matches = pre_name_dob_keys.intersection(post_name_dob_keys)\n",
    "    name_dob_match_count = len(name_dob_matches)\n",
    "    \n",
    "    stats['name_dob_match_count'] = name_dob_match_count\n",
    "    \n",
    "    print(f\"Found {name_dob_match_count} additional matches by full name + DOB\")\n",
    "    \n",
    "    # 2.3 Try matching by first name, last name, and birth year for remaining records\n",
    "    print(\"\\nPerforming first name + last name + birth year matching...\")\n",
    "    \n",
    "    # Get records without email or name+DOB matches\n",
    "    pre_matched_ids = set(pre_df[pre_df['email_normalized'].isin(email_matches)].index).union(\n",
    "                       set(pre_valid_name_dob[pre_valid_name_dob['name_dob_key'].isin(name_dob_matches)].index))\n",
    "    \n",
    "    post_matched_ids = set(post_df[post_df['email_normalized'].isin(email_matches)].index).union(\n",
    "                        set(post_valid_name_dob[post_valid_name_dob['name_dob_key'].isin(name_dob_matches)].index))\n",
    "    \n",
    "    pre_unmatched = pre_df[~pre_df.index.isin(pre_matched_ids)]\n",
    "    post_unmatched = post_df[~post_df.index.isin(post_matched_ids)]\n",
    "    \n",
    "    # Create a composite key for first+last+year matching\n",
    "    def create_name_year_key(row):\n",
    "        if (pd.isna(row['first_name_norm']) or pd.isna(row['last_name_norm']) or \n",
    "            pd.isna(row['dob_year'])):\n",
    "            return None\n",
    "        return f\"{row['first_name_norm']}_{row['last_name_norm']}_{row['dob_year']}\"\n",
    "    \n",
    "    pre_unmatched['name_year_key'] = pre_unmatched.apply(create_name_year_key, axis=1)\n",
    "    post_unmatched['name_year_key'] = post_unmatched.apply(create_name_year_key, axis=1)\n",
    "    \n",
    "    # Remove invalid keys\n",
    "    pre_valid_name_year = pre_unmatched.dropna(subset=['name_year_key'])\n",
    "    post_valid_name_year = post_unmatched.dropna(subset=['name_year_key'])\n",
    "    \n",
    "    pre_valid_name_year_count = len(pre_valid_name_year)\n",
    "    post_valid_name_year_count = len(post_valid_name_year)\n",
    "    \n",
    "    stats['pre_valid_name_year_count'] = pre_valid_name_year_count\n",
    "    stats['post_valid_name_year_count'] = post_valid_name_year_count\n",
    "    \n",
    "    print(f\"Pre-enrollment records with valid name+year (still unmatched): {pre_valid_name_year_count}\")\n",
    "    print(f\"Post-course records with valid name+year (still unmatched): {post_valid_name_year_count}\")\n",
    "    \n",
    "    # Get unique keys\n",
    "    pre_name_year_keys = set(pre_valid_name_year['name_year_key'].unique())\n",
    "    post_name_year_keys = set(post_valid_name_year['name_year_key'].unique())\n",
    "    \n",
    "    # Find matching name+year combinations\n",
    "    name_year_matches = pre_name_year_keys.intersection(post_name_year_keys)\n",
    "    name_year_match_count = len(name_year_matches)\n",
    "    \n",
    "    stats['name_year_match_count'] = name_year_match_count\n",
    "    \n",
    "    print(f\"Found {name_year_match_count} additional matches by first name + last name + birth year\")\n",
    "    \n",
    "    # Calculate total matches\n",
    "    total_matches = email_match_count + name_dob_match_count + name_year_match_count\n",
    "    stats['total_matches'] = total_matches\n",
    "    \n",
    "    print(f\"\\nTotal matched records: {total_matches}\")\n",
    "    print(f\"Overall match rate: {total_matches/pre_unique_email_count*100:.2f}% of pre-enrollment emails\")\n",
    "    print(f\"Overall match rate: {total_matches/post_unique_email_count*100:.2f}% of post-course emails\")\n",
    "    \n",
    "    # 3. Create matching pairs for merging\n",
    "    print(\"\\nCreating matching pairs...\")\n",
    "    \n",
    "    # 3.1 Email matches\n",
    "    email_match_pairs = []\n",
    "    for email in email_matches:\n",
    "        # There could be multiple records with the same email in either dataset\n",
    "        pre_matches = pre_df_valid_email[pre_df_valid_email['email_normalized'] == email]\n",
    "        post_matches = post_df_valid_email[post_df_valid_email['email_normalized'] == email]\n",
    "        \n",
    "        # Create all possible pairs\n",
    "        for pre_idx, pre_row in pre_matches.iterrows():\n",
    "            for post_idx, post_row in post_matches.iterrows():\n",
    "                email_match_pairs.append({\n",
    "                    'pre_idx': pre_idx,\n",
    "                    'post_idx': post_idx,\n",
    "                    'match_type': 'email',\n",
    "                    'pre_email': pre_row['original_email'],\n",
    "                    'post_email': post_row['original_email'],\n",
    "                    'pre_normalized': pre_row['email_normalized'],\n",
    "                    'post_normalized': post_row['email_normalized'],\n",
    "                    'pre_name': pre_row['full_name'] if 'full_name' in pre_row else '',\n",
    "                    'post_name': post_row['full_name'] if 'full_name' in post_row else ''\n",
    "                })\n",
    "    \n",
    "    # 3.2 Name+DOB matches\n",
    "    name_dob_match_pairs = []\n",
    "    for key in name_dob_matches:\n",
    "        # Get matching records\n",
    "        pre_matches = pre_valid_name_dob[pre_valid_name_dob['name_dob_key'] == key]\n",
    "        post_matches = post_valid_name_dob[post_valid_name_dob['name_dob_key'] == key]\n",
    "        \n",
    "        # Create all possible pairs\n",
    "        for pre_idx, pre_row in pre_matches.iterrows():\n",
    "            for post_idx, post_row in post_matches.iterrows():\n",
    "                name_dob_match_pairs.append({\n",
    "                    'pre_idx': pre_idx,\n",
    "                    'post_idx': post_idx,\n",
    "                    'match_type': 'name_dob',\n",
    "                    'pre_email': pre_row['original_email'],\n",
    "                    'post_email': post_row['original_email'],\n",
    "                    'pre_normalized': pre_row['email_normalized'] if not pd.isna(pre_row['email_normalized']) else '',\n",
    "                    'post_normalized': post_row['email_normalized'] if not pd.isna(post_row['email_normalized']) else '',\n",
    "                    'pre_name': pre_row['full_name'] if 'full_name' in pre_row else '',\n",
    "                    'post_name': post_row['full_name'] if 'full_name' in post_row else '',\n",
    "                    'match_key': key\n",
    "                })\n",
    "    \n",
    "    # 3.3 First name + last name + birth year matches\n",
    "    name_year_match_pairs = []\n",
    "    for key in name_year_matches:\n",
    "        # Get matching records\n",
    "        pre_matches = pre_valid_name_year[pre_valid_name_year['name_year_key'] == key]\n",
    "        post_matches = post_valid_name_year[post_valid_name_year['name_year_key'] == key]\n",
    "        \n",
    "        # Create all possible pairs\n",
    "        for pre_idx, pre_row in pre_matches.iterrows():\n",
    "            for post_idx, post_row in post_matches.iterrows():\n",
    "                name_year_match_pairs.append({\n",
    "                    'pre_idx': pre_idx,\n",
    "                    'post_idx': post_idx,\n",
    "                    'match_type': 'name_year',\n",
    "                    'pre_email': pre_row['original_email'],\n",
    "                    'post_email': post_row['original_email'],\n",
    "                    'pre_normalized': pre_row['email_normalized'] if not pd.isna(pre_row['email_normalized']) else '',\n",
    "                    'post_normalized': post_row['email_normalized'] if not pd.isna(post_row['email_normalized']) else '',\n",
    "                    'pre_name': pre_row['full_name'] if 'full_name' in pre_row else '',\n",
    "                    'post_name': post_row['full_name'] if 'full_name' in post_row else '',\n",
    "                    'match_key': key\n",
    "                })\n",
    "    \n",
    "    # Combine all match pairs\n",
    "    all_match_pairs = email_match_pairs + name_dob_match_pairs + name_year_match_pairs\n",
    "    \n",
    "    # Create a DataFrame with all match pairs\n",
    "    match_pairs_df = pd.DataFrame(all_match_pairs)\n",
    "    \n",
    "    # Count matches by type\n",
    "    match_type_counts = match_pairs_df['match_type'].value_counts()\n",
    "    \n",
    "    stats['email_match_pairs'] = len(email_match_pairs)\n",
    "    stats['name_dob_match_pairs'] = len(name_dob_match_pairs)\n",
    "    stats['name_year_match_pairs'] = len(name_year_match_pairs)\n",
    "    stats['total_match_pairs'] = len(all_match_pairs)\n",
    "    \n",
    "    print(\"\\nMatches by type:\")\n",
    "    for match_type, count in match_type_counts.items():\n",
    "        print(f\"  {match_type}: {count} match pairs\")\n",
    "    \n",
    "    # 4. Analyze domain corrections that led to successful matches\n",
    "    print(\"\\nAnalyzing domain corrections that led to successful matches...\")\n",
    "    \n",
    "    # Find emails that matched due to domain correction\n",
    "    corrected_matches = []\n",
    "    for idx, row in match_pairs_df.iterrows():\n",
    "        if row['match_type'] == 'email':\n",
    "            pre_email = row['pre_email']\n",
    "            post_email = row['post_email']\n",
    "            pre_norm = row['pre_normalized']\n",
    "            post_norm = row['post_normalized']\n",
    "            \n",
    "            # Check if the original emails were different but normalized ones match\n",
    "            if pre_email != post_email and pre_norm == post_norm:\n",
    "                corrected_matches.append({\n",
    "                    'pre_email': pre_email,\n",
    "                    'post_email': post_email,\n",
    "                    'normalized_email': pre_norm,\n",
    "                    'pre_domain': extract_domain(pre_email),\n",
    "                    'post_domain': extract_domain(post_email),\n",
    "                    'corrected_domain': extract_domain(pre_norm)\n",
    "                })\n",
    "    \n",
    "    corrected_matches_df = pd.DataFrame(corrected_matches)\n",
    "    stats['corrected_domain_matches'] = len(corrected_matches_df)\n",
    "    \n",
    "    print(f\"Found {len(corrected_matches_df)} matches due to domain correction\")\n",
    "    \n",
    "    # 5. Save match analysis\n",
    "    print(\"\\nSaving match analysis...\")\n",
    "    \n",
    "    output_dir = os.path.dirname(pre_course_path)\n",
    "    output_file = os.path.join(output_dir, \"comprehensive_match_analysis.xlsx\")\n",
    "    \n",
    "    # Create summary dataframe\n",
    "    summary_data = [\n",
    "        {'Category': 'Dataset Sizes', 'Metric': 'Total Pre-Enrollment Records', 'Value': stats['pre_total']},\n",
    "        {'Category': 'Dataset Sizes', 'Metric': 'Total Post-Course Records', 'Value': stats['post_total']},\n",
    "        \n",
    "        {'Category': 'Email Standardization', 'Metric': 'Pre-Enrollment Records with Original Email', 'Value': stats['pre_valid_email_before']},\n",
    "        {'Category': 'Email Standardization', 'Metric': 'Pre-Enrollment Records with Normalized Email', 'Value': stats['pre_valid_normalized_before']},\n",
    "        {'Category': 'Email Standardization', 'Metric': 'Post-Course Records with Original Email', 'Value': stats['post_valid_email_before']},\n",
    "        {'Category': 'Email Standardization', 'Metric': 'Post-Course Records with Normalized Email', 'Value': stats['post_valid_normalized_before']},\n",
    "        \n",
    "        {'Category': 'Domain Correction', 'Metric': 'Pre-Enrollment Domains Corrected', 'Value': stats['pre_domain_corrections_count']},\n",
    "        {'Category': 'Domain Correction', 'Metric': 'Post-Course Domains Corrected', 'Value': stats['post_domain_corrections_count']},\n",
    "        {'Category': 'Domain Correction', 'Metric': 'Matches Due to Domain Correction', 'Value': stats['corrected_domain_matches']},\n",
    "        \n",
    "        {'Category': 'Valid Identifiers', 'Metric': 'Pre-Enrollment Records with Valid Email', 'Value': stats['pre_valid_email_count']},\n",
    "        {'Category': 'Valid Identifiers', 'Metric': 'Post-Course Records with Valid Email', 'Value': stats['post_valid_email_count']},\n",
    "        {'Category': 'Valid Identifiers', 'Metric': 'Pre-Enrollment Unique Emails', 'Value': stats['pre_unique_email_count']},\n",
    "        {'Category': 'Valid Identifiers', 'Metric': 'Post-Course Unique Emails', 'Value': stats['post_unique_email_count']},\n",
    "        {'Category': 'Valid Identifiers', 'Metric': 'Pre-Enrollment Unmatched Records with Valid Name+DOB', 'Value': stats['pre_valid_name_dob_count']},\n",
    "        {'Category': 'Valid Identifiers', 'Metric': 'Post-Course Unmatched Records with Valid Name+DOB', 'Value': stats['post_valid_name_dob_count']},\n",
    "        {'Category': 'Valid Identifiers', 'Metric': 'Pre-Enrollment Remaining Records with Valid Name+Year', 'Value': stats['pre_valid_name_year_count']},\n",
    "        {'Category': 'Valid Identifiers', 'Metric': 'Post-Course Remaining Records with Valid Name+Year', 'Value': stats['post_valid_name_year_count']},\n",
    "        \n",
    "        {'Category': 'Matches by Type', 'Metric': 'Matches by Email', 'Value': stats['email_match_count']},\n",
    "        {'Category': 'Matches by Type', 'Metric': 'Additional Matches by Full Name + DOB', 'Value': stats['name_dob_match_count']},\n",
    "        {'Category': 'Matches by Type', 'Metric': 'Additional Matches by Name + Birth Year', 'Value': stats['name_year_match_count']},\n",
    "        {'Category': 'Matches by Type', 'Metric': 'Total Unique Matches', 'Value': stats['total_matches']},\n",
    "        \n",
    "        {'Category': 'Match Pairs', 'Metric': 'Email Match Pairs', 'Value': stats['email_match_pairs']},\n",
    "        {'Category': 'Match Pairs', 'Metric': 'Name+DOB Match Pairs', 'Value': stats['name_dob_match_pairs']},\n",
    "        {'Category': 'Match Pairs', 'Metric': 'Name+Year Match Pairs', 'Value': stats['name_year_match_pairs']},\n",
    "        {'Category': 'Match Pairs', 'Metric': 'Total Match Pairs', 'Value': stats['total_match_pairs']},\n",
    "        \n",
    "        {'Category': 'Match Rates', 'Metric': 'Email Match Rate (% of Pre-Enrollment Emails)', 'Value': f\"{email_match_count/pre_unique_email_count*100:.2f}%\"},\n",
    "        {'Category': 'Match Rates', 'Metric': 'Email Match Rate (% of Post-Course Emails)', 'Value': f\"{email_match_count/post_unique_email_count*100:.2f}%\"},\n",
    "        {'Category': 'Match Rates', 'Metric': 'Overall Match Rate (% of Pre-Enrollment)', 'Value': f\"{total_matches/stats['pre_unique_email_count']*100:.2f}%\"},\n",
    "        {'Category': 'Match Rates', 'Metric': 'Overall Match Rate (% of Post-Course)', 'Value': f\"{total_matches/stats['post_unique_email_count']*100:.2f}%\"}\n",
    "    ]\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    # Save to Excel\n",
    "    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "        summary_df.to_excel(writer, sheet_name='Match Summary', index=False)\n",
    "        match_pairs_df.to_excel(writer, sheet_name='All Match Pairs', index=False)\n",
    "        \n",
    "        # Save domain correction analysis\n",
    "        corrected_matches_df.to_excel(writer, sheet_name='Domain Correction Matches', index=False)\n",
    "        \n",
    "        # Sample of each match type\n",
    "        for match_type in match_pairs_df['match_type'].unique():\n",
    "            sample_df = match_pairs_df[match_pairs_df['match_type'] == match_type].head(500)\n",
    "            sample_df.to_excel(writer, sheet_name=f'{match_type}_matches_sample', index=False)\n",
    "        \n",
    "        # Domain analysis\n",
    "        # Original pre-enrollment domains\n",
    "        pre_original_domains = pre_df['original_domain'].value_counts().reset_index()\n",
    "        pre_original_domains.columns = ['Domain', 'Count']\n",
    "        pre_original_domains['Percentage'] = pre_original_domains['Count'] / pre_original_domains['Count'].sum() * 100\n",
    "        \n",
    "        # Corrected pre-enrollment domains\n",
    "        pre_corrected_domains = pre_df['corrected_domain'].value_counts().reset_index()\n",
    "        pre_corrected_domains.columns = ['Domain', 'Count']\n",
    "        pre_corrected_domains['Percentage'] = pre_corrected_domains['Count'] / pre_corrected_domains['Count'].sum() * 100\n",
    "        \n",
    "        # Original post-course domains\n",
    "        post_original_domains = post_df['original_domain'].value_counts().reset_index()\n",
    "        post_original_domains.columns = ['Domain', 'Count']\n",
    "        post_original_domains['Percentage'] = post_original_domains['Count'] / post_original_domains['Count'].sum() * 100\n",
    "        \n",
    "        # Corrected post-course domains\n",
    "        post_corrected_domains = post_df['corrected_domain'].value_counts().reset_index()\n",
    "        post_corrected_domains.columns = ['Domain', 'Count']\n",
    "        post_corrected_domains['Percentage'] = post_corrected_domains['Count'] / post_corrected_domains['Count'].sum() * 100\n",
    "        \n",
    "        # Save domain analysis\n",
    "        pre_original_domains.to_excel(writer, sheet_name='Pre Original Domains', index=False)\n",
    "        pre_corrected_domains.to_excel(writer, sheet_name='Pre Corrected Domains', index=False)\n",
    "        post_original_domains.to_excel(writer, sheet_name='Post Original Domains', index=False)\n",
    "        post_corrected_domains.to_excel(writer, sheet_name='Post Corrected Domains', index=False)\n",
    "        \n",
    "        # Domain correction details\n",
    "        pre_domain_changes_df = pre_df[pre_df['original_domain'] != pre_df['corrected_domain']].dropna(subset=['original_domain', 'corrected_domain'])\n",
    "        pre_domain_changes_df = pre_domain_changes_df[['original_email', 'email_normalized', 'original_domain', 'corrected_domain']]\n",
    "        pre_domain_changes_df.to_excel(writer, sheet_name='Pre Domain Corrections', index=False)\n",
    "        \n",
    "        post_domain_changes_df = post_df[post_df['original_domain'] != post_df['corrected_domain']].dropna(subset=['original_domain', 'corrected_domain'])\n",
    "        post_domain_changes_df = post_domain_changes_df[['original_email', 'email_normalized', 'original_domain', 'corrected_domain']]\n",
    "        post_domain_changes_df.to_excel(writer, sheet_name='Post Domain Corrections', index=False)\n",
    "    \n",
    "    print(f\"Saved comprehensive match analysis to: {output_file}\")\n",
    "    \n",
    "    # Return results\n",
    "    return {\n",
    "        'stats': stats,\n",
    "        'match_pairs_df': match_pairs_df,\n",
    "        'corrected_matches_df': corrected_matches_df,\n",
    "        'pre_df': pre_df,\n",
    "        'post_df': post_df,\n",
    "        'email_matches': email_matches,\n",
    "        'name_dob_matches': name_dob_matches,\n",
    "        'name_year_matches': name_year_matches\n",
    "    }\n",
    "\n",
    "def merge_survey_data(results, output_path=None, include_match_types=None):\n",
    "    \"\"\"\n",
    "    Merge the pre-enrollment and post-course survey data based on matching records.\n",
    "    \n",
    "    Parameters:\n",
    "    results (dict): Results from analyze_survey_matches\n",
    "    output_path (str, optional): Path to save the merged data\n",
    "    include_match_types (list, optional): Types of matches to include ('email', 'name_dob', 'name_year')\n",
    "    \"\"\"\n",
    "    # If match types not specified, use all\n",
    "    if include_match_types is None:\n",
    "        include_match_types = ['email', 'name_dob', 'name_year']\n",
    "    \n",
    "    # Validate match types\n",
    "    valid_match_types = ['email', 'name_dob', 'name_year']\n",
    "    for match_type in include_match_types:\n",
    "        if match_type not in valid_match_types:\n",
    "            print(f\"Warning: Invalid match type '{match_type}'. Ignoring.\")\n",
    "            include_match_types.remove(match_type)\n",
    "    \n",
    "    if not include_match_types:\n",
    "        print(\"Error: No valid match types specified.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Merging datasets using match types: {include_match_types}\")\n",
    "    \n",
    "    # Filter match pairs by type\n",
    "    match_pairs_df = results['match_pairs_df']\n",
    "    filtered_pairs = match_pairs_df[match_pairs_df['match_type'].isin(include_match_types)]\n",
    "    \n",
    "    if len(filtered_pairs) == 0:\n",
    "        print(\"No matching pairs found with specified match types.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Using {len(filtered_pairs)} matching pairs for merging.\")\n",
    "    \n",
    "    # Get original dataframes\n",
    "    pre_df = results['pre_df'].copy()\n",
    "    post_df = results['post_df'].copy()\n",
    "    \n",
    "    # Create a list of unique pre and post indices\n",
    "    pre_indices = filtered_pairs['pre_idx'].unique()\n",
    "    post_indices = filtered_pairs['post_idx'].unique()\n",
    "    \n",
    "    # Filter dataframes to only include matched records\n",
    "    pre_matched = pre_df.loc[pre_indices].copy()\n",
    "    post_matched = post_df.loc[post_indices].copy()\n",
    "    \n",
    "    print(f\"Filtered to {len(pre_matched)} pre-enrollment records and {len(post_matched)} post-course records\")\n",
    "    \n",
    "    # Create a mapping table for merging\n",
    "    # This table will map each pre_idx to the corresponding post_idx\n",
    "    # In case of multiple matches, we'll use the first one for each pre_idx\n",
    "    mapping = {}\n",
    "    for _, group in filtered_pairs.groupby('pre_idx'):\n",
    "        # Use the first match for each pre_idx (prioritizing email matches if available)\n",
    "        priority_order = {'email': 0, 'name_dob': 1, 'name_year': 2}\n",
    "        group_sorted = group.sort_values(by='match_type', key=lambda x: x.map(priority_order))\n",
    "        mapping[group_sorted.iloc[0]['pre_idx']] = group_sorted.iloc[0]['post_idx']\n",
    "    \n",
    "    # Create a new column in pre_matched with the corresponding post_idx\n",
    "    pre_matched['matched_post_idx'] = pre_matched.index.map(lambda x: mapping.get(x, None))\n",
    "    \n",
    "    # Drop rows without a match\n",
    "    pre_matched = pre_matched.dropna(subset=['matched_post_idx'])\n",
    "    \n",
    "    # Reset the index to make it a regular column\n",
    "    pre_matched = pre_matched.reset_index()\n",
    "    post_matched = post_matched.reset_index()\n",
    "    \n",
    "    # Rename columns to avoid conflicts\n",
    "    pre_cols = pre_matched.columns\n",
    "    post_cols = post_matched.columns\n",
    "    \n",
    "    pre_renamed = [col if col not in post_cols or col == 'index' else f\"pre_{col}\" for col in pre_cols]\n",
    "    post_renamed = [col if col not in pre_cols or col == 'index' else f\"post_{col}\" for col in post_cols]\n",
    "    \n",
    "    pre_matched.columns = pre_renamed\n",
    "    post_matched.columns = post_renamed\n",
    "    \n",
    "    # Merge the datasets\n",
    "    merged_df = pd.merge(\n",
    "        pre_matched,\n",
    "        post_matched,\n",
    "        left_on='matched_post_idx',\n",
    "        right_on='index',\n",
    "        how='inner'\n",
    "    )\n",
    "    \n",
    "    # Clean up the merged dataset\n",
    "    # Remove index and matched_post_idx columns\n",
    "    merged_df = merged_df.drop(['matched_post_idx', 'index_x', 'index_y'], axis=1, errors='ignore')\n",
    "    \n",
    "    # Remove normalized and temporary columns\n",
    "    columns_to_drop = [col for col in merged_df.columns if any(x in col for x in \n",
    "                     ['_normalized', '_norm', 'dob_year', 'dob_month', 'dob_day', \n",
    "                      'name_dob_key', 'name_year_key', 'original_domain', 'corrected_domain'])]\n",
    "    merged_df = merged_df.drop(columns_to_drop, axis=1, errors='ignore')\n",
    "    \n",
    "    print(f\"Merged dataset has {len(merged_df)} rows and {len(merged_df.columns)} columns\")\n",
    "    \n",
    "    # Add a column indicating the match type\n",
    "    match_types = {}\n",
    "    for _, row in filtered_pairs.iterrows():\n",
    "        if row['pre_idx'] in mapping and mapping[row['pre_idx']] == row['post_idx']:\n",
    "            match_types[row['pre_idx']] = row['match_type']\n",
    "    \n",
    "    # Convert pre_idx back from string to original type if needed\n",
    "    if 'pre_index' in merged_df.columns:\n",
    "        merged_df['match_type'] = merged_df['pre_index'].map(match_types)\n",
    "    \n",
    "    # Save to Excel if output path provided\n",
    "    if output_path:\n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        \n",
    "        merged_df.to_excel(output_path, index=False)\n",
    "        print(f\"Merged data saved to: {output_path}\")\n",
    "        \n",
    "        # Also save a version with match types separated\n",
    "        output_dir = os.path.dirname(output_path)\n",
    "        base_name = os.path.splitext(os.path.basename(output_path))[0]\n",
    "        \n",
    "        with pd.ExcelWriter(os.path.join(output_dir, f\"{base_name}_by_match_type.xlsx\"), engine='openpyxl') as writer:\n",
    "            merged_df.to_excel(writer, sheet_name='All Matches', index=False)\n",
    "            \n",
    "            # Split by match type\n",
    "            for match_type in include_match_types:\n",
    "                if match_type in merged_df['match_type'].values:\n",
    "                    type_df = merged_df[merged_df['match_type'] == match_type]\n",
    "                    type_df.to_excel(writer, sheet_name=f'{match_type}_matches', index=False)\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pre_course_path = '/Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/ES/Cleaned & Enhanced ES Data .xlsx'\n",
    "    post_course_path = '/Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/PCS/Cleaned_Merged_PCS_2cleaned.xlsx'\n",
    "    \n",
    "    # Analyze matches with comprehensive domain correction and multiple identifiers\n",
    "    results = analyze_survey_matches(pre_course_path, post_course_path)\n",
    "    \n",
    "    # If you want to merge the datasets, uncomment this section\n",
    "    # By default, include all match types\n",
    "    output_dir = os.path.dirname(pre_course_path)\n",
    "    merged_output_path = os.path.join(output_dir, \"Comprehensive_Merged_Surveys.xlsx\")\n",
    "    merged_df = merge_survey_data(results, merged_output_path)\n",
    "    \n",
    "    # Or, to only include email and name+DOB matches (excluding the less reliable name+year matches)\n",
    "    # merged_df = merge_survey_data(results, merged_output_path, include_match_types=['email', 'name_dob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9cee6264-5cea-4613-b13e-6665ad2d7b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing data quality for: /Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/ES/Comprehensive_Merged_Surveys.xlsx\n",
      "Loading data...\n",
      "Loaded 5820 records with 229 columns\n",
      "\n",
      "1. Basic Dataset Information\n",
      "Warning: Column 'match_type' not found in dataset\n",
      "Column pre_email_address: 738 null values (12.68%)\n",
      "Column post_email_address: 0 null values (0.00%)\n",
      "Column pre_full_name: 0 null values (0.00%)\n",
      "Column post_full_name: 0 null values (0.00%)\n",
      "Column pre_date_of_birth: 0 null values (0.00%)\n",
      "Column post_date_of_birth: 0 null values (0.00%)\n",
      "\n",
      "2. Match Type Analysis\n",
      "No match_type column found. Adding match type inference based on available data...\n",
      "Inferred match type distribution:\n",
      "  email: 3188 (54.78%)\n",
      "  name_dob: 2476 (42.54%)\n",
      "  name_year: 124 (2.13%)\n",
      "  unknown: 32 (0.55%)\n",
      "\n",
      "3. Email Quality Analysis\n",
      "Found 14 email-related columns: ['pre_email_address', 'post_email_address', 'pre_creating_and_sending_emails', 'pre_opening_and_replying_to_emails', 'downloading_attachments_i_receive_in_an_email_documents_or_pictures', 'adding_an_attachment_to_an_email_i_am_sending_like_documents_or_pictures', 'pre_original_email', 'post_creating_and_sending_emails', 'post_opening_and_replying_to_emails', 'downloading_attachments_i_receive_in_emails_like_documents_or_pictures', 'adding_an_attachment_to_an_email_i_am_sending', 'communicating_with_others_by_email', 'resume_email', 'post_original_email']\n",
      "Using pre_email_address and post_email_address for email analysis\n",
      "Found 0 pre-enrollment emails with duplicates\n",
      "Found 152 post-course emails with duplicates\n",
      "Top 10 post-course duplicated emails:\n",
      "  ctis3marla@gmail.com: 4 occurrences\n",
      "  emilyc1920blancogmail.com: 4 occurrences\n",
      "  zbouchallla812@gmail.com: 4 occurrences\n",
      "  debarrosda79@gmail.com: 3 occurrences\n",
      "  dukensemile193gmail.com: 3 occurrences\n",
      "  aliciaorbegoso24gmail.com: 3 occurrences\n",
      "  angelinem576@gmail.com: 3 occurrences\n",
      "  jperrygranny02310@gmail.com: 3 occurrences\n",
      "  ekarima566@gmail.com: 3 occurrences\n",
      "  destineg811@gmail.com: 3 occurrences\n",
      "\n",
      "Analyzing email format consistency between pre and post...\n",
      "Found 324 records where pre and post email domains don't match\n",
      "Sample of domain mismatches:\n",
      "  sovannyos1@gmail.com -> sovannyos1@gmai.com\n",
      "  rambertchristopher162@gmail.com -> rambertchristopher162@gmaill.com\n",
      "  csl.clarens24@gmail.coom -> csl.clarens24@gmail.com\n",
      "  jyancey09@gmaill.com -> jyancey09@gmail.com\n",
      "  galaamohammed123@gamail.com -> galaamohammed123@gmail.com\n",
      "  winnifredsimms1@gmial.com -> winnifredsimms1@gmail.com\n",
      "  rlmonzon66@gmail.com -> rlmonzon66@gmial.com\n",
      "  mamadousbigbrother@gmail.com -> mamadousbigbrother@gmai.com\n",
      "  marievenamacilien@gmail.com -> marievenamacilien@gmai.com\n",
      "  sonia.fuentes3@gmail.com -> sonia.fuentes3@gimail.com\n",
      "\n",
      "4. Name and DOB Consistency Analysis\n",
      "Exact name matches: 5183 (89.05%)\n",
      "Found 637 records with name differences\n",
      "Name difference categories:\n",
      "  Different Number of Name Parts: 252 (39.56%)\n",
      "  Last Name Match Only: 133 (20.88%)\n",
      "  First Name Match Only: 126 (19.78%)\n",
      "  Other Difference: 105 (16.48%)\n",
      "  First/Last Swapped: 21 (3.30%)\n",
      "Exact DOB matches: 5408 (92.92%)\n",
      "Found 412 records with DOB differences\n",
      "  Same year, different month/day: 188 records\n",
      "  Month/day swapped: 61 records\n",
      "\n",
      "5. Match Quality Analysis\n",
      "Match confidence distribution:\n",
      "  Very Low: 170 (2.92%)\n",
      "  Low: 2492 (42.82%)\n",
      "  Moderate: 260 (4.47%)\n",
      "  Good: 394 (6.77%)\n",
      "  High: 86 (1.48%)\n",
      "  Perfect: 2418 (41.55%)\n",
      "Found 2662 potentially problematic matches with confidence < 70%\n",
      "\n",
      "6. Additional Data Quality Checks\n",
      "Age at course statistics:\n",
      "  Min: -1.0\n",
      "  Max: 141.0\n",
      "  Mean: 44.0\n",
      "  Found 22 records with age < 5\n",
      "  Found 8 records with age > 90\n",
      "Found 73 records where pre-course year is after post-course year\n",
      "\n",
      "7. Generating Summary Report\n",
      "Data quality analysis complete. Reports saved to: /Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/ES/Data_Quality_Analysis\n"
     ]
    }
   ],
   "source": [
    "#merge data quality analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def analyze_data_quality(merged_file_path):\n",
    "    \"\"\"\n",
    "    Comprehensive analysis of data quality in the merged dataset\n",
    "    \n",
    "    Parameters:\n",
    "    merged_file_path (str): Path to the merged Excel file\n",
    "    \"\"\"\n",
    "    print(f\"Analyzing data quality for: {merged_file_path}\")\n",
    "    \n",
    "    # Load the merged data\n",
    "    print(\"Loading data...\")\n",
    "    merged_df = pd.read_excel(merged_file_path)\n",
    "    print(f\"Loaded {len(merged_df)} records with {len(merged_df.columns)} columns\")\n",
    "    \n",
    "    # Create output directory for reports\n",
    "    output_dir = os.path.dirname(merged_file_path)\n",
    "    analysis_dir = os.path.join(output_dir, \"Data_Quality_Analysis\")\n",
    "    os.makedirs(analysis_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize report\n",
    "    report_data = []\n",
    "    \n",
    "    # 1. Basic Dataset Information\n",
    "    print(\"\\n1. Basic Dataset Information\")\n",
    "    report_data.append({\"Category\": \"Basic Info\", \"Metric\": \"Total Records\", \"Value\": len(merged_df)})\n",
    "    report_data.append({\"Category\": \"Basic Info\", \"Metric\": \"Total Columns\", \"Value\": len(merged_df.columns)})\n",
    "    \n",
    "    # Check for null values in key columns\n",
    "    key_columns = [\n",
    "        'pre_email_address', 'post_email_address', \n",
    "        'pre_full_name', 'post_full_name',\n",
    "        'pre_date_of_birth', 'post_date_of_birth',\n",
    "        'match_type'\n",
    "    ]\n",
    "    \n",
    "    # Adjust column names if they don't exist exactly as above\n",
    "    existing_columns = []\n",
    "    for col in key_columns:\n",
    "        if col in merged_df.columns:\n",
    "            existing_columns.append(col)\n",
    "        else:\n",
    "            print(f\"Warning: Column '{col}' not found in dataset\")\n",
    "    \n",
    "    for col in existing_columns:\n",
    "        null_count = merged_df[col].isna().sum()\n",
    "        null_percent = (null_count / len(merged_df)) * 100\n",
    "        report_data.append({\"Category\": \"Missing Values\", \"Metric\": f\"Null in {col}\", \n",
    "                           \"Value\": f\"{null_count} ({null_percent:.2f}%)\"})\n",
    "        print(f\"Column {col}: {null_count} null values ({null_percent:.2f}%)\")\n",
    "    \n",
    "    # 2. Match Type Analysis\n",
    "    print(\"\\n2. Match Type Analysis\")\n",
    "    if 'match_type' in merged_df.columns:\n",
    "        match_counts = merged_df['match_type'].value_counts()\n",
    "        print(\"Match type distribution:\")\n",
    "        for match_type, count in match_counts.items():\n",
    "            percent = (count / len(merged_df)) * 100\n",
    "            report_data.append({\"Category\": \"Match Types\", \"Metric\": f\"{match_type}\", \n",
    "                               \"Value\": f\"{count} ({percent:.2f}%)\"})\n",
    "            print(f\"  {match_type}: {count} ({percent:.2f}%)\")\n",
    "    else:\n",
    "        print(\"No match_type column found. Adding match type inference based on available data...\")\n",
    "        \n",
    "        # Infer match type based on available data\n",
    "        def infer_match_type(row):\n",
    "            # Check if email addresses match\n",
    "            if 'pre_email_address' in merged_df.columns and 'post_email_address' in merged_df.columns:\n",
    "                if pd.notna(row['pre_email_address']) and pd.notna(row['post_email_address']):\n",
    "                    if str(row['pre_email_address']).lower().strip() == str(row['post_email_address']).lower().strip():\n",
    "                        return 'email'\n",
    "            \n",
    "            # Check if full names and DOB match\n",
    "            if ('pre_full_name' in merged_df.columns and 'post_full_name' in merged_df.columns and\n",
    "                'pre_date_of_birth' in merged_df.columns and 'post_date_of_birth' in merged_df.columns):\n",
    "                if (pd.notna(row['pre_full_name']) and pd.notna(row['post_full_name']) and\n",
    "                    pd.notna(row['pre_date_of_birth']) and pd.notna(row['post_date_of_birth'])):\n",
    "                    if (str(row['pre_full_name']).lower().strip() == str(row['post_full_name']).lower().strip() and\n",
    "                        str(row['pre_date_of_birth']) == str(row['post_date_of_birth'])):\n",
    "                        return 'name_dob'\n",
    "            \n",
    "            # Check if first and last names and birth year match\n",
    "            if ('pre_first_name' in merged_df.columns and 'post_first_name' in merged_df.columns and\n",
    "                'pre_last_name' in merged_df.columns and 'post_last_name' in merged_df.columns and\n",
    "                'pre_date_of_birth' in merged_df.columns and 'post_date_of_birth' in merged_df.columns):\n",
    "                if (pd.notna(row['pre_first_name']) and pd.notna(row['post_first_name']) and\n",
    "                    pd.notna(row['pre_last_name']) and pd.notna(row['post_last_name'])):\n",
    "                    if (str(row['pre_first_name']).lower().strip() == str(row['post_first_name']).lower().strip() and\n",
    "                        str(row['pre_last_name']).lower().strip() == str(row['post_last_name']).lower().strip()):\n",
    "                        return 'name_year'\n",
    "            \n",
    "            return 'unknown'\n",
    "        \n",
    "        merged_df['inferred_match_type'] = merged_df.apply(infer_match_type, axis=1)\n",
    "        \n",
    "        inferred_counts = merged_df['inferred_match_type'].value_counts()\n",
    "        print(\"Inferred match type distribution:\")\n",
    "        for match_type, count in inferred_counts.items():\n",
    "            percent = (count / len(merged_df)) * 100\n",
    "            report_data.append({\"Category\": \"Inferred Match Types\", \"Metric\": f\"{match_type}\", \n",
    "                               \"Value\": f\"{count} ({percent:.2f}%)\"})\n",
    "            print(f\"  {match_type}: {count} ({percent:.2f}%)\")\n",
    "    \n",
    "    # 3. Email Quality Analysis\n",
    "    print(\"\\n3. Email Quality Analysis\")\n",
    "    email_cols = [col for col in merged_df.columns if 'email' in col.lower()]\n",
    "    print(f\"Found {len(email_cols)} email-related columns: {email_cols}\")\n",
    "    \n",
    "    pre_email_col = None\n",
    "    post_email_col = None\n",
    "    \n",
    "    # Identify the main email columns\n",
    "    for col in email_cols:\n",
    "        if 'pre' in col.lower() and 'address' in col.lower():\n",
    "            pre_email_col = col\n",
    "        elif 'post' in col.lower() and 'address' in col.lower():\n",
    "            post_email_col = col\n",
    "    \n",
    "    if not pre_email_col:\n",
    "        pre_email_col = 'pre_email_address' if 'pre_email_address' in merged_df.columns else 'pre_original_email'\n",
    "    \n",
    "    if not post_email_col:\n",
    "        post_email_col = 'post_email_address' if 'post_email_address' in merged_df.columns else 'post_original_email'\n",
    "    \n",
    "    print(f\"Using {pre_email_col} and {post_email_col} for email analysis\")\n",
    "    \n",
    "    # Check for duplicate emails\n",
    "    if pre_email_col in merged_df.columns:\n",
    "        pre_email_counts = merged_df[pre_email_col].value_counts()\n",
    "        pre_duplicates = pre_email_counts[pre_email_counts > 1]\n",
    "        print(f\"Found {len(pre_duplicates)} pre-enrollment emails with duplicates\")\n",
    "        report_data.append({\"Category\": \"Duplicate Analysis\", \n",
    "                           \"Metric\": \"Pre-enrollment Emails with Duplicates\", \n",
    "                           \"Value\": len(pre_duplicates)})\n",
    "        \n",
    "        if len(pre_duplicates) > 0:\n",
    "            print(\"Top 10 pre-enrollment duplicated emails:\")\n",
    "            for email, count in pre_duplicates.head(10).items():\n",
    "                if pd.notna(email):\n",
    "                    print(f\"  {email}: {count} occurrences\")\n",
    "            \n",
    "            # Save duplicates to file\n",
    "            pre_dup_df = pd.DataFrame({'Email': pre_duplicates.index, 'Count': pre_duplicates.values})\n",
    "            pre_dup_df.to_excel(os.path.join(analysis_dir, \"pre_duplicate_emails.xlsx\"), index=False)\n",
    "    \n",
    "    if post_email_col in merged_df.columns:\n",
    "        post_email_counts = merged_df[post_email_col].value_counts()\n",
    "        post_duplicates = post_email_counts[post_email_counts > 1]\n",
    "        print(f\"Found {len(post_duplicates)} post-course emails with duplicates\")\n",
    "        report_data.append({\"Category\": \"Duplicate Analysis\", \n",
    "                           \"Metric\": \"Post-course Emails with Duplicates\", \n",
    "                           \"Value\": len(post_duplicates)})\n",
    "        \n",
    "        if len(post_duplicates) > 0:\n",
    "            print(\"Top 10 post-course duplicated emails:\")\n",
    "            for email, count in post_duplicates.head(10).items():\n",
    "                if pd.notna(email):\n",
    "                    print(f\"  {email}: {count} occurrences\")\n",
    "            \n",
    "            # Save duplicates to file\n",
    "            post_dup_df = pd.DataFrame({'Email': post_duplicates.index, 'Count': post_duplicates.values})\n",
    "            post_dup_df.to_excel(os.path.join(analysis_dir, \"post_duplicate_emails.xlsx\"), index=False)\n",
    "    \n",
    "    # Analyze email format inconsistencies\n",
    "    if pre_email_col in merged_df.columns and post_email_col in merged_df.columns:\n",
    "        print(\"\\nAnalyzing email format consistency between pre and post...\")\n",
    "        \n",
    "        # Define a function to extract domain from email\n",
    "        def extract_domain(email):\n",
    "            if pd.isna(email):\n",
    "                return None\n",
    "            try:\n",
    "                return str(email).lower().split('@')[1]\n",
    "            except (IndexError, AttributeError):\n",
    "                return None\n",
    "        \n",
    "        # Add domain columns\n",
    "        merged_df['pre_email_domain'] = merged_df[pre_email_col].apply(extract_domain)\n",
    "        merged_df['post_email_domain'] = merged_df[post_email_col].apply(extract_domain)\n",
    "        \n",
    "        # Count records where domains don't match\n",
    "        domain_mismatch = merged_df[(merged_df['pre_email_domain'].notna()) & \n",
    "                                    (merged_df['post_email_domain'].notna()) & \n",
    "                                    (merged_df['pre_email_domain'] != merged_df['post_email_domain'])]\n",
    "        \n",
    "        print(f\"Found {len(domain_mismatch)} records where pre and post email domains don't match\")\n",
    "        report_data.append({\"Category\": \"Email Consistency\", \n",
    "                           \"Metric\": \"Records with Different Email Domains\", \n",
    "                           \"Value\": len(domain_mismatch)})\n",
    "        \n",
    "        if len(domain_mismatch) > 0:\n",
    "            print(\"Sample of domain mismatches:\")\n",
    "            sample = domain_mismatch.head(10)\n",
    "            for _, row in sample.iterrows():\n",
    "                print(f\"  {row[pre_email_col]} -> {row[post_email_col]}\")\n",
    "            \n",
    "            # Save domain mismatches to file\n",
    "            domain_mismatch[['pre_email_domain', 'post_email_domain', pre_email_col, post_email_col]].to_excel(\n",
    "                os.path.join(analysis_dir, \"email_domain_mismatches.xlsx\"), index=False)\n",
    "    \n",
    "    # 4. Name and DOB Consistency Analysis\n",
    "    print(\"\\n4. Name and DOB Consistency Analysis\")\n",
    "    \n",
    "    # Check name consistency\n",
    "    if 'pre_full_name' in merged_df.columns and 'post_full_name' in merged_df.columns:\n",
    "        # Normalize names for comparison\n",
    "        def normalize_name(name):\n",
    "            if pd.isna(name):\n",
    "                return None\n",
    "            return re.sub(r'[^a-zA-Z ]', '', str(name).lower()).strip()\n",
    "        \n",
    "        merged_df['pre_name_norm'] = merged_df['pre_full_name'].apply(normalize_name)\n",
    "        merged_df['post_name_norm'] = merged_df['post_full_name'].apply(normalize_name)\n",
    "        \n",
    "        # Count exact matches and near matches\n",
    "        exact_name_matches = sum(merged_df['pre_name_norm'] == merged_df['post_name_norm'])\n",
    "        exact_match_pct = (exact_name_matches / len(merged_df)) * 100\n",
    "        \n",
    "        print(f\"Exact name matches: {exact_name_matches} ({exact_match_pct:.2f}%)\")\n",
    "        report_data.append({\"Category\": \"Name Consistency\", \n",
    "                           \"Metric\": \"Exact Name Matches\", \n",
    "                           \"Value\": f\"{exact_name_matches} ({exact_match_pct:.2f}%)\"})\n",
    "        \n",
    "        # Analyze name differences\n",
    "        name_diff_df = merged_df[merged_df['pre_name_norm'] != merged_df['post_name_norm']].copy()\n",
    "        \n",
    "        if len(name_diff_df) > 0:\n",
    "            print(f\"Found {len(name_diff_df)} records with name differences\")\n",
    "            \n",
    "            # Categorize name differences\n",
    "            def categorize_name_diff(pre, post):\n",
    "                if pd.isna(pre) or pd.isna(post):\n",
    "                    return \"Missing Name\"\n",
    "                \n",
    "                pre_parts = pre.split()\n",
    "                post_parts = post.split()\n",
    "                \n",
    "                if len(pre_parts) != len(post_parts):\n",
    "                    return \"Different Number of Name Parts\"\n",
    "                \n",
    "                if len(pre_parts) == 1 or len(post_parts) == 1:\n",
    "                    return \"Single Name Part\"\n",
    "                \n",
    "                # Check for first name match only\n",
    "                if pre_parts[0] == post_parts[0] and pre_parts[1:] != post_parts[1:]:\n",
    "                    return \"First Name Match Only\"\n",
    "                \n",
    "                # Check for last name match only\n",
    "                if pre_parts[-1] == post_parts[-1] and pre_parts[:-1] != post_parts[:-1]:\n",
    "                    return \"Last Name Match Only\"\n",
    "                \n",
    "                # Check for swapped first/last\n",
    "                if len(pre_parts) >= 2 and len(post_parts) >= 2:\n",
    "                    if pre_parts[0] == post_parts[-1] and pre_parts[-1] == post_parts[0]:\n",
    "                        return \"First/Last Swapped\"\n",
    "                \n",
    "                return \"Other Difference\"\n",
    "            \n",
    "            name_diff_df['name_diff_type'] = name_diff_df.apply(\n",
    "                lambda row: categorize_name_diff(row['pre_name_norm'], row['post_name_norm']), axis=1)\n",
    "            \n",
    "            diff_counts = name_diff_df['name_diff_type'].value_counts()\n",
    "            print(\"Name difference categories:\")\n",
    "            for diff_type, count in diff_counts.items():\n",
    "                pct = (count / len(name_diff_df)) * 100\n",
    "                print(f\"  {diff_type}: {count} ({pct:.2f}%)\")\n",
    "                report_data.append({\"Category\": \"Name Differences\", \n",
    "                                   \"Metric\": diff_type, \n",
    "                                   \"Value\": f\"{count} ({pct:.2f}%)\"})\n",
    "            \n",
    "            # Save name differences to file\n",
    "            name_cols = [col for col in merged_df.columns if 'name' in col.lower()]\n",
    "            name_diff_df[name_cols + ['name_diff_type']].to_excel(\n",
    "                os.path.join(analysis_dir, \"name_differences.xlsx\"), index=False)\n",
    "    \n",
    "    # Check DOB consistency\n",
    "    if 'pre_date_of_birth' in merged_df.columns and 'post_date_of_birth' in merged_df.columns:\n",
    "        # Convert dates to string format for comparison\n",
    "        merged_df['pre_dob_str'] = merged_df['pre_date_of_birth'].astype(str)\n",
    "        merged_df['post_dob_str'] = merged_df['post_date_of_birth'].astype(str)\n",
    "        \n",
    "        # Count exact DOB matches\n",
    "        exact_dob_matches = sum(merged_df['pre_dob_str'] == merged_df['post_dob_str'])\n",
    "        exact_dob_pct = (exact_dob_matches / len(merged_df)) * 100\n",
    "        \n",
    "        print(f\"Exact DOB matches: {exact_dob_matches} ({exact_dob_pct:.2f}%)\")\n",
    "        report_data.append({\"Category\": \"DOB Consistency\", \n",
    "                           \"Metric\": \"Exact DOB Matches\", \n",
    "                           \"Value\": f\"{exact_dob_matches} ({exact_dob_pct:.2f}%)\"})\n",
    "        \n",
    "        # Analyze DOB differences\n",
    "        dob_diff_df = merged_df[merged_df['pre_dob_str'] != merged_df['post_dob_str']].copy()\n",
    "        \n",
    "        if len(dob_diff_df) > 0:\n",
    "            print(f\"Found {len(dob_diff_df)} records with DOB differences\")\n",
    "            \n",
    "            # Try to parse DOBs to detect patterns\n",
    "            def parse_dob(dob_str):\n",
    "                if pd.isna(dob_str):\n",
    "                    return None, None, None\n",
    "                \n",
    "                # Try multiple date formats\n",
    "                formats = ['%Y-%m-%d', '%m/%d/%Y', '%m-%d-%Y', '%d/%m/%Y', '%d-%m-%Y']\n",
    "                \n",
    "                for fmt in formats:\n",
    "                    try:\n",
    "                        dt = pd.to_datetime(dob_str, format=fmt)\n",
    "                        return dt.year, dt.month, dt.day\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                return None, None, None\n",
    "            \n",
    "            # Extract year, month, day components\n",
    "            dob_diff_df['pre_year'], dob_diff_df['pre_month'], dob_diff_df['pre_day'] = zip(\n",
    "                *dob_diff_df['pre_dob_str'].apply(parse_dob))\n",
    "            \n",
    "            dob_diff_df['post_year'], dob_diff_df['post_month'], dob_diff_df['post_day'] = zip(\n",
    "                *dob_diff_df['post_dob_str'].apply(parse_dob))\n",
    "            \n",
    "            # Categorize differences\n",
    "            year_match = sum((dob_diff_df['pre_year'] == dob_diff_df['post_year']) & \n",
    "                             (dob_diff_df['pre_year'].notna()))\n",
    "            \n",
    "            month_day_swapped = sum((dob_diff_df['pre_month'] == dob_diff_df['post_day']) & \n",
    "                                   (dob_diff_df['pre_day'] == dob_diff_df['post_month']) &\n",
    "                                   (dob_diff_df['pre_month'].notna()))\n",
    "            \n",
    "            print(f\"  Same year, different month/day: {year_match} records\")\n",
    "            print(f\"  Month/day swapped: {month_day_swapped} records\")\n",
    "            \n",
    "            report_data.append({\"Category\": \"DOB Differences\", \n",
    "                               \"Metric\": \"Same Year, Different Month/Day\", \n",
    "                               \"Value\": year_match})\n",
    "            \n",
    "            report_data.append({\"Category\": \"DOB Differences\", \n",
    "                               \"Metric\": \"Month/Day Swapped\", \n",
    "                               \"Value\": month_day_swapped})\n",
    "            \n",
    "            # Save DOB differences to file\n",
    "            dob_cols = [col for col in merged_df.columns if 'birth' in col.lower() or 'dob' in col.lower()]\n",
    "            dob_analysis_cols = ['pre_dob_str', 'post_dob_str', 'pre_year', 'pre_month', 'pre_day',\n",
    "                               'post_year', 'post_month', 'post_day']\n",
    "            \n",
    "            dob_diff_df[dob_cols + dob_analysis_cols].to_excel(\n",
    "                os.path.join(analysis_dir, \"dob_differences.xlsx\"), index=False)\n",
    "    \n",
    "    # 5. Match Quality Analysis - Confidence Scoring\n",
    "    print(\"\\n5. Match Quality Analysis\")\n",
    "    \n",
    "    # Create a confidence score for each match\n",
    "    def calculate_match_confidence(row):\n",
    "        score = 0\n",
    "        max_score = 0\n",
    "        \n",
    "        # Email match (highest confidence)\n",
    "        if pre_email_col in merged_df.columns and post_email_col in merged_df.columns:\n",
    "            max_score += 5\n",
    "            if pd.notna(row[pre_email_col]) and pd.notna(row[post_email_col]):\n",
    "                pre_email = str(row[pre_email_col]).lower().strip()\n",
    "                post_email = str(row[post_email_col]).lower().strip()\n",
    "                \n",
    "                if pre_email == post_email:\n",
    "                    score += 5  # Exact match\n",
    "                elif pre_email.split('@')[0] == post_email.split('@')[0]:\n",
    "                    score += 4  # Username matches but different domain\n",
    "        \n",
    "        # Name match\n",
    "        if 'pre_name_norm' in merged_df.columns and 'post_name_norm' in merged_df.columns:\n",
    "            max_score += 3\n",
    "            if pd.notna(row['pre_name_norm']) and pd.notna(row['post_name_norm']):\n",
    "                if row['pre_name_norm'] == row['post_name_norm']:\n",
    "                    score += 3  # Exact match\n",
    "                elif 'pre_first_name' in merged_df.columns and 'post_first_name' in merged_df.columns:\n",
    "                    # First name match\n",
    "                    pre_first = normalize_name(row['pre_first_name'])\n",
    "                    post_first = normalize_name(row['post_first_name'])\n",
    "                    if pd.notna(pre_first) and pd.notna(post_first) and pre_first == post_first:\n",
    "                        score += 1.5\n",
    "                    \n",
    "                    # Last name match\n",
    "                    if 'pre_last_name' in merged_df.columns and 'post_last_name' in merged_df.columns:\n",
    "                        pre_last = normalize_name(row['pre_last_name'])\n",
    "                        post_last = normalize_name(row['post_last_name'])\n",
    "                        if pd.notna(pre_last) and pd.notna(post_last) and pre_last == post_last:\n",
    "                            score += 1.5\n",
    "        \n",
    "        # DOB match\n",
    "        if 'pre_dob_str' in merged_df.columns and 'post_dob_str' in merged_df.columns:\n",
    "            max_score += 3\n",
    "            if pd.notna(row['pre_dob_str']) and pd.notna(row['post_dob_str']):\n",
    "                if row['pre_dob_str'] == row['post_dob_str']:\n",
    "                    score += 3  # Exact match\n",
    "                elif hasattr(row, 'pre_year') and hasattr(row, 'post_year'):\n",
    "                    # Year match\n",
    "                    if pd.notna(row['pre_year']) and pd.notna(row['post_year']) and row['pre_year'] == row['post_year']:\n",
    "                        score += 1.5\n",
    "                    \n",
    "                    # Month match\n",
    "                    if pd.notna(row['pre_month']) and pd.notna(row['post_month']) and row['pre_month'] == row['post_month']:\n",
    "                        score += 0.75\n",
    "                    \n",
    "                    # Day match\n",
    "                    if pd.notna(row['pre_day']) and pd.notna(row['post_day']) and row['pre_day'] == row['post_day']:\n",
    "                        score += 0.75\n",
    "        \n",
    "        # Calculate percentage\n",
    "        confidence = (score / max_score * 100) if max_score > 0 else 0\n",
    "        \n",
    "        # Round to 2 decimal places\n",
    "        return round(confidence, 2)\n",
    "    \n",
    "    # Add confidence score\n",
    "    merged_df['match_confidence'] = merged_df.apply(calculate_match_confidence, axis=1)\n",
    "    \n",
    "    # Analyze confidence distribution\n",
    "    confidence_bins = [0, 50, 70, 80, 90, 95, 100]\n",
    "    merged_df['confidence_category'] = pd.cut(merged_df['match_confidence'], confidence_bins, \n",
    "                                             labels=['Very Low', 'Low', 'Moderate', 'Good', 'High', 'Perfect'])\n",
    "    \n",
    "    conf_counts = merged_df['confidence_category'].value_counts().sort_index()\n",
    "    \n",
    "    print(\"Match confidence distribution:\")\n",
    "    for category, count in conf_counts.items():\n",
    "        pct = (count / len(merged_df)) * 100\n",
    "        print(f\"  {category}: {count} ({pct:.2f}%)\")\n",
    "        report_data.append({\"Category\": \"Match Confidence\", \n",
    "                           \"Metric\": str(category), \n",
    "                           \"Value\": f\"{count} ({pct:.2f}%)\"})\n",
    "    \n",
    "    # Export low confidence matches for review\n",
    "    low_conf = merged_df[merged_df['match_confidence'] < 70].copy()\n",
    "    if len(low_conf) > 0:\n",
    "        print(f\"Found {len(low_conf)} potentially problematic matches with confidence < 70%\")\n",
    "        report_data.append({\"Category\": \"Data Quality Concerns\", \n",
    "                           \"Metric\": \"Low Confidence Matches (<70%)\", \n",
    "                           \"Value\": len(low_conf)})\n",
    "        \n",
    "        # Select relevant columns for review\n",
    "        review_cols = [\n",
    "            pre_email_col, post_email_col,\n",
    "            'pre_full_name', 'post_full_name',\n",
    "            'pre_first_name', 'post_first_name',\n",
    "            'pre_last_name', 'post_last_name',\n",
    "            'pre_date_of_birth', 'post_date_of_birth',\n",
    "            'match_confidence', 'confidence_category'\n",
    "        ]\n",
    "        \n",
    "        # Filter to only include columns that exist\n",
    "        review_cols = [col for col in review_cols if col in merged_df.columns]\n",
    "        \n",
    "        # Add match type if it exists\n",
    "        if 'match_type' in merged_df.columns:\n",
    "            review_cols.append('match_type')\n",
    "        elif 'inferred_match_type' in merged_df.columns:\n",
    "            review_cols.append('inferred_match_type')\n",
    "        \n",
    "        low_conf[review_cols].to_excel(\n",
    "            os.path.join(analysis_dir, \"low_confidence_matches.xlsx\"), index=False)\n",
    "    \n",
    "    # 6. Additional Data Quality Checks\n",
    "    print(\"\\n6. Additional Data Quality Checks\")\n",
    "    \n",
    "    # Check for outliers in age_at_course\n",
    "    if 'age_at_course' in merged_df.columns:\n",
    "        age_stats = merged_df['age_at_course'].describe()\n",
    "        \n",
    "        print(\"Age at course statistics:\")\n",
    "        print(f\"  Min: {age_stats['min']}\")\n",
    "        print(f\"  Max: {age_stats['max']}\")\n",
    "        print(f\"  Mean: {age_stats['mean']:.1f}\")\n",
    "        \n",
    "        # Identify potentially incorrect ages\n",
    "        young_outliers = merged_df[merged_df['age_at_course'] < 5]\n",
    "        old_outliers = merged_df[merged_df['age_at_course'] > 90]\n",
    "        \n",
    "        if len(young_outliers) > 0:\n",
    "            print(f\"  Found {len(young_outliers)} records with age < 5\")\n",
    "            report_data.append({\"Category\": \"Data Quality Concerns\", \n",
    "                               \"Metric\": \"Very Young Age (<5)\", \n",
    "                               \"Value\": len(young_outliers)})\n",
    "        \n",
    "        if len(old_outliers) > 0:\n",
    "            print(f\"  Found {len(old_outliers)} records with age > 90\")\n",
    "            report_data.append({\"Category\": \"Data Quality Concerns\", \n",
    "                               \"Metric\": \"Very Old Age (>90)\", \n",
    "                               \"Value\": len(old_outliers)})\n",
    "        \n",
    "        # Save age outliers to file\n",
    "        if len(young_outliers) > 0 or len(old_outliers) > 0:\n",
    "            age_outliers = pd.concat([young_outliers, old_outliers])\n",
    "            \n",
    "            age_cols = ['pre_full_name', 'post_full_name', 'pre_date_of_birth', 'post_date_of_birth', \n",
    "                      'age_at_course', 'pre_course_year', 'post_course_year']\n",
    "            \n",
    "            age_cols = [col for col in age_cols if col in merged_df.columns]\n",
    "            \n",
    "            age_outliers[age_cols].to_excel(\n",
    "                os.path.join(analysis_dir, \"age_outliers.xlsx\"), index=False)\n",
    "    \n",
    "    # Check for inconsistencies in course years\n",
    "    if 'pre_course_year' in merged_df.columns and 'post_course_year' in merged_df.columns:\n",
    "        year_diff = merged_df['post_course_year'] - merged_df['pre_course_year']\n",
    "        \n",
    "        future_pre = merged_df[year_diff < 0]\n",
    "        if len(future_pre) > 0:\n",
    "            print(f\"Found {len(future_pre)} records where pre-course year is after post-course year\")\n",
    "            report_data.append({\"Category\": \"Data Quality Concerns\", \n",
    "                               \"Metric\": \"Pre-course Year After Post-course Year\", \n",
    "                               \"Value\": len(future_pre)})\n",
    "            \n",
    "            year_cols = ['pre_course_year', 'post_course_year', 'pre_course_date', 'post_course_date']\n",
    "            year_cols = [col for col in year_cols if col in merged_df.columns]\n",
    "            \n",
    "            future_pre[year_cols].to_excel(\n",
    "                os.path.join(analysis_dir, \"year_inconsistencies.xlsx\"), index=False)\n",
    "    \n",
    "    # 7. Generate Summary Report\n",
    "    print(\"\\n7. Generating Summary Report\")\n",
    "    \n",
    "    # Create a DataFrame of all reported metrics\n",
    "    report_df = pd.DataFrame(report_data)\n",
    "    \n",
    "    # Save the full report\n",
    "    report_df.to_excel(os.path.join(analysis_dir, \"data_quality_report.xlsx\"), index=False)\n",
    "    \n",
    "    print(f\"Data quality analysis complete. Reports saved to: {analysis_dir}\")\n",
    "    \n",
    "    # Return dataset with quality analysis columns\n",
    "    return merged_df\n",
    "\n",
    "# Run the analysis with your file\n",
    "merged_file_path = '/Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/ES/Comprehensive_Merged_Surveys.xlsx'\n",
    "analyzed_df = analyze_data_quality(merged_file_path)\n",
    "def visualize_data_quality(analyzed_df, analysis_dir):\n",
    "    \"\"\"\n",
    "    Generate visualizations of data quality metrics\n",
    "    \n",
    "    Parameters:\n",
    "    analyzed_df (DataFrame): The dataset with quality analysis columns added\n",
    "    analysis_dir (str): Directory to save visualizations\n",
    "    \"\"\"\n",
    "    print(\"Generating data quality visualizations...\")\n",
    "    \n",
    "    # Create visualizations directory\n",
    "    viz_dir = os.path.join(analysis_dir, \"Visualizations\")\n",
    "    os.makedirs(viz_dir, exist_ok=True)\n",
    "    \n",
    "    # Set style\n",
    "    plt.style.use('ggplot')\n",
    "    \n",
    "    # 1. Match Type Distribution\n",
    "    if 'match_type' in analyzed_df.columns:\n",
    "        match_type_col = 'match_type'\n",
    "    elif 'inferred_match_type' in analyzed_df.columns:\n",
    "        match_type_col = 'inferred_match_type'\n",
    "    else:\n",
    "        match_type_col = None\n",
    "    \n",
    "    if match_type_col:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        match_counts = analyzed_df[match_type_col].value_counts()\n",
    "        \n",
    "        # Plot\n",
    "        ax = sns.barplot(x=match_counts.index, y=match_counts.values)\n",
    "        \n",
    "        # Add count labels\n",
    "        for i, count in enumerate(match_counts.values):\n",
    "            ax.text(i, count + 50, f\"{count}\", ha='center')\n",
    "        \n",
    "        plt.title('Distribution of Match Types', fontsize=15)\n",
    "        plt.xlabel('Match Type', fontsize=12)\n",
    "        plt.ylabel('Count', fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(viz_dir, \"match_type_distribution.png\"), dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    # 2. Match Confidence Distribution\n",
    "    if 'confidence_category' in analyzed_df.columns:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        conf_counts = analyzed_df['confidence_category'].value_counts().sort_index()\n",
    "        \n",
    "        # Custom color map based on confidence level\n",
    "        colors = ['#d53e4f', '#fc8d59', '#fee08b', '#e6f598', '#99d594', '#3288bd']\n",
    "        \n",
    "        # Plot\n",
    "        ax = sns.barplot(x=conf_counts.index, y=conf_counts.values, palette=colors)\n",
    "        \n",
    "        # Add count labels\n",
    "        for i, count in enumerate(conf_counts.values):\n",
    "            ax.text(i, count + 50, f\"{count}\", ha='center')\n",
    "        \n",
    "        plt.title('Distribution of Match Confidence', fontsize=15)\n",
    "        plt.xlabel('Confidence Level', fontsize=12)\n",
    "        plt.ylabel('Count', fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(viz_dir, \"match_confidence_distribution.png\"), dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    # 3. Email Consistency Analysis\n",
    "    email_consistency_data = []\n",
    "    \n",
    "    # Pre-enrollment email duplicates\n",
    "    pre_email_col = [col for col in analyzed_df.columns if 'pre' in col.lower() and 'email' in col.lower() and 'address' in col.lower()]\n",
    "    if pre_email_col:\n",
    "        pre_email_col = pre_email_col[0]\n",
    "        pre_dup_count = sum(analyzed_df[pre_email_col].duplicated())\n",
    "        email_consistency_data.append({'Category': 'Pre-enrollment Email Duplicates', 'Count': pre_dup_count})\n",
    "    \n",
    "    # Post-course email duplicates\n",
    "    post_email_col = [col for col in analyzed_df.columns if 'post' in col.lower() and 'email' in col.lower() and 'address' in col.lower()]\n",
    "    if post_email_col:\n",
    "        post_email_col = post_email_col[0]\n",
    "        post_dup_count = sum(analyzed_df[post_email_col].duplicated())\n",
    "        email_consistency_data.append({'Category': 'Post-course Email Duplicates', 'Count': post_dup_count})\n",
    "    \n",
    "    # Domain mismatches\n",
    "    if 'pre_email_domain' in analyzed_df.columns and 'post_email_domain' in analyzed_df.columns:\n",
    "        domain_mismatch_count = sum((analyzed_df['pre_email_domain'].notna()) & \n",
    "                                   (analyzed_df['post_email_domain'].notna()) & \n",
    "                                   (analyzed_df['pre_email_domain'] != analyzed_df['post_email_domain']))\n",
    "        email_consistency_data.append({'Category': 'Email Domain Mismatches', 'Count': domain_mismatch_count})\n",
    "    \n",
    "    if email_consistency_data:\n",
    "        email_df = pd.DataFrame(email_consistency_data)\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        ax = sns.barplot(x='Category', y='Count', data=email_df)\n",
    "        \n",
    "        # Add count labels\n",
    "        for i, count in enumerate(email_df['Count']):\n",
    "            ax.text(i, count + 5, f\"{count}\", ha='center')\n",
    "        \n",
    "        plt.title('Email Consistency Issues', fontsize=15)\n",
    "        plt.xlabel('Issue Type', fontsize=12)\n",
    "        plt.ylabel('Count', fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(viz_dir, \"email_consistency_issues.png\"), dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    # 4. Name and DOB Consistency\n",
    "    name_dob_data = []\n",
    "    \n",
    "    # Name consistency\n",
    "    if 'pre_name_norm' in analyzed_df.columns and 'post_name_norm' in analyzed_df.columns:\n",
    "        exact_name_matches = sum(analyzed_df['pre_name_norm'] == analyzed_df['post_name_norm'])\n",
    "        name_mismatches = len(analyzed_df) - exact_name_matches\n",
    "        \n",
    "        name_dob_data.append({'Category': 'Exact Name Matches', 'Count': exact_name_matches})\n",
    "        name_dob_data.append({'Category': 'Name Mismatches', 'Count': name_mismatches})\n",
    "    \n",
    "    # DOB consistency\n",
    "    if 'pre_dob_str' in analyzed_df.columns and 'post_dob_str' in analyzed_df.columns:\n",
    "        exact_dob_matches = sum(analyzed_df['pre_dob_str'] == analyzed_df['post_dob_str'])\n",
    "        dob_mismatches = len(analyzed_df) - exact_dob_matches\n",
    "        \n",
    "        name_dob_data.append({'Category': 'Exact DOB Matches', 'Count': exact_dob_matches})\n",
    "        name_dob_data.append({'Category': 'DOB Mismatches', 'Count': dob_mismatches})\n",
    "    \n",
    "    if name_dob_data:\n",
    "        name_dob_df = pd.DataFrame(name_dob_data)\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        ax = sns.barplot(x='Category', y='Count', data=name_dob_df)\n",
    "        \n",
    "        # Add count labels\n",
    "        for i, count in enumerate(name_dob_df['Count']):\n",
    "            ax.text(i, count + 100, f\"{count}\", ha='center')\n",
    "        \n",
    "        plt.title('Name and DOB Consistency', fontsize=15)\n",
    "        plt.xlabel('Category', fontsize=12)\n",
    "        plt.ylabel('Count', fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(viz_dir, \"name_dob_consistency.png\"), dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    # 5. Data Quality Concerns\n",
    "    quality_concerns = []\n",
    "    \n",
    "    # Low confidence matches\n",
    "    if 'match_confidence' in analyzed_df.columns:\n",
    "        low_conf_count = sum(analyzed_df['match_confidence'] < 70)\n",
    "        quality_concerns.append({'Concern': 'Low Confidence Matches', 'Count': low_conf_count})\n",
    "    \n",
    "    # Age outliers\n",
    "    if 'age_at_course' in analyzed_df.columns:\n",
    "        young_outliers = sum(analyzed_df['age_at_course'] < 5)\n",
    "        old_outliers = sum(analyzed_df['age_at_course'] > 90)\n",
    "        \n",
    "        if young_outliers > 0:\n",
    "            quality_concerns.append({'Concern': 'Very Young Age (<5)', 'Count': young_outliers})\n",
    "        \n",
    "        if old_outliers > 0:\n",
    "            quality_concerns.append({'Concern': 'Very Old Age (>90)', 'Count': old_outliers})\n",
    "    \n",
    "    # Course year inconsistencies\n",
    "    if 'pre_course_year' in analyzed_df.columns and 'post_course_year' in analyzed_df.columns:\n",
    "        year_issues = sum(analyzed_df['post_course_year'] < analyzed_df['pre_course_year'])\n",
    "        if year_issues > 0:\n",
    "            quality_concerns.append({'Concern': 'Year Inconsistencies', 'Count': year_issues})\n",
    "    \n",
    "    if quality_concerns:\n",
    "        concerns_df = pd.DataFrame(quality_concerns)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        ax = sns.barplot(x='Concern', y='Count', data=concerns_df, color='tomato')\n",
    "        \n",
    "        # Add count labels\n",
    "        for i, count in enumerate(concerns_df['Count']):\n",
    "            ax.text(i, count + 5, f\"{count}\", ha='center')\n",
    "        \n",
    "        plt.title('Data Quality Concerns', fontsize=15)\n",
    "        plt.xlabel('Issue Type', fontsize=12)\n",
    "        plt.ylabel('Count', fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(viz_dir, \"data_quality_concerns.png\"), dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    print(f\"Visualizations saved to: {viz_dir}\")\n",
    "\n",
    "# Update the analyze_data_quality function to call the visualization function\n",
    "def analyze_data_quality(merged_file_path):\n",
    "    # ... [all existing code] ...\n",
    "    \n",
    "    # At the end, add:\n",
    "    # Generate visualizations\n",
    "    visualize_data_quality(merged_df, analysis_dir)\n",
    "    \n",
    "    print(f\"Data quality analysis complete. Reports and visualizations saved to: {analysis_dir}\")\n",
    "    \n",
    "    # Return dataset with quality analysis columns\n",
    "    return merged_df\n",
    "def generate_recommendations(analyzed_df, analysis_dir):\n",
    "    \"\"\"\n",
    "    Generate recommendations for data quality improvements\n",
    "    \n",
    "    Parameters:\n",
    "    analyzed_df (DataFrame): The dataset with quality analysis columns added\n",
    "    analysis_dir (str): Directory to save recommendations\n",
    "    \"\"\"\n",
    "    print(\"\\nGenerating recommendations based on data quality analysis...\")\n",
    "    \n",
    "    recommendations = []\n",
    "    \n",
    "    # 1. Low confidence matches\n",
    "    if 'match_confidence' in analyzed_df.columns:\n",
    "        low_conf_count = sum(analyzed_df['match_confidence'] < 70)\n",
    "        if low_conf_count > 0:\n",
    "            low_conf_pct = (low_conf_count / len(analyzed_df)) * 100\n",
    "            \n",
    "            if low_conf_pct > 20:\n",
    "                recommendations.append({\n",
    "                    'Issue': 'High percentage of low confidence matches',\n",
    "                    'Impact': 'HIGH',\n",
    "                    'Recommendation': 'Consider reviewing and potentially excluding matches with confidence < 70%. These matches could significantly affect data analysis reliability.',\n",
    "                    'Action Items': [\n",
    "                        'Review low_confidence_matches.xlsx from the analysis',\n",
    "                        'Consider creating a filtered dataset that excludes matches below a threshold',\n",
    "                        'If keeping low confidence matches, flag them in analyses'\n",
    "                    ]\n",
    "                })\n",
    "            else:\n",
    "                recommendations.append({\n",
    "                    'Issue': 'Some low confidence matches detected',\n",
    "                    'Impact': 'MEDIUM',\n",
    "                    'Recommendation': 'Review low confidence matches but they may not significantly impact overall analysis.',\n",
    "                    'Action Items': [\n",
    "                        'Review low_confidence_matches.xlsx from the analysis',\n",
    "                        'Consider flagging these records in your analyses'\n",
    "                    ]\n",
    "                })\n",
    "    \n",
    "    # 2. Email duplicates\n",
    "    pre_email_col = [col for col in analyzed_df.columns if 'pre' in col.lower() and 'email' in col.lower() and 'address' in col.lower()]\n",
    "    if pre_email_col:\n",
    "        pre_email_col = pre_email_col[0]\n",
    "        pre_dup_count = sum(analyzed_df[pre_email_col].duplicated())\n",
    "        \n",
    "        if pre_dup_count > 0:\n",
    "            pre_dup_pct = (pre_dup_count / len(analyzed_df)) * 100\n",
    "            \n",
    "            if pre_dup_pct > 10:\n",
    "                recommendations.append({\n",
    "                    'Issue': 'Significant number of duplicate pre-enrollment emails',\n",
    "                    'Impact': 'HIGH',\n",
    "                    'Recommendation': 'Duplicate emails suggest the same person may appear multiple times in the dataset.',\n",
    "                    'Action Items': [\n",
    "                        'Review pre_duplicate_emails.xlsx from the analysis',\n",
    "                        'Consider consolidating records for the same person',\n",
    "                        'Investigate why duplicates exist (multiple enrollments? data entry errors?)'\n",
    "                    ]\n",
    "                })\n",
    "            else:\n",
    "                recommendations.append({\n",
    "                    'Issue': 'Some duplicate pre-enrollment emails',\n",
    "                    'Impact': 'MEDIUM',\n",
    "                    'Recommendation': 'Some duplication exists but likely won\\'t significantly impact analysis.',\n",
    "                    'Action Items': [\n",
    "                        'Review pre_duplicate_emails.xlsx from the analysis',\n",
    "                        'Flag duplicate records in your analyses'\n",
    "                    ]\n",
    "                })\n",
    "    \n",
    "    # 3. Name and DOB inconsistencies\n",
    "    if 'pre_name_norm' in analyzed_df.columns and 'post_name_norm' in analyzed_df.columns:\n",
    "        name_mismatch_count = sum(analyzed_df['pre_name_norm'] != analyzed_df['post_name_norm'])\n",
    "        name_mismatch_pct = (name_mismatch_count / len(analyzed_df)) * 100\n",
    "        \n",
    "        if name_mismatch_pct > 30:\n",
    "            recommendations.append({\n",
    "                'Issue': 'High percentage of name mismatches',\n",
    "                'Impact': 'HIGH',\n",
    "                'Recommendation': 'Many records show different names between pre and post surveys, raising concerns about match accuracy.',\n",
    "                'Action Items': [\n",
    "                    'Review name_differences.xlsx from the analysis',\n",
    "                    'Consider prioritizing email matches over name-based matches',\n",
    "                    'Investigate patterns in name differences (nicknames, maiden/married names, etc.)'\n",
    "                ]\n",
    "            })\n",
    "        elif name_mismatch_pct > 10:\n",
    "            recommendations.append({\n",
    "                'Issue': 'Moderate percentage of name mismatches',\n",
    "                'Impact': 'MEDIUM',\n",
    "                'Recommendation': 'Some name inconsistencies exist and should be reviewed.',\n",
    "                'Action Items': [\n",
    "                    'Review name_differences.xlsx from the analysis',\n",
    "                    'Consider if differences follow patterns that could be standardized'\n",
    "                ]\n",
    "            })\n",
    "    \n",
    "    # 4. Overall data quality recommendation\n",
    "    if 'match_confidence' in analyzed_df.columns:\n",
    "        high_conf_count = sum(analyzed_df['match_confidence'] >= 90)\n",
    "        high_conf_pct = (high_conf_count / len(analyzed_df)) * 100\n",
    "        \n",
    "        if high_conf_pct > 80:\n",
    "            recommendations.append({\n",
    "                'Issue': 'Overall data quality',\n",
    "                'Impact': 'POSITIVE',\n",
    "                'Recommendation': 'Data quality is generally good with most matches having high confidence.',\n",
    "                'Action Items': [\n",
    "                    'Proceed with analysis using the current merged dataset',\n",
    "                    'Consider excluding or flagging only the most problematic matches'\n",
    "                ]\n",
    "            })\n",
    "        elif high_conf_pct > 50:\n",
    "            recommendations.append({\n",
    "                'Issue': 'Overall data quality',\n",
    "                'Impact': 'NEUTRAL',\n",
    "                'Recommendation': 'Data quality is acceptable but has room for improvement.',\n",
    "                'Action Items': [\n",
    "                    'Consider creating a \"gold standard\" subset of high-confidence matches for key analyses',\n",
    "                    'Use the full dataset for less sensitive analyses'\n",
    "                ]\n",
    "            })\n",
    "        else:\n",
    "            recommendations.append({\n",
    "                'Issue': 'Overall data quality',\n",
    "                'Impact': 'CONCERN',\n",
    "                'Recommendation': 'Data quality issues may significantly impact analysis reliability.',\n",
    "                'Action Items': [\n",
    "                    'Consider redoing the matching process with stricter criteria',\n",
    "                    'For now, work primarily with high-confidence matches only',\n",
    "                    'Flag data quality issues in any reports or analyses'\n",
    "                ]\n",
    "            })\n",
    "    \n",
    "    # Save recommendations to Excel\n",
    "    recommendations_df = pd.DataFrame(recommendations)\n",
    "    recommendations_df.to_excel(os.path.join(analysis_dir, \"data_quality_recommendations.xlsx\"), index=False)\n",
    "    \n",
    "    # Also print summary of recommendations\n",
    "    print(\"\\nKey recommendations:\")\n",
    "    for i, rec in enumerate(recommendations, 1):\n",
    "        print(f\"{i}. {rec['Issue']} - Impact: {rec['Impact']}\")\n",
    "        print(f\"   {rec['Recommendation']}\")\n",
    "    \n",
    "    print(f\"\\nDetailed recommendations saved to: {os.path.join(analysis_dir, 'data_quality_recommendations.xlsx')}\")\n",
    "\n",
    "# Update analyze_data_quality to include recommendations\n",
    "def analyze_data_quality(merged_file_path):\n",
    "    # ... [all existing code] ...\n",
    "    \n",
    "    # At the end, add:\n",
    "    # Generate visualizations\n",
    "    visualize_data_quality(merged_df, analysis_dir)\n",
    "    \n",
    "    # Generate recommendations\n",
    "    generate_recommendations(merged_df, analysis_dir)\n",
    "    \n",
    "    print(f\"Data quality analysis complete. Reports, visualizations, and recommendations saved to: {analysis_dir}\")\n",
    "    \n",
    "    # Return dataset with quality analysis columns\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b486968c-15b7-4921-a0e2-2d4a3b1fe6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating high-quality merged dataset from original files\n",
      "\n",
      "1. Loading datasets\n",
      "Loading pre-enrollment data from: /Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/ES/Cleaned & Enhanced ES Data .xlsx\n",
      "Loaded 29700 pre-enrollment records\n",
      "Loading post-course data from: /Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/PCS/Cleaned_Merged_PCS_2cleaned.xlsx\n",
      "Loaded 7806 post-course records\n",
      "\n",
      "2. Standardizing and cleaning data\n",
      "Using pre-enrollment email column: email_address\n",
      "Using post-course email column: email_address\n",
      "Using pre-enrollment full name column: full_name\n",
      "Using post-course full name column: full_name\n",
      "Using pre-enrollment DOB column: None\n",
      "Using post-course DOB column: date_of_birth\n",
      "Standardized 5548 pre-enrollment emails\n",
      "Standardized 2395 post-course emails\n",
      "Normalized 29614 pre-enrollment names\n",
      "Normalized 7749 post-course names\n",
      "Standardized 7708 post-course DOBs\n",
      "\n",
      "3. Performing high-quality matching\n",
      "Matching by standardized email...\n",
      "Found 3241 matches by email\n",
      "Matching by full name + DOB...\n",
      "Found 0 additional matches by full name + DOB\n",
      "Total matches: 3241\n",
      "\n",
      "4. Creating merged dataset\n",
      "\n",
      "Warning: Found 5 pre-enrollment records matched to multiple post-course records\n",
      "Resolving by keeping only the highest quality match for each pre-enrollment record\n",
      "After resolving duplicates: 3236 matches\n",
      "\n",
      "5. Performing quality checks on merged dataset\n",
      "\n",
      "Found 44 records with pre-course year after post-course year\n",
      "Consider addressing these timeline issues separately as mentioned\n",
      "\n",
      "6. Saving high-quality merged dataset to /Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/ES/Direct_High_Quality_Merged_Dataset.xlsx\n",
      "\n",
      "High-quality merged dataset created successfully!\n",
      "Total records: 3236\n",
      "Match breakdown:\n",
      "  Email matches: 3236\n",
      "  Full name + DOB matches: 0\n"
     ]
    }
   ],
   "source": [
    "#High Quality Merged Data\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "def create_high_quality_merged_dataset(pre_course_path, post_course_path):\n",
    "    \"\"\"\n",
    "    Create a high-quality merged dataset directly from pre-enrollment and post-course data,\n",
    "    using strict matching criteria (email or full name + DOB).\n",
    "    \n",
    "    Parameters:\n",
    "    pre_course_path (str): Path to the pre-enrollment Excel file\n",
    "    post_course_path (str): Path to the post-course Excel file\n",
    "    \"\"\"\n",
    "    print(\"Creating high-quality merged dataset from original files\")\n",
    "    \n",
    "    # Step 1: Load the datasets\n",
    "    print(\"\\n1. Loading datasets\")\n",
    "    print(f\"Loading pre-enrollment data from: {pre_course_path}\")\n",
    "    pre_df = pd.read_excel(pre_course_path)\n",
    "    print(f\"Loaded {len(pre_df)} pre-enrollment records\")\n",
    "    \n",
    "    print(f\"Loading post-course data from: {post_course_path}\")\n",
    "    post_df = pd.read_excel(post_course_path)\n",
    "    print(f\"Loaded {len(post_df)} post-course records\")\n",
    "    \n",
    "    # Create output directory and file path\n",
    "    output_dir = os.path.dirname(pre_course_path)\n",
    "    output_file = os.path.join(output_dir, \"Direct_High_Quality_Merged_Dataset.xlsx\")\n",
    "    \n",
    "    # Step 2: Standardize and clean data for matching\n",
    "    print(\"\\n2. Standardizing and cleaning data\")\n",
    "    \n",
    "    # Domain corrections dictionary\n",
    "    domain_corrections = {\n",
    "        # Gmail variations\n",
    "        'gmai.com': 'gmail.com',\n",
    "        'gmil.com': 'gmail.com',\n",
    "        'gmal.com': 'gmail.com',\n",
    "        'gamail.com': 'gmail.com',\n",
    "        'gimail.com': 'gmail.com',\n",
    "        'gmaill.com': 'gmail.com',\n",
    "        'gmail.comm': 'gmail.com',\n",
    "        'gmial.com': 'gmail.com',\n",
    "        'gmail.cm': 'gmail.com',\n",
    "        'gmail.org': 'gmail.com',\n",
    "        'gemail.com': 'gmail.com',\n",
    "        'gmail.com.com': 'gmail.com',\n",
    "        'gmai.coml': 'gmail.com',\n",
    "        'gmail.coom': 'gmail.com',\n",
    "        'jmail.com': 'gmail.com',\n",
    "        'gmeil.com': 'gmail.com',\n",
    "        'gmall.com': 'gmail.com',\n",
    "        'gail.com': 'gmail.com',\n",
    "        'gmail.co': 'gmail.com',\n",
    "        'gmaiil.com': 'gmail.com',\n",
    "        'gmaim.com': 'gmail.com',\n",
    "        'gmail.om': 'gmail.com',\n",
    "        'gmail.ccom': 'gmail.com',\n",
    "        'g.com': 'gmail.com',\n",
    "        'gmail.vom': 'gmail.com',\n",
    "        'gmali.com': 'gmail.com',\n",
    "        'gmaio.com': 'gmail.com',\n",
    "        'g-mail.com': 'gmail.com',\n",
    "        'gamill.com': 'gmail.com',\n",
    "        'gmaol.com': 'gmail.com',\n",
    "        'gmsil.com': 'gmail.com',\n",
    "        'gmaii.com': 'gmail.com',\n",
    "        'gmail.oeg': 'gmail.com',\n",
    "        'gmanil.com': 'gmail.com',\n",
    "        'gmaili.com': 'gmail.com',\n",
    "        'gmail.crom': 'gmail.com',\n",
    "        'gmail.com11': 'gmail.com',\n",
    "        'gmill.com': 'gmail.com',\n",
    "        'gmail.com3': 'gmail.com',\n",
    "        'gnmail.com': 'gmail.com',\n",
    "        'gmaij.com': 'gmail.com',\n",
    "        'gmail.coml.com': 'gmail.com',\n",
    "        \n",
    "        # Yahoo variations\n",
    "        'yahoo.con': 'yahoo.com',\n",
    "        'yaoo.com': 'yahoo.com',\n",
    "        'hayoo.com': 'yahoo.com',\n",
    "        'yhoo.com': 'yahoo.com',\n",
    "        'yahoo.comm': 'yahoo.com',\n",
    "        'myyahoo.com': 'yahoo.com',\n",
    "        'yahoo.comb': 'yahoo.com',\n",
    "        'yahool.com': 'yahoo.com',\n",
    "        'yaho.cm': 'yahoo.com',\n",
    "        'yahio.un': 'yahoo.com',\n",
    "        \n",
    "        # Hotmail variations\n",
    "        'hmail.com': 'hotmail.com',\n",
    "        'hotmai.com': 'hotmail.com',\n",
    "        'hotamil.com': 'hotmail.com',\n",
    "        'hotmail.con': 'hotmail.com',\n",
    "        'hoo.com': 'hotmail.com',\n",
    "        'hotmiail.com': 'hotmail.com',\n",
    "        'htomail.com': 'hotmail.com',\n",
    "        'hiotmail.com': 'hotmail.com',\n",
    "        'hitmail.com': 'hotmail.com',\n",
    "        \n",
    "        # iCloud variations\n",
    "        'icould.com': 'icloud.com',\n",
    "        'iclod.com': 'icloud.com',\n",
    "        'cloud.com': 'icloud.com',\n",
    "        'icoud.com': 'icloud.com',\n",
    "        'ichoud.com': 'icloud.com',\n",
    "        'iclou.com': 'icloud.com',\n",
    "        \n",
    "        # AOL variations\n",
    "        'ao.com': 'aol.com',\n",
    "        'alo.com': 'aol.com',\n",
    "        'aol.co': 'aol.com',\n",
    "        'aol.om': 'aol.com',\n",
    "        \n",
    "        # Boston Public Schools variations\n",
    "        'bostonk12.com': 'bostonk12.org',\n",
    "        'boston12.org': 'bostonk12.org',\n",
    "        'bostobpublicschools.org': 'bostonpublicschools.org',\n",
    "        'bostonnpublicschools.org': 'bostonpublicschools.org',\n",
    "        'bostonnk12.org': 'bostonk12.org',\n",
    "        'bostonpulicschools.org': 'bostonpublicschools.org',\n",
    "        'bostonpublicschool.org.org': 'bostonpublicschools.org',\n",
    "        \n",
    "        # Other common corrections\n",
    "        'live.con': 'live.com',\n",
    "        'live.cm': 'live.com',\n",
    "        'outlook.pt': 'outlook.com',\n",
    "        'verizon.com': 'verizon.net',\n",
    "        'comcast.com': 'comcast.net',\n",
    "    }\n",
    "    \n",
    "    # Function to standardize email\n",
    "    def standardize_email(email):\n",
    "        if pd.isna(email):\n",
    "            return None\n",
    "        \n",
    "        # Convert to string, lowercase, and strip spaces\n",
    "        email_str = str(email).lower().strip()\n",
    "        \n",
    "        # Remove any spaces\n",
    "        email_str = email_str.replace(' ', '')\n",
    "        \n",
    "        # Basic validation\n",
    "        if '@' not in email_str or '.' not in email_str:\n",
    "            return None\n",
    "        \n",
    "        # Extract username and domain\n",
    "        username, domain = email_str.split('@', 1)\n",
    "        \n",
    "        # Apply domain correction\n",
    "        corrected_domain = domain_corrections.get(domain, domain)\n",
    "        \n",
    "        # Rebuild corrected email\n",
    "        return f\"{username}@{corrected_domain}\"\n",
    "    \n",
    "    # Function to normalize name\n",
    "    def normalize_name(name):\n",
    "        if pd.isna(name):\n",
    "            return None\n",
    "        \n",
    "        # Convert to string, lowercase, and strip spaces\n",
    "        name_str = str(name).lower().strip()\n",
    "        \n",
    "        # Remove extra spaces\n",
    "        name_str = re.sub(r'\\s+', ' ', name_str)\n",
    "        \n",
    "        # Remove non-alphabetic characters except spaces\n",
    "        name_str = re.sub(r'[^a-z ]', '', name_str)\n",
    "        \n",
    "        return name_str\n",
    "    \n",
    "    # Function to standardize date of birth\n",
    "    def standardize_dob(dob):\n",
    "        if pd.isna(dob):\n",
    "            return None\n",
    "        \n",
    "        # Try to handle various date formats\n",
    "        try:\n",
    "            # If it's already a datetime\n",
    "            if isinstance(dob, (pd.Timestamp, datetime)):\n",
    "                return dob.strftime('%Y-%m-%d')\n",
    "            \n",
    "            # Convert to string\n",
    "            dob_str = str(dob).strip()\n",
    "            \n",
    "            # Try parsing with pandas\n",
    "            try:\n",
    "                dob_date = pd.to_datetime(dob_str)\n",
    "                \n",
    "                # Adjust years for two-digit years (19xx vs 20xx)\n",
    "                if dob_date.year > datetime.now().year and dob_date.year < 2100:\n",
    "                    dob_date = dob_date.replace(year=dob_date.year - 100)\n",
    "                \n",
    "                return dob_date.strftime('%Y-%m-%d')\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Handle numeric formats without separators\n",
    "            if dob_str.isdigit():\n",
    "                if len(dob_str) == 8:  # MMDDYYYY\n",
    "                    month = int(dob_str[0:2])\n",
    "                    day = int(dob_str[2:4])\n",
    "                    year = int(dob_str[4:8])\n",
    "                    \n",
    "                    if 1 <= month <= 12 and 1 <= day <= 31:\n",
    "                        return f\"{year:04d}-{month:02d}-{day:02d}\"\n",
    "                \n",
    "                elif len(dob_str) == 6:  # MMDDYY\n",
    "                    month = int(dob_str[0:2])\n",
    "                    day = int(dob_str[2:4])\n",
    "                    year = int(dob_str[4:6])\n",
    "                    \n",
    "                    # Adjust years for two-digit years\n",
    "                    full_year = 1900 + year if year >= 50 else 2000 + year\n",
    "                    \n",
    "                    if 1 <= month <= 12 and 1 <= day <= 31:\n",
    "                        return f\"{full_year:04d}-{month:02d}-{day:02d}\"\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # If all parsing attempts fail, return None\n",
    "        return None\n",
    "    \n",
    "    # Identify email columns\n",
    "    pre_email_col = 'email_address' if 'email_address' in pre_df.columns else None\n",
    "    if not pre_email_col:\n",
    "        for col in pre_df.columns:\n",
    "            if 'email' in col.lower() and 'pre' in col.lower():\n",
    "                pre_email_col = col\n",
    "                break\n",
    "    \n",
    "    post_email_col = 'email_address' if 'email_address' in post_df.columns else None\n",
    "    if not post_email_col:\n",
    "        for col in post_df.columns:\n",
    "            if 'email' in col.lower() and 'post' in col.lower():\n",
    "                post_email_col = col\n",
    "                break\n",
    "    \n",
    "    print(f\"Using pre-enrollment email column: {pre_email_col}\")\n",
    "    print(f\"Using post-course email column: {post_email_col}\")\n",
    "    \n",
    "    # Identify name columns\n",
    "    pre_fullname_col = 'full_name' if 'full_name' in pre_df.columns else None\n",
    "    if not pre_fullname_col:\n",
    "        for col in pre_df.columns:\n",
    "            if 'full_name' in col.lower() and 'pre' in col.lower():\n",
    "                pre_fullname_col = col\n",
    "                break\n",
    "    \n",
    "    # If no full_name column, try to create one from first and last name\n",
    "    if not pre_fullname_col:\n",
    "        if 'first_name' in pre_df.columns and 'last_name' in pre_df.columns:\n",
    "            pre_df['full_name'] = pre_df['first_name'].fillna('') + ' ' + pre_df['last_name'].fillna('')\n",
    "            pre_df['full_name'] = pre_df['full_name'].str.strip()\n",
    "            pre_fullname_col = 'full_name'\n",
    "    \n",
    "    post_fullname_col = 'full_name' if 'full_name' in post_df.columns else None\n",
    "    if not post_fullname_col:\n",
    "        for col in post_df.columns:\n",
    "            if 'full_name' in col.lower() and 'post' in col.lower():\n",
    "                post_fullname_col = col\n",
    "                break\n",
    "    \n",
    "    # If no full_name column, try to create one from first and last name\n",
    "    if not post_fullname_col:\n",
    "        if 'first_name' in post_df.columns and 'last_name' in post_df.columns:\n",
    "            post_df['full_name'] = post_df['first_name'].fillna('') + ' ' + post_df['last_name'].fillna('')\n",
    "            post_df['full_name'] = post_df['full_name'].str.strip()\n",
    "            post_fullname_col = 'full_name'\n",
    "    \n",
    "    print(f\"Using pre-enrollment full name column: {pre_fullname_col}\")\n",
    "    print(f\"Using post-course full name column: {post_fullname_col}\")\n",
    "    \n",
    "    # Identify DOB columns\n",
    "    pre_dob_col = 'date_of_birth' if 'date_of_birth' in pre_df.columns else None\n",
    "    if not pre_dob_col:\n",
    "        for col in pre_df.columns:\n",
    "            if ('birth' in col.lower() or 'dob' in col.lower()) and 'pre' in col.lower():\n",
    "                pre_dob_col = col\n",
    "                break\n",
    "    \n",
    "    post_dob_col = 'date_of_birth' if 'date_of_birth' in post_df.columns else None\n",
    "    if not post_dob_col:\n",
    "        for col in post_df.columns:\n",
    "            if ('birth' in col.lower() or 'dob' in col.lower()) and 'post' in col.lower():\n",
    "                post_dob_col = col\n",
    "                break\n",
    "    \n",
    "    print(f\"Using pre-enrollment DOB column: {pre_dob_col}\")\n",
    "    print(f\"Using post-course DOB column: {post_dob_col}\")\n",
    "    \n",
    "    # Apply standardization\n",
    "    # Email\n",
    "    if pre_email_col:\n",
    "        pre_df['email_original'] = pre_df[pre_email_col]\n",
    "        pre_df['email_standardized'] = pre_df[pre_email_col].apply(standardize_email)\n",
    "        pre_email_changes = sum(pre_df['email_original'] != pre_df['email_standardized'])\n",
    "        print(f\"Standardized {pre_email_changes} pre-enrollment emails\")\n",
    "    \n",
    "    if post_email_col:\n",
    "        post_df['email_original'] = post_df[post_email_col]\n",
    "        post_df['email_standardized'] = post_df[post_email_col].apply(standardize_email)\n",
    "        post_email_changes = sum(post_df['email_original'] != post_df['email_standardized'])\n",
    "        print(f\"Standardized {post_email_changes} post-course emails\")\n",
    "    \n",
    "    # Names\n",
    "    if pre_fullname_col:\n",
    "        pre_df['name_normalized'] = pre_df[pre_fullname_col].apply(normalize_name)\n",
    "        print(f\"Normalized {sum(pre_df['name_normalized'].notna())} pre-enrollment names\")\n",
    "    \n",
    "    if post_fullname_col:\n",
    "        post_df['name_normalized'] = post_df[post_fullname_col].apply(normalize_name)\n",
    "        print(f\"Normalized {sum(post_df['name_normalized'].notna())} post-course names\")\n",
    "    \n",
    "    # DOB\n",
    "    if pre_dob_col:\n",
    "        pre_df['dob_standardized'] = pre_df[pre_dob_col].apply(standardize_dob)\n",
    "        print(f\"Standardized {sum(pre_df['dob_standardized'].notna())} pre-enrollment DOBs\")\n",
    "    \n",
    "    if post_dob_col:\n",
    "        post_df['dob_standardized'] = post_df[post_dob_col].apply(standardize_dob)\n",
    "        print(f\"Standardized {sum(post_df['dob_standardized'].notna())} post-course DOBs\")\n",
    "    \n",
    "    # Step 3: Perform matching\n",
    "    print(\"\\n3. Performing high-quality matching\")\n",
    "    \n",
    "    # 3.1: Email matching\n",
    "    print(\"Matching by standardized email...\")\n",
    "    \n",
    "    email_matches = []\n",
    "    \n",
    "    if 'email_standardized' in pre_df.columns and 'email_standardized' in post_df.columns:\n",
    "        # Create dictionaries for faster lookup\n",
    "        post_email_dict = {}\n",
    "        for idx, row in post_df.iterrows():\n",
    "            email = row['email_standardized']\n",
    "            if pd.notna(email):\n",
    "                if email not in post_email_dict:\n",
    "                    post_email_dict[email] = []\n",
    "                post_email_dict[email].append(idx)\n",
    "        \n",
    "        # Find matches\n",
    "        for pre_idx, pre_row in pre_df.iterrows():\n",
    "            pre_email = pre_row['email_standardized']\n",
    "            \n",
    "            if pd.notna(pre_email) and pre_email in post_email_dict:\n",
    "                for post_idx in post_email_dict[pre_email]:\n",
    "                    email_matches.append({\n",
    "                        'pre_idx': pre_idx,\n",
    "                        'post_idx': post_idx,\n",
    "                        'match_type': 'email'\n",
    "                    })\n",
    "    \n",
    "    print(f\"Found {len(email_matches)} matches by email\")\n",
    "    \n",
    "    # 3.2: Full name + DOB matching (only for records not matched by email)\n",
    "    print(\"Matching by full name + DOB...\")\n",
    "    \n",
    "    name_dob_matches = []\n",
    "    \n",
    "    if ('name_normalized' in pre_df.columns and 'name_normalized' in post_df.columns and\n",
    "        'dob_standardized' in pre_df.columns and 'dob_standardized' in post_df.columns):\n",
    "        \n",
    "        # Get pre-enrollment indices already matched by email\n",
    "        matched_pre_indices = set(match['pre_idx'] for match in email_matches)\n",
    "        \n",
    "        # Create a dictionary for post-course name+DOB\n",
    "        post_name_dob_dict = {}\n",
    "        for idx, row in post_df.iterrows():\n",
    "            name = row['name_normalized']\n",
    "            dob = row['dob_standardized']\n",
    "            \n",
    "            if pd.notna(name) and pd.notna(dob):\n",
    "                key = f\"{name}|{dob}\"\n",
    "                if key not in post_name_dob_dict:\n",
    "                    post_name_dob_dict[key] = []\n",
    "                post_name_dob_dict[key].append(idx)\n",
    "        \n",
    "        # Find matches for unmatched pre-enrollment records\n",
    "        for pre_idx, pre_row in pre_df.iterrows():\n",
    "            # Skip if already matched by email\n",
    "            if pre_idx in matched_pre_indices:\n",
    "                continue\n",
    "            \n",
    "            pre_name = pre_row['name_normalized']\n",
    "            pre_dob = pre_row['dob_standardized']\n",
    "            \n",
    "            if pd.notna(pre_name) and pd.notna(pre_dob):\n",
    "                key = f\"{pre_name}|{pre_dob}\"\n",
    "                \n",
    "                if key in post_name_dob_dict:\n",
    "                    for post_idx in post_name_dob_dict[key]:\n",
    "                        name_dob_matches.append({\n",
    "                            'pre_idx': pre_idx,\n",
    "                            'post_idx': post_idx,\n",
    "                            'match_type': 'full_name_dob'\n",
    "                        })\n",
    "    \n",
    "    print(f\"Found {len(name_dob_matches)} additional matches by full name + DOB\")\n",
    "    \n",
    "    # Step 4: Combine all matches\n",
    "    all_matches = email_matches + name_dob_matches\n",
    "    print(f\"Total matches: {len(all_matches)}\")\n",
    "    \n",
    "    # Step 5: Create merged dataset\n",
    "    print(\"\\n4. Creating merged dataset\")\n",
    "    \n",
    "    # Check for duplicate matches (same pre_idx matched to multiple post_idx)\n",
    "    pre_idx_counts = {}\n",
    "    for match in all_matches:\n",
    "        pre_idx = match['pre_idx']\n",
    "        if pre_idx not in pre_idx_counts:\n",
    "            pre_idx_counts[pre_idx] = 0\n",
    "        pre_idx_counts[pre_idx] += 1\n",
    "    \n",
    "    duplicate_pre_matches = {pre_idx: count for pre_idx, count in pre_idx_counts.items() if count > 1}\n",
    "    \n",
    "    if duplicate_pre_matches:\n",
    "        print(f\"\\nWarning: Found {len(duplicate_pre_matches)} pre-enrollment records matched to multiple post-course records\")\n",
    "        print(\"Resolving by keeping only the highest quality match for each pre-enrollment record\")\n",
    "        \n",
    "        # Keep only one match per pre_idx, prioritizing email matches\n",
    "        filtered_matches = []\n",
    "        seen_pre_idx = set()\n",
    "        \n",
    "        # First add all email matches\n",
    "        for match in email_matches:\n",
    "            pre_idx = match['pre_idx']\n",
    "            if pre_idx not in seen_pre_idx:\n",
    "                filtered_matches.append(match)\n",
    "                seen_pre_idx.add(pre_idx)\n",
    "        \n",
    "        # Then add name+DOB matches for unmatched pre_idx\n",
    "        for match in name_dob_matches:\n",
    "            pre_idx = match['pre_idx']\n",
    "            if pre_idx not in seen_pre_idx:\n",
    "                filtered_matches.append(match)\n",
    "                seen_pre_idx.add(pre_idx)\n",
    "        \n",
    "        print(f\"After resolving duplicates: {len(filtered_matches)} matches\")\n",
    "        all_matches = filtered_matches\n",
    "    \n",
    "    # Create a dataframe of matches\n",
    "    matches_df = pd.DataFrame(all_matches)\n",
    "    \n",
    "    # Add match confidence score\n",
    "    def calculate_match_confidence(row):\n",
    "        if row['match_type'] == 'email':\n",
    "            return 100\n",
    "        elif row['match_type'] == 'full_name_dob':\n",
    "            return 90\n",
    "        return 50\n",
    "    \n",
    "    matches_df['match_confidence'] = matches_df.apply(calculate_match_confidence, axis=1)\n",
    "    \n",
    "    # Perform the merge\n",
    "    merged_data = []\n",
    "    \n",
    "    # Prefix mappings to avoid column name conflicts\n",
    "    pre_prefix_map = {}\n",
    "    post_prefix_map = {}\n",
    "    \n",
    "    # Add prefixes to pre columns\n",
    "    for col in pre_df.columns:\n",
    "        if col.startswith('pre_'):\n",
    "            pre_prefix_map[col] = col\n",
    "        else:\n",
    "            pre_prefix_map[col] = f\"pre_{col}\"\n",
    "    \n",
    "    # Add prefixes to post columns\n",
    "    for col in post_df.columns:\n",
    "        if col.startswith('post_'):\n",
    "            post_prefix_map[col] = col\n",
    "        else:\n",
    "            post_prefix_map[col] = f\"post_{col}\"\n",
    "    \n",
    "    # Merge the data\n",
    "    for _, match in matches_df.iterrows():\n",
    "        pre_idx = match['pre_idx']\n",
    "        post_idx = match['post_idx']\n",
    "        \n",
    "        pre_row = pre_df.loc[pre_idx].to_dict()\n",
    "        post_row = post_df.loc[post_idx].to_dict()\n",
    "        \n",
    "        merged_row = {}\n",
    "        \n",
    "        # Add prefixed pre columns\n",
    "        for col, value in pre_row.items():\n",
    "            merged_row[pre_prefix_map[col]] = value\n",
    "        \n",
    "        # Add prefixed post columns\n",
    "        for col, value in post_row.items():\n",
    "            merged_row[post_prefix_map[col]] = value\n",
    "        \n",
    "        # Add match metadata\n",
    "        merged_row['match_type'] = match['match_type']\n",
    "        merged_row['match_confidence'] = match['match_confidence']\n",
    "        \n",
    "        merged_data.append(merged_row)\n",
    "    \n",
    "    # Create the final merged dataframe\n",
    "    merged_df = pd.DataFrame(merged_data)\n",
    "    \n",
    "    # Step 6: Quality checking\n",
    "    print(\"\\n5. Performing quality checks on merged dataset\")\n",
    "    \n",
    "    # Age calculation where possible\n",
    "    if ('pre_date_of_birth' in merged_df.columns and 'post_course_year' in merged_df.columns):\n",
    "        def calculate_age(row):\n",
    "            try:\n",
    "                dob = pd.to_datetime(row['pre_date_of_birth'])\n",
    "                course_year = row['post_course_year']\n",
    "                if pd.notna(dob) and pd.notna(course_year):\n",
    "                    return course_year - dob.year\n",
    "                return None\n",
    "            except:\n",
    "                return None\n",
    "        \n",
    "        merged_df['age_at_course'] = merged_df.apply(calculate_age, axis=1)\n",
    "        \n",
    "        # Identify age outliers\n",
    "        age_stats = merged_df['age_at_course'].describe()\n",
    "        print(\"\\nAge at course statistics:\")\n",
    "        print(f\"  Min: {age_stats['min']}\")\n",
    "        print(f\"  Max: {age_stats['max']}\")\n",
    "        print(f\"  Mean: {age_stats['mean']:.1f}\")\n",
    "        \n",
    "        young_outliers = merged_df[merged_df['age_at_course'] < 5].shape[0]\n",
    "        old_outliers = merged_df[merged_df['age_at_course'] > 90].shape[0]\n",
    "        \n",
    "        if young_outliers > 0 or old_outliers > 0:\n",
    "            print(f\"  Found {young_outliers} records with age < 5\")\n",
    "            print(f\"  Found {old_outliers} records with age > 90\")\n",
    "            print(\"  Consider addressing these age outliers separately as mentioned\")\n",
    "    \n",
    "    # Course year consistency\n",
    "    if 'pre_course_year' in merged_df.columns and 'post_course_year' in merged_df.columns:\n",
    "        timeline_issues = sum(merged_df['post_course_year'] < merged_df['pre_course_year'])\n",
    "        \n",
    "        if timeline_issues > 0:\n",
    "            print(f\"\\nFound {timeline_issues} records with pre-course year after post-course year\")\n",
    "            print(\"Consider addressing these timeline issues separately as mentioned\")\n",
    "    \n",
    "    # Step 7: Save the merged dataset\n",
    "    print(f\"\\n6. Saving high-quality merged dataset to {output_file}\")\n",
    "    merged_df.to_excel(output_file, index=False)\n",
    "    \n",
    "    print(\"\\nHigh-quality merged dataset created successfully!\")\n",
    "    print(f\"Total records: {len(merged_df)}\")\n",
    "    print(\"Match breakdown:\")\n",
    "    print(f\"  Email matches: {sum(merged_df['match_type'] == 'email')}\")\n",
    "    print(f\"  Full name + DOB matches: {sum(merged_df['match_type'] == 'full_name_dob')}\")\n",
    "    \n",
    "    # Return the merged dataframe\n",
    "    return merged_df\n",
    "\n",
    "# Run the function with your original files\n",
    "pre_course_path = '/Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/ES/Cleaned & Enhanced ES Data .xlsx'\n",
    "post_course_path = '/Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/PCS/Cleaned_Merged_PCS_2cleaned.xlsx'\n",
    "high_quality_merged_df = create_high_quality_merged_dataset(pre_course_path, post_course_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2aa2b677-aeed-41fe-bb72-1f00a6075dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debugging matching process between pre-enrollment and post-course data\n",
      "\n",
      "1. Loading datasets\n",
      "Loaded 29700 pre-enrollment records\n",
      "Loaded 7806 post-course records\n",
      "\n",
      "2. Examining column names\n",
      "Pre-enrollment DOB column candidates:\n",
      "  date_of_birth_standardized: 29611 non-null values (99.7%)\n",
      "  childs_date_of_birth: 7097 non-null values (23.9%)\n",
      "Post-course DOB column candidates:\n",
      "  date_of_birth: 7748 non-null values (99.3%)\n",
      "\n",
      "Pre-enrollment name column candidates:\n",
      "  name_of_siteschool: 18047 non-null values (60.8%)\n",
      "  course_name: 19991 non-null values (67.3%)\n",
      "  first_name: 29614 non-null values (99.7%)\n",
      "  last_name: 29613 non-null values (99.7%)\n",
      "  full_name: 29614 non-null values (99.7%)\n",
      "  childs_first_name: 7052 non-null values (23.7%)\n",
      "  childs_last_name: 7050 non-null values (23.7%)\n",
      "  name_of_business: 315 non-null values (1.1%)\n",
      "  please_enter_anyall_of_your_social_media_handles_separated_by_a_comma_eg_techgoeshome_facebookcomyourbusinessname: 1877 non-null values (6.3%)\n",
      "  middle_name: 7148 non-null values (24.1%)\n",
      "  Unnamed: 33: 0 non-null values (0.0%)\n",
      "Post-course name column candidates:\n",
      "  course_name: 7686 non-null values (98.5%)\n",
      "  first_name: 7749 non-null values (99.3%)\n",
      "  last_name: 7747 non-null values (99.2%)\n",
      "  please_share_anyall_of_your_social_media_handles_separated_by_a_comma_eg_techgoeshome_facebookcomyourbusinessname: 180 non-null values (2.3%)\n",
      "  form_name: 5524 non-null values (70.8%)\n",
      "  Unnamed: 90: 0 non-null values (0.0%)\n",
      "  Unnamed: 91: 0 non-null values (0.0%)\n",
      "  full_name: 7749 non-null values (99.3%)\n",
      "\n",
      "Selected columns for matching:\n",
      "Pre-enrollment DOB column: date_of_birth_standardized\n",
      "Post-course DOB column: date_of_birth\n",
      "Pre-enrollment full name column: full_name\n",
      "Post-course full name column: full_name\n",
      "\n",
      "3. Examining DOB formats\n",
      "Pre-enrollment DOB samples:\n",
      "  06-27-1988 (type: str)\n",
      "  06-08-1968 (type: str)\n",
      "  10-25-1993 (type: str)\n",
      "  01-02-1996 (type: str)\n",
      "  06-04-1985 (type: str)\n",
      "Post-course DOB samples:\n",
      "  09-15-1997 (type: str)\n",
      "  05-27-1996 (type: str)\n",
      "  07-04-1981 (type: str)\n",
      "  08-31-1980 (type: str)\n",
      "  04-22-1997 (type: str)\n",
      "\n",
      "4. After DOB standardization\n",
      "Pre-enrollment standardized DOBs: 29585 non-null values\n",
      "Post-course standardized DOBs: 7708 non-null values\n",
      "\n",
      "Pre-enrollment DOB standardization examples:\n",
      "  Original: 12-27-1955 -> Standardized: 1955-12-27\n",
      "  Original: 03-03-1960 -> Standardized: 1960-03-03\n",
      "  Original: 07-17-1972 -> Standardized: 1972-07-17\n",
      "  Original: 01-04-1956 -> Standardized: 1956-01-04\n",
      "  Original: 01-14-1927 -> Standardized: 1927-01-14\n",
      "Post-course DOB standardization examples:\n",
      "  Original: 11-11-1951 -> Standardized: 1951-11-11\n",
      "  Original: 08-20-1957 -> Standardized: 1957-08-20\n",
      "  Original: 10-06-1944 -> Standardized: 1944-10-06\n",
      "  Original: 10-26-1959 -> Standardized: 1959-10-26\n",
      "  Original: 04-23-1953 -> Standardized: 1953-04-23\n",
      "\n",
      "5. Name normalization\n",
      "Pre-enrollment normalized names: 29614 non-null values\n",
      "Post-course normalized names: 7749 non-null values\n",
      "\n",
      "Pre-enrollment name normalization examples:\n",
      "  Original: jacki sargent -> Normalized: jacki sargent\n",
      "  Original: Eileen Peterson -> Normalized: eileen peterson\n",
      "  Original: NGHIA HUYNH -> Normalized: nghia huynh\n",
      "  Original: Celina Guardado -> Normalized: celina guardado\n",
      "  Original: Carmen Concepcion -> Normalized: carmen concepcion\n",
      "Post-course name normalization examples:\n",
      "  Original: Marjorie Theodore -> Normalized: marjorie theodore\n",
      "  Original: Pedro Hilario -> Normalized: pedro hilario\n",
      "  Original: Yan Hua Lin -> Normalized: yan hua lin\n",
      "  Original: Rhodoora Tesorio -> Normalized: rhodoora tesorio\n",
      "  Original: Reginald Abel -> Normalized: reginald abel\n",
      "\n",
      "6. Attempting full name + DOB matching\n",
      "Pre-enrollment records with valid name+DOB keys: 29585\n",
      "Post-course records with valid name+DOB keys: 7708\n",
      "Number of matching name+DOB combinations: 4803\n",
      "\n",
      "Sample matching records:\n",
      "Key: munia elakeili|1981-07-23\n",
      "  Pre-enrollment: Munia Elakeili (DOB: 07-23-1981)\n",
      "  Post-course: Munia Elakeili (DOB: 07-23-1981)\n",
      "Key: alex pheun|2007-01-07\n",
      "  Pre-enrollment: Alex Pheun (DOB: 01-07-2007)\n",
      "  Post-course: Alex Pheun (DOB: 01-07-2007)\n",
      "Key: eliette jean beauplan|1971-10-17\n",
      "  Pre-enrollment: Eliette Jean Beauplan (DOB: 10-17-1971)\n",
      "  Post-course: Eliette Jean Beauplan (DOB: 10-17-1971)\n",
      "Key: safae boudynah|1986-08-22\n",
      "  Pre-enrollment: Safae Boudynah (DOB: 08-22-1986)\n",
      "  Post-course: Safae Boudynah (DOB: 08-22-1986)\n",
      "Key: paul decologero|2006-06-02\n",
      "  Pre-enrollment: paul decologero (DOB: 06-02-2006)\n",
      "  Post-course: Paul Decologero (DOB: 06-02-2006)\n",
      "\n",
      "7. Analyzing DOB distribution\n",
      "Top 5 birth years in pre-enrollment data:\n",
      "  1777: 1\n",
      "  1789: 1\n",
      "  1864: 1\n",
      "  1866: 1\n",
      "  1869: 1\n",
      "Top 5 birth years in post-course data:\n",
      "  1871: 1\n",
      "  1883: 1\n",
      "  1926: 1\n",
      "  1930: 1\n",
      "  1931: 1\n",
      "\n",
      "Debugging completed. You can use this information to adjust the matching algorithm.\n"
     ]
    }
   ],
   "source": [
    "#name + BOB match Debuging\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "def debug_matching_process(pre_course_path, post_course_path):\n",
    "    \"\"\"\n",
    "    Debug the matching process, focusing on the full name + DOB matching.\n",
    "    \n",
    "    Parameters:\n",
    "    pre_course_path (str): Path to the pre-enrollment Excel file\n",
    "    post_course_path (str): Path to the post-course Excel file\n",
    "    \"\"\"\n",
    "    print(\"Debugging matching process between pre-enrollment and post-course data\")\n",
    "    \n",
    "    # Step 1: Load the datasets\n",
    "    print(\"\\n1. Loading datasets\")\n",
    "    pre_df = pd.read_excel(pre_course_path)\n",
    "    print(f\"Loaded {len(pre_df)} pre-enrollment records\")\n",
    "    \n",
    "    post_df = pd.read_excel(post_course_path)\n",
    "    print(f\"Loaded {len(post_df)} post-course records\")\n",
    "    \n",
    "    # Step 2: Examine column names\n",
    "    print(\"\\n2. Examining column names\")\n",
    "    \n",
    "    # Check for DOB columns\n",
    "    pre_dob_candidates = [col for col in pre_df.columns if 'birth' in col.lower() or 'dob' in col.lower()]\n",
    "    post_dob_candidates = [col for col in post_df.columns if 'birth' in col.lower() or 'dob' in col.lower()]\n",
    "    \n",
    "    print(\"Pre-enrollment DOB column candidates:\")\n",
    "    for col in pre_dob_candidates:\n",
    "        non_null = pre_df[col].notna().sum()\n",
    "        print(f\"  {col}: {non_null} non-null values ({non_null/len(pre_df)*100:.1f}%)\")\n",
    "    \n",
    "    print(\"Post-course DOB column candidates:\")\n",
    "    for col in post_dob_candidates:\n",
    "        non_null = post_df[col].notna().sum()\n",
    "        print(f\"  {col}: {non_null} non-null values ({non_null/len(post_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Check for name columns\n",
    "    pre_name_candidates = [col for col in pre_df.columns if 'name' in col.lower()]\n",
    "    post_name_candidates = [col for col in post_df.columns if 'name' in col.lower()]\n",
    "    \n",
    "    print(\"\\nPre-enrollment name column candidates:\")\n",
    "    for col in pre_name_candidates:\n",
    "        non_null = pre_df[col].notna().sum()\n",
    "        print(f\"  {col}: {non_null} non-null values ({non_null/len(pre_df)*100:.1f}%)\")\n",
    "    \n",
    "    print(\"Post-course name column candidates:\")\n",
    "    for col in post_name_candidates:\n",
    "        non_null = post_df[col].notna().sum()\n",
    "        print(f\"  {col}: {non_null} non-null values ({non_null/len(post_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Step 3: Select the appropriate columns\n",
    "    pre_dob_col = None\n",
    "    if pre_dob_candidates:\n",
    "        # Choose the column with the most non-null values\n",
    "        pre_dob_col = max(pre_dob_candidates, key=lambda col: pre_df[col].notna().sum())\n",
    "    \n",
    "    post_dob_col = None\n",
    "    if post_dob_candidates:\n",
    "        # Choose the column with the most non-null values\n",
    "        post_dob_col = max(post_dob_candidates, key=lambda col: post_df[col].notna().sum())\n",
    "    \n",
    "    pre_fullname_col = 'full_name' if 'full_name' in pre_df.columns else None\n",
    "    if not pre_fullname_col and 'pre_full_name' in pre_df.columns:\n",
    "        pre_fullname_col = 'pre_full_name'\n",
    "    \n",
    "    post_fullname_col = 'full_name' if 'full_name' in post_df.columns else None\n",
    "    if not post_fullname_col and 'post_full_name' in post_df.columns:\n",
    "        post_fullname_col = 'post_full_name'\n",
    "    \n",
    "    print(f\"\\nSelected columns for matching:\")\n",
    "    print(f\"Pre-enrollment DOB column: {pre_dob_col}\")\n",
    "    print(f\"Post-course DOB column: {post_dob_col}\")\n",
    "    print(f\"Pre-enrollment full name column: {pre_fullname_col}\")\n",
    "    print(f\"Post-course full name column: {post_fullname_col}\")\n",
    "    \n",
    "    # Step 4: Examine DOB formats\n",
    "    if pre_dob_col and post_dob_col:\n",
    "        print(\"\\n3. Examining DOB formats\")\n",
    "        \n",
    "        # Get sample values\n",
    "        pre_dob_samples = pre_df[pre_dob_col].dropna().sample(min(5, pre_df[pre_dob_col].notna().sum())).tolist()\n",
    "        post_dob_samples = post_df[post_dob_col].dropna().sample(min(5, post_df[post_dob_col].notna().sum())).tolist()\n",
    "        \n",
    "        print(\"Pre-enrollment DOB samples:\")\n",
    "        for sample in pre_dob_samples:\n",
    "            print(f\"  {sample} (type: {type(sample).__name__})\")\n",
    "        \n",
    "        print(\"Post-course DOB samples:\")\n",
    "        for sample in post_dob_samples:\n",
    "            print(f\"  {sample} (type: {type(sample).__name__})\")\n",
    "    \n",
    "    # Step 5: Standardize DOBs\n",
    "    def standardize_dob(dob):\n",
    "        if pd.isna(dob):\n",
    "            return None\n",
    "        \n",
    "        # Try to handle various date formats\n",
    "        try:\n",
    "            # If it's already a datetime\n",
    "            if isinstance(dob, (pd.Timestamp, datetime)):\n",
    "                return dob.strftime('%Y-%m-%d')\n",
    "            \n",
    "            # Convert to string\n",
    "            dob_str = str(dob).strip()\n",
    "            \n",
    "            # Try parsing with pandas\n",
    "            try:\n",
    "                dob_date = pd.to_datetime(dob_str)\n",
    "                \n",
    "                # Adjust years for two-digit years (19xx vs 20xx)\n",
    "                if dob_date.year > datetime.now().year and dob_date.year < 2100:\n",
    "                    dob_date = dob_date.replace(year=dob_date.year - 100)\n",
    "                \n",
    "                return dob_date.strftime('%Y-%m-%d')\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Handle numeric formats without separators\n",
    "            if dob_str.isdigit():\n",
    "                if len(dob_str) == 8:  # MMDDYYYY\n",
    "                    month = int(dob_str[0:2])\n",
    "                    day = int(dob_str[2:4])\n",
    "                    year = int(dob_str[4:8])\n",
    "                    \n",
    "                    if 1 <= month <= 12 and 1 <= day <= 31:\n",
    "                        return f\"{year:04d}-{month:02d}-{day:02d}\"\n",
    "                \n",
    "                elif len(dob_str) == 6:  # MMDDYY\n",
    "                    month = int(dob_str[0:2])\n",
    "                    day = int(dob_str[2:4])\n",
    "                    year = int(dob_str[4:6])\n",
    "                    \n",
    "                    # Adjust years for two-digit years\n",
    "                    full_year = 1900 + year if year >= 50 else 2000 + year\n",
    "                    \n",
    "                    if 1 <= month <= 12 and 1 <= day <= 31:\n",
    "                        return f\"{full_year:04d}-{month:02d}-{day:02d}\"\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # If all parsing attempts fail, return None\n",
    "        return None\n",
    "    \n",
    "    if pre_dob_col and post_dob_col:\n",
    "        pre_df['dob_standardized'] = pre_df[pre_dob_col].apply(standardize_dob)\n",
    "        post_df['dob_standardized'] = post_df[post_dob_col].apply(standardize_dob)\n",
    "        \n",
    "        print(\"\\n4. After DOB standardization\")\n",
    "        print(f\"Pre-enrollment standardized DOBs: {pre_df['dob_standardized'].notna().sum()} non-null values\")\n",
    "        print(f\"Post-course standardized DOBs: {post_df['dob_standardized'].notna().sum()} non-null values\")\n",
    "        \n",
    "        # Show standardized samples\n",
    "        pre_std_samples = pre_df[['dob_standardized', pre_dob_col]].dropna(subset=['dob_standardized']).sample(min(5, pre_df['dob_standardized'].notna().sum())).values.tolist()\n",
    "        post_std_samples = post_df[['dob_standardized', post_dob_col]].dropna(subset=['dob_standardized']).sample(min(5, post_df['dob_standardized'].notna().sum())).values.tolist()\n",
    "        \n",
    "        print(\"\\nPre-enrollment DOB standardization examples:\")\n",
    "        for std, orig in pre_std_samples:\n",
    "            print(f\"  Original: {orig} -> Standardized: {std}\")\n",
    "        \n",
    "        print(\"Post-course DOB standardization examples:\")\n",
    "        for std, orig in post_std_samples:\n",
    "            print(f\"  Original: {orig} -> Standardized: {std}\")\n",
    "    \n",
    "    # Step 6: Normalize names\n",
    "    def normalize_name(name):\n",
    "        if pd.isna(name):\n",
    "            return None\n",
    "        \n",
    "        # Convert to string, lowercase, and strip spaces\n",
    "        name_str = str(name).lower().strip()\n",
    "        \n",
    "        # Remove extra spaces\n",
    "        name_str = re.sub(r'\\s+', ' ', name_str)\n",
    "        \n",
    "        # Remove non-alphabetic characters except spaces\n",
    "        name_str = re.sub(r'[^a-z ]', '', name_str)\n",
    "        \n",
    "        return name_str\n",
    "    \n",
    "    if pre_fullname_col and post_fullname_col:\n",
    "        pre_df['name_normalized'] = pre_df[pre_fullname_col].apply(normalize_name)\n",
    "        post_df['name_normalized'] = post_df[post_fullname_col].apply(normalize_name)\n",
    "        \n",
    "        print(\"\\n5. Name normalization\")\n",
    "        print(f\"Pre-enrollment normalized names: {pre_df['name_normalized'].notna().sum()} non-null values\")\n",
    "        print(f\"Post-course normalized names: {post_df['name_normalized'].notna().sum()} non-null values\")\n",
    "        \n",
    "        # Show normalized samples\n",
    "        pre_name_samples = pre_df[[pre_fullname_col, 'name_normalized']].dropna().sample(min(5, pre_df['name_normalized'].notna().sum())).values.tolist()\n",
    "        post_name_samples = post_df[[post_fullname_col, 'name_normalized']].dropna().sample(min(5, post_df['name_normalized'].notna().sum())).values.tolist()\n",
    "        \n",
    "        print(\"\\nPre-enrollment name normalization examples:\")\n",
    "        for orig, norm in pre_name_samples:\n",
    "            print(f\"  Original: {orig} -> Normalized: {norm}\")\n",
    "        \n",
    "        print(\"Post-course name normalization examples:\")\n",
    "        for orig, norm in post_name_samples:\n",
    "            print(f\"  Original: {orig} -> Normalized: {norm}\")\n",
    "    \n",
    "    # Step 7: Attempt full name + DOB matching\n",
    "    print(\"\\n6. Attempting full name + DOB matching\")\n",
    "    \n",
    "    if ('name_normalized' in pre_df.columns and 'name_normalized' in post_df.columns and\n",
    "        'dob_standardized' in pre_df.columns and 'dob_standardized' in post_df.columns):\n",
    "        \n",
    "        # Create name+DOB keys for both datasets\n",
    "        pre_df['name_dob_key'] = pre_df.apply(\n",
    "            lambda row: f\"{row['name_normalized']}|{row['dob_standardized']}\" \n",
    "            if pd.notna(row['name_normalized']) and pd.notna(row['dob_standardized']) else None, \n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        post_df['name_dob_key'] = post_df.apply(\n",
    "            lambda row: f\"{row['name_normalized']}|{row['dob_standardized']}\" \n",
    "            if pd.notna(row['name_normalized']) and pd.notna(row['dob_standardized']) else None, \n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        print(f\"Pre-enrollment records with valid name+DOB keys: {pre_df['name_dob_key'].notna().sum()}\")\n",
    "        print(f\"Post-course records with valid name+DOB keys: {post_df['name_dob_key'].notna().sum()}\")\n",
    "        \n",
    "        # Find common keys\n",
    "        pre_keys = set(pre_df['name_dob_key'].dropna())\n",
    "        post_keys = set(post_df['name_dob_key'].dropna())\n",
    "        \n",
    "        common_keys = pre_keys.intersection(post_keys)\n",
    "        print(f\"Number of matching name+DOB combinations: {len(common_keys)}\")\n",
    "        \n",
    "        # Show a few examples of matching records\n",
    "        if common_keys:\n",
    "            sample_keys = list(common_keys)[:min(5, len(common_keys))]\n",
    "            \n",
    "            print(\"\\nSample matching records:\")\n",
    "            for key in sample_keys:\n",
    "                pre_matches = pre_df[pre_df['name_dob_key'] == key]\n",
    "                post_matches = post_df[post_df['name_dob_key'] == key]\n",
    "                \n",
    "                print(f\"Key: {key}\")\n",
    "                print(f\"  Pre-enrollment: {pre_matches[pre_fullname_col].iloc[0]} (DOB: {pre_matches[pre_dob_col].iloc[0] if pre_dob_col else 'N/A'})\")\n",
    "                print(f\"  Post-course: {post_matches[post_fullname_col].iloc[0]} (DOB: {post_matches[post_dob_col].iloc[0]})\")\n",
    "    else:\n",
    "        print(\"Cannot perform name+DOB matching due to missing data\")\n",
    "    \n",
    "    # Step 8: Check for DOB value distribution\n",
    "    if 'dob_standardized' in pre_df.columns and 'dob_standardized' in post_df.columns:\n",
    "        print(\"\\n7. Analyzing DOB distribution\")\n",
    "        \n",
    "        # Count years\n",
    "        if pre_df['dob_standardized'].notna().any():\n",
    "            pre_years = pre_df['dob_standardized'].dropna().str[:4].value_counts().sort_index()\n",
    "            print(\"Top 5 birth years in pre-enrollment data:\")\n",
    "            for year, count in pre_years.head().items():\n",
    "                print(f\"  {year}: {count}\")\n",
    "        else:\n",
    "            print(\"No valid DOB years in pre-enrollment data\")\n",
    "        \n",
    "        if post_df['dob_standardized'].notna().any():\n",
    "            post_years = post_df['dob_standardized'].dropna().str[:4].value_counts().sort_index()\n",
    "            print(\"Top 5 birth years in post-course data:\")\n",
    "            for year, count in post_years.head().items():\n",
    "                print(f\"  {year}: {count}\")\n",
    "        else:\n",
    "            print(\"No valid DOB years in post-course data\")\n",
    "    \n",
    "    print(\"\\nDebugging completed. You can use this information to adjust the matching algorithm.\")\n",
    "\n",
    "# Run the debug function\n",
    "pre_course_path = '/Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/ES/Cleaned & Enhanced ES Data .xlsx'\n",
    "post_course_path = '/Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/PCS/Cleaned_Merged_PCS_2cleaned.xlsx'\n",
    "debug_matching_process(pre_course_path, post_course_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "576dadd2-bcaa-4c0d-a7ec-00c173a8fd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating high-quality merged dataset from original files\n",
      "\n",
      "1. Loading datasets\n",
      "Loading pre-enrollment data from: /Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/ES/Cleaned & Enhanced ES Data .xlsx\n",
      "Loaded 29700 pre-enrollment records\n",
      "Loading post-course data from: /Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/PCS/Cleaned_Merged_PCS_2cleaned.xlsx\n",
      "Loaded 7806 post-course records\n",
      "\n",
      "2. Standardizing and cleaning data\n",
      "Using pre-enrollment email column: email_address\n",
      "Using post-course email column: email_address\n",
      "Using pre-enrollment full name column: full_name\n",
      "Using post-course full name column: full_name\n",
      "Using pre-enrollment DOB column: date_of_birth_standardized\n",
      "Using post-course DOB column: date_of_birth\n",
      "Standardized 281 pre-enrollment emails\n",
      "Standardized 51 post-course emails\n",
      "Normalized 29614 pre-enrollment names\n",
      "Normalized 7749 post-course names\n",
      "Standardized 29583 pre-enrollment DOBs\n",
      "Standardized 7708 post-course DOBs\n",
      "\n",
      "3. Performing matching using multiple identifiers\n",
      "Matching by standardized email...\n",
      "Found 3241 matches by email\n",
      "Matching by full name + DOB...\n",
      "Found 5314 matches by full name + DOB\n",
      "Total raw matches (before de-duplication): 8555\n",
      "\n",
      "4. Resolving duplicate matches\n",
      "Found 2460 duplicate match pairs\n",
      "Unique matches after de-duplication: 6095\n",
      "Final match breakdown:\n",
      "  Email matches: 3241\n",
      "  Full name + DOB matches: 2854\n",
      "  Total unique matches: 6095\n",
      "\n",
      "5. Creating merged dataset\n",
      "Warning: 123 pre-enrollment records are matched to multiple post-course records\n",
      "Keeping only one match per pre-enrollment record\n",
      "\n",
      "6. Performing quality checks on merged dataset\n",
      "\n",
      "Age at course statistics:\n",
      "  Min: 0.0\n",
      "  Max: 93.0\n",
      "  Mean: 44.4\n",
      "  Found 19 records with age < 5\n",
      "  Found 7 records with age > 90\n",
      "  Consider addressing these age outliers separately as mentioned\n",
      "\n",
      "Found 90 records with pre-course year after post-course year\n",
      "Consider addressing these timeline issues separately as mentioned\n",
      "\n",
      "7. Saving high-quality merged dataset to /Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/ES/Direct_High_Quality_Merged_Dataset.xlsx\n",
      "\n",
      "High-quality merged dataset created successfully!\n",
      "Total records: 5967\n",
      "Match breakdown:\n",
      "  Email matches: 3241\n",
      "  Full name + DOB matches: 2726\n"
     ]
    }
   ],
   "source": [
    "#enhanced matching with email, name + DOB\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "def create_high_quality_merged_dataset(pre_course_path, post_course_path):\n",
    "    \"\"\"\n",
    "    Create a high-quality merged dataset directly from pre-enrollment and post-course data,\n",
    "    using both email and full name + DOB matching.\n",
    "    \n",
    "    Parameters:\n",
    "    pre_course_path (str): Path to the pre-enrollment Excel file\n",
    "    post_course_path (str): Path to the post-course Excel file\n",
    "    \"\"\"\n",
    "    print(\"Creating high-quality merged dataset from original files\")\n",
    "    \n",
    "    # Step 1: Load the datasets\n",
    "    print(\"\\n1. Loading datasets\")\n",
    "    print(f\"Loading pre-enrollment data from: {pre_course_path}\")\n",
    "    pre_df = pd.read_excel(pre_course_path)\n",
    "    print(f\"Loaded {len(pre_df)} pre-enrollment records\")\n",
    "    \n",
    "    print(f\"Loading post-course data from: {post_course_path}\")\n",
    "    post_df = pd.read_excel(post_course_path)\n",
    "    print(f\"Loaded {len(post_df)} post-course records\")\n",
    "    \n",
    "    # Create output directory and file path\n",
    "    output_dir = os.path.dirname(pre_course_path)\n",
    "    output_file = os.path.join(output_dir, \"Direct_High_Quality_Merged_Dataset.xlsx\")\n",
    "    \n",
    "    # Step 2: Standardize and clean data for matching\n",
    "    print(\"\\n2. Standardizing and cleaning data\")\n",
    "    \n",
    "    # Domain corrections dictionary\n",
    "    domain_corrections = {\n",
    "        # Gmail variations\n",
    "        'gmai.com': 'gmail.com',\n",
    "        'gmil.com': 'gmail.com',\n",
    "        'gmal.com': 'gmail.com',\n",
    "        'gamail.com': 'gmail.com',\n",
    "        'gimail.com': 'gmail.com',\n",
    "        'gmaill.com': 'gmail.com',\n",
    "        'gmail.comm': 'gmail.com',\n",
    "        'gmial.com': 'gmail.com',\n",
    "        'gmail.cm': 'gmail.com',\n",
    "        'gmail.org': 'gmail.com',\n",
    "        'gemail.com': 'gmail.com',\n",
    "        'gmail.com.com': 'gmail.com',\n",
    "        'gmai.coml': 'gmail.com',\n",
    "        'gmail.coom': 'gmail.com',\n",
    "        'jmail.com': 'gmail.com',\n",
    "        'gmeil.com': 'gmail.com',\n",
    "        'gmall.com': 'gmail.com',\n",
    "        'gail.com': 'gmail.com',\n",
    "        'gmail.co': 'gmail.com',\n",
    "        'gmaiil.com': 'gmail.com',\n",
    "        'gmaim.com': 'gmail.com',\n",
    "        'gmail.om': 'gmail.com',\n",
    "        'gmail.ccom': 'gmail.com',\n",
    "        'g.com': 'gmail.com',\n",
    "        'gmail.vom': 'gmail.com',\n",
    "        'gmali.com': 'gmail.com',\n",
    "        'gmaio.com': 'gmail.com',\n",
    "        'g-mail.com': 'gmail.com',\n",
    "        'gamill.com': 'gmail.com',\n",
    "        'gmaol.com': 'gmail.com',\n",
    "        'gmsil.com': 'gmail.com',\n",
    "        'gmaii.com': 'gmail.com',\n",
    "        'gmail.oeg': 'gmail.com',\n",
    "        'gmanil.com': 'gmail.com',\n",
    "        'gmaili.com': 'gmail.com',\n",
    "        'gmail.crom': 'gmail.com',\n",
    "        'gmail.com11': 'gmail.com',\n",
    "        'gmill.com': 'gmail.com',\n",
    "        'gmail.com3': 'gmail.com',\n",
    "        'gnmail.com': 'gmail.com',\n",
    "        'gmaij.com': 'gmail.com',\n",
    "        'gmail.coml.com': 'gmail.com',\n",
    "        \n",
    "        # Yahoo variations\n",
    "        'yahoo.con': 'yahoo.com',\n",
    "        'yaoo.com': 'yahoo.com',\n",
    "        'hayoo.com': 'yahoo.com',\n",
    "        'yhoo.com': 'yahoo.com',\n",
    "        'yahoo.comm': 'yahoo.com',\n",
    "        'myyahoo.com': 'yahoo.com',\n",
    "        'yahoo.comb': 'yahoo.com',\n",
    "        'yahool.com': 'yahoo.com',\n",
    "        'yaho.cm': 'yahoo.com',\n",
    "        'yahio.un': 'yahoo.com',\n",
    "        \n",
    "        # Hotmail variations\n",
    "        'hmail.com': 'hotmail.com',\n",
    "        'hotmai.com': 'hotmail.com',\n",
    "        'hotamil.com': 'hotmail.com',\n",
    "        'hotmail.con': 'hotmail.com',\n",
    "        'hoo.com': 'hotmail.com',\n",
    "        'hotmiail.com': 'hotmail.com',\n",
    "        'htomail.com': 'hotmail.com',\n",
    "        'hiotmail.com': 'hotmail.com',\n",
    "        'hitmail.com': 'hotmail.com',\n",
    "        \n",
    "        # iCloud variations\n",
    "        'icould.com': 'icloud.com',\n",
    "        'iclod.com': 'icloud.com',\n",
    "        'cloud.com': 'icloud.com',\n",
    "        'icoud.com': 'icloud.com',\n",
    "        'ichoud.com': 'icloud.com',\n",
    "        'iclou.com': 'icloud.com',\n",
    "        \n",
    "        # AOL variations\n",
    "        'ao.com': 'aol.com',\n",
    "        'alo.com': 'aol.com',\n",
    "        'aol.co': 'aol.com',\n",
    "        'aol.om': 'aol.com',\n",
    "        \n",
    "        # Boston Public Schools variations\n",
    "        'bostonk12.com': 'bostonk12.org',\n",
    "        'boston12.org': 'bostonk12.org',\n",
    "        'bostobpublicschools.org': 'bostonpublicschools.org',\n",
    "        'bostonnpublicschools.org': 'bostonpublicschools.org',\n",
    "        'bostonnk12.org': 'bostonk12.org',\n",
    "        'bostonpulicschools.org': 'bostonpublicschools.org',\n",
    "        'bostonpublicschool.org.org': 'bostonpublicschools.org',\n",
    "        \n",
    "        # Other common corrections\n",
    "        'live.con': 'live.com',\n",
    "        'live.cm': 'live.com',\n",
    "        'outlook.pt': 'outlook.com',\n",
    "        'verizon.com': 'verizon.net',\n",
    "        'comcast.com': 'comcast.net',\n",
    "    }\n",
    "    \n",
    "    # Email standardization\n",
    "    def standardize_email(email):\n",
    "        if pd.isna(email):\n",
    "            return None\n",
    "        \n",
    "        # Convert to string, lowercase, and strip spaces\n",
    "        email_str = str(email).lower().strip()\n",
    "        \n",
    "        # Remove any spaces\n",
    "        email_str = email_str.replace(' ', '')\n",
    "        \n",
    "        # Basic validation\n",
    "        if '@' not in email_str or '.' not in email_str:\n",
    "            return None\n",
    "        \n",
    "        # Extract username and domain\n",
    "        username, domain = email_str.split('@', 1)\n",
    "        \n",
    "        # Apply domain correction\n",
    "        corrected_domain = domain_corrections.get(domain, domain)\n",
    "        \n",
    "        # Rebuild corrected email\n",
    "        return f\"{username}@{corrected_domain}\"\n",
    "    \n",
    "    # Name normalization\n",
    "    def normalize_name(name):\n",
    "        if pd.isna(name):\n",
    "            return None\n",
    "        \n",
    "        # Convert to string, lowercase, and strip spaces\n",
    "        name_str = str(name).lower().strip()\n",
    "        \n",
    "        # Remove extra spaces\n",
    "        name_str = re.sub(r'\\s+', ' ', name_str)\n",
    "        \n",
    "        # Remove non-alphabetic characters except spaces\n",
    "        name_str = re.sub(r'[^a-z ]', '', name_str)\n",
    "        \n",
    "        return name_str\n",
    "    \n",
    "    # DOB standardization\n",
    "    def standardize_dob(dob):\n",
    "        if pd.isna(dob):\n",
    "            return None\n",
    "        \n",
    "        # Try to handle various date formats\n",
    "        try:\n",
    "            # If it's already a datetime\n",
    "            if isinstance(dob, (pd.Timestamp, datetime)):\n",
    "                return dob.strftime('%Y-%m-%d')\n",
    "            \n",
    "            # Convert to string\n",
    "            dob_str = str(dob).strip()\n",
    "            \n",
    "            # Try parsing with pandas\n",
    "            try:\n",
    "                dob_date = pd.to_datetime(dob_str)\n",
    "                \n",
    "                # Adjust years for two-digit years (19xx vs 20xx)\n",
    "                if dob_date.year > datetime.now().year and dob_date.year < 2100:\n",
    "                    dob_date = dob_date.replace(year=dob_date.year - 100)\n",
    "                \n",
    "                # Additional sanity check for very old birth years\n",
    "                if dob_date.year < 1900:\n",
    "                    # This is likely an error, try to fix common issues\n",
    "                    # For example, if the date looks like 01-01-0001, it might be a placeholder\n",
    "                    if dob_date.year < 1800:\n",
    "                        return None\n",
    "                \n",
    "                return dob_date.strftime('%Y-%m-%d')\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Handle numeric formats without separators\n",
    "            if dob_str.isdigit():\n",
    "                if len(dob_str) == 8:  # MMDDYYYY\n",
    "                    month = int(dob_str[0:2])\n",
    "                    day = int(dob_str[2:4])\n",
    "                    year = int(dob_str[4:8])\n",
    "                    \n",
    "                    if 1 <= month <= 12 and 1 <= day <= 31:\n",
    "                        return f\"{year:04d}-{month:02d}-{day:02d}\"\n",
    "                \n",
    "                elif len(dob_str) == 6:  # MMDDYY\n",
    "                    month = int(dob_str[0:2])\n",
    "                    day = int(dob_str[2:4])\n",
    "                    year = int(dob_str[4:6])\n",
    "                    \n",
    "                    # Adjust years for two-digit years\n",
    "                    full_year = 1900 + year if year >= 50 else 2000 + year\n",
    "                    \n",
    "                    if 1 <= month <= 12 and 1 <= day <= 31:\n",
    "                        return f\"{full_year:04d}-{month:02d}-{day:02d}\"\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # If all parsing attempts fail, return None\n",
    "        return None\n",
    "    \n",
    "    # Select the correct columns based on the debugging results\n",
    "    pre_email_col = 'email_address'\n",
    "    post_email_col = 'email_address'\n",
    "    pre_fullname_col = 'full_name'\n",
    "    post_fullname_col = 'full_name'\n",
    "    pre_dob_col = 'date_of_birth_standardized'\n",
    "    post_dob_col = 'date_of_birth'\n",
    "    \n",
    "    print(f\"Using pre-enrollment email column: {pre_email_col}\")\n",
    "    print(f\"Using post-course email column: {post_email_col}\")\n",
    "    print(f\"Using pre-enrollment full name column: {pre_fullname_col}\")\n",
    "    print(f\"Using post-course full name column: {post_fullname_col}\")\n",
    "    print(f\"Using pre-enrollment DOB column: {pre_dob_col}\")\n",
    "    print(f\"Using post-course DOB column: {post_dob_col}\")\n",
    "    \n",
    "    # Apply standardization\n",
    "    # Email\n",
    "    if pre_email_col in pre_df.columns:\n",
    "        pre_df['email_original'] = pre_df[pre_email_col]\n",
    "        pre_df['email_standardized'] = pre_df[pre_email_col].apply(standardize_email)\n",
    "        pre_email_changes = sum((pre_df['email_original'] != pre_df['email_standardized']) & \n",
    "                               pre_df['email_standardized'].notna())\n",
    "        print(f\"Standardized {pre_email_changes} pre-enrollment emails\")\n",
    "    \n",
    "    if post_email_col in post_df.columns:\n",
    "        post_df['email_original'] = post_df[post_email_col]\n",
    "        post_df['email_standardized'] = post_df[post_email_col].apply(standardize_email)\n",
    "        post_email_changes = sum((post_df['email_original'] != post_df['email_standardized']) & \n",
    "                                post_df['email_standardized'].notna())\n",
    "        print(f\"Standardized {post_email_changes} post-course emails\")\n",
    "    \n",
    "    # Names\n",
    "    if pre_fullname_col in pre_df.columns:\n",
    "        pre_df['name_normalized'] = pre_df[pre_fullname_col].apply(normalize_name)\n",
    "        print(f\"Normalized {sum(pre_df['name_normalized'].notna())} pre-enrollment names\")\n",
    "    \n",
    "    if post_fullname_col in post_df.columns:\n",
    "        post_df['name_normalized'] = post_df[post_fullname_col].apply(normalize_name)\n",
    "        print(f\"Normalized {sum(post_df['name_normalized'].notna())} post-course names\")\n",
    "    \n",
    "    # DOB\n",
    "    if pre_dob_col in pre_df.columns:\n",
    "        pre_df['dob_standardized'] = pre_df[pre_dob_col].apply(standardize_dob)\n",
    "        print(f\"Standardized {sum(pre_df['dob_standardized'].notna())} pre-enrollment DOBs\")\n",
    "    \n",
    "    if post_dob_col in post_df.columns:\n",
    "        post_df['dob_standardized'] = post_df[post_dob_col].apply(standardize_dob)\n",
    "        print(f\"Standardized {sum(post_df['dob_standardized'].notna())} post-course DOBs\")\n",
    "    \n",
    "    # Step 3: Perform matching independently using both methods\n",
    "    print(\"\\n3. Performing matching using multiple identifiers\")\n",
    "    \n",
    "    # 3.1: Email matching\n",
    "    print(\"Matching by standardized email...\")\n",
    "    \n",
    "    email_matches = []\n",
    "    \n",
    "    if 'email_standardized' in pre_df.columns and 'email_standardized' in post_df.columns:\n",
    "        # Create dictionaries for faster lookup\n",
    "        post_email_dict = {}\n",
    "        for idx, row in post_df.iterrows():\n",
    "            email = row['email_standardized']\n",
    "            if pd.notna(email):\n",
    "                if email not in post_email_dict:\n",
    "                    post_email_dict[email] = []\n",
    "                post_email_dict[email].append(idx)\n",
    "        \n",
    "        # Find matches\n",
    "        for pre_idx, pre_row in pre_df.iterrows():\n",
    "            pre_email = pre_row['email_standardized']\n",
    "            \n",
    "            if pd.notna(pre_email) and pre_email in post_email_dict:\n",
    "                for post_idx in post_email_dict[pre_email]:\n",
    "                    email_matches.append({\n",
    "                        'pre_idx': pre_idx,\n",
    "                        'post_idx': post_idx,\n",
    "                        'match_type': 'email'\n",
    "                    })\n",
    "    \n",
    "    print(f\"Found {len(email_matches)} matches by email\")\n",
    "    \n",
    "    # 3.2: Full name + DOB matching (independently, not just for unmatched records)\n",
    "    print(\"Matching by full name + DOB...\")\n",
    "    \n",
    "    name_dob_matches = []\n",
    "    \n",
    "    if ('name_normalized' in pre_df.columns and 'name_normalized' in post_df.columns and\n",
    "        'dob_standardized' in pre_df.columns and 'dob_standardized' in post_df.columns):\n",
    "        \n",
    "        # Create name+DOB keys for both datasets\n",
    "        pre_df['name_dob_key'] = pre_df.apply(\n",
    "            lambda row: f\"{row['name_normalized']}|{row['dob_standardized']}\" \n",
    "            if pd.notna(row['name_normalized']) and pd.notna(row['dob_standardized']) else None, \n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        post_df['name_dob_key'] = post_df.apply(\n",
    "            lambda row: f\"{row['name_normalized']}|{row['dob_standardized']}\" \n",
    "            if pd.notna(row['name_normalized']) and pd.notna(row['dob_standardized']) else None, \n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # Create dictionary for post-course name+DOB\n",
    "        post_name_dob_dict = {}\n",
    "        for idx, row in post_df.iterrows():\n",
    "            key = row['name_dob_key']\n",
    "            if pd.notna(key):\n",
    "                if key not in post_name_dob_dict:\n",
    "                    post_name_dob_dict[key] = []\n",
    "                post_name_dob_dict[key].append(idx)\n",
    "        \n",
    "        # Find matches\n",
    "        for pre_idx, pre_row in pre_df.iterrows():\n",
    "            pre_key = pre_row['name_dob_key']\n",
    "            \n",
    "            if pd.notna(pre_key) and pre_key in post_name_dob_dict:\n",
    "                for post_idx in post_name_dob_dict[pre_key]:\n",
    "                    name_dob_matches.append({\n",
    "                        'pre_idx': pre_idx,\n",
    "                        'post_idx': post_idx,\n",
    "                        'match_type': 'full_name_dob'\n",
    "                    })\n",
    "    \n",
    "    print(f\"Found {len(name_dob_matches)} matches by full name + DOB\")\n",
    "    \n",
    "    # Step 4: Combine all matches\n",
    "    all_matches = email_matches + name_dob_matches\n",
    "    print(f\"Total raw matches (before de-duplication): {len(all_matches)}\")\n",
    "    \n",
    "    # Step 5: Resolve duplicates by creating a unique identifier for each match pair\n",
    "    print(\"\\n4. Resolving duplicate matches\")\n",
    "    \n",
    "    match_df = pd.DataFrame(all_matches)\n",
    "    match_df['match_pair'] = match_df['pre_idx'].astype(str) + '_' + match_df['post_idx'].astype(str)\n",
    "    \n",
    "    # Count duplicate pairs\n",
    "    pair_counts = match_df['match_pair'].value_counts()\n",
    "    duplicate_pairs = pair_counts[pair_counts > 1].index.tolist()\n",
    "    print(f\"Found {len(duplicate_pairs)} duplicate match pairs\")\n",
    "    \n",
    "    # For duplicate pairs, keep the email match (higher confidence)\n",
    "    if duplicate_pairs:\n",
    "        # Sort by match type to prioritize email matches\n",
    "        match_df['match_type_priority'] = match_df['match_type'].map({'email': 1, 'full_name_dob': 2})\n",
    "        match_df = match_df.sort_values(['match_pair', 'match_type_priority'])\n",
    "        \n",
    "        # Keep only the first occurrence of each match pair (which will be email match if exists)\n",
    "        match_df = match_df.drop_duplicates(subset=['match_pair'], keep='first')\n",
    "        \n",
    "        # Drop the temporary column\n",
    "        match_df = match_df.drop(columns=['match_type_priority'])\n",
    "    \n",
    "    print(f\"Unique matches after de-duplication: {len(match_df)}\")\n",
    "    \n",
    "    # Step 6: Count matches by type\n",
    "    email_match_count = sum(match_df['match_type'] == 'email')\n",
    "    name_dob_match_count = sum(match_df['match_type'] == 'full_name_dob')\n",
    "    \n",
    "    print(f\"Final match breakdown:\")\n",
    "    print(f\"  Email matches: {email_match_count}\")\n",
    "    print(f\"  Full name + DOB matches: {name_dob_match_count}\")\n",
    "    print(f\"  Total unique matches: {len(match_df)}\")\n",
    "    \n",
    "    # Step 7: Create merged dataset\n",
    "    print(\"\\n5. Creating merged dataset\")\n",
    "    \n",
    "    # Check for duplicate pre_idx (one pre-enrollment record matched to multiple post-course records)\n",
    "    pre_idx_counts = match_df['pre_idx'].value_counts()\n",
    "    duplicate_pre = pre_idx_counts[pre_idx_counts > 1]\n",
    "    \n",
    "    if len(duplicate_pre) > 0:\n",
    "        print(f\"Warning: {len(duplicate_pre)} pre-enrollment records are matched to multiple post-course records\")\n",
    "        print(\"Keeping only one match per pre-enrollment record\")\n",
    "        \n",
    "        # For each pre_idx with multiple matches, keep only the match with the highest confidence\n",
    "        # (email matches over name+DOB matches)\n",
    "        for pre_idx in duplicate_pre.index:\n",
    "            matches = match_df[match_df['pre_idx'] == pre_idx]\n",
    "            # If there's an email match, keep it; otherwise keep the first name+DOB match\n",
    "            if any(matches['match_type'] == 'email'):\n",
    "                email_match = matches[matches['match_type'] == 'email'].iloc[0]\n",
    "                match_df = match_df[~((match_df['pre_idx'] == pre_idx) & (match_df['match_type'] == 'full_name_dob'))]\n",
    "            else:\n",
    "                # Keep only the first name+DOB match\n",
    "                first_match = matches.iloc[0]\n",
    "                match_df = match_df[~((match_df['pre_idx'] == pre_idx) & (match_df.index != first_match.name))]\n",
    "    \n",
    "    # Perform the merge\n",
    "    merged_data = []\n",
    "    \n",
    "    # Prefix mappings to avoid column name conflicts\n",
    "    pre_prefix_map = {}\n",
    "    post_prefix_map = {}\n",
    "    \n",
    "    # Add prefixes to pre columns\n",
    "    for col in pre_df.columns:\n",
    "        if col.startswith('pre_'):\n",
    "            pre_prefix_map[col] = col\n",
    "        else:\n",
    "            pre_prefix_map[col] = f\"pre_{col}\"\n",
    "    \n",
    "    # Add prefixes to post columns\n",
    "    for col in post_df.columns:\n",
    "        if col.startswith('post_'):\n",
    "            post_prefix_map[col] = col\n",
    "        else:\n",
    "            post_prefix_map[col] = f\"post_{col}\"\n",
    "    \n",
    "    # Merge the data\n",
    "    for _, match in match_df.iterrows():\n",
    "        pre_idx = match['pre_idx']\n",
    "        post_idx = match['post_idx']\n",
    "        \n",
    "        pre_row = pre_df.loc[pre_idx].to_dict()\n",
    "        post_row = post_df.loc[post_idx].to_dict()\n",
    "        \n",
    "        merged_row = {}\n",
    "        \n",
    "        # Add prefixed pre columns\n",
    "        for col, value in pre_row.items():\n",
    "            merged_row[pre_prefix_map[col]] = value\n",
    "        \n",
    "        # Add prefixed post columns\n",
    "        for col, value in post_row.items():\n",
    "            merged_row[post_prefix_map[col]] = value\n",
    "        \n",
    "        # Add match metadata\n",
    "        merged_row['match_type'] = match['match_type']\n",
    "        merged_row['match_confidence'] = 100 if match['match_type'] == 'email' else 90\n",
    "        \n",
    "        merged_data.append(merged_row)\n",
    "    \n",
    "    # Create the final merged dataframe\n",
    "    merged_df = pd.DataFrame(merged_data)\n",
    "    \n",
    "    # Step 8: Quality checking\n",
    "    print(\"\\n6. Performing quality checks on merged dataset\")\n",
    "    \n",
    "    # Age calculation where possible\n",
    "    if ('pre_dob_standardized' in merged_df.columns and 'post_course_year' in merged_df.columns):\n",
    "        def calculate_age(row):\n",
    "            try:\n",
    "                if pd.notna(row['pre_dob_standardized']):\n",
    "                    dob_str = row['pre_dob_standardized']\n",
    "                    dob_year = int(dob_str.split('-')[0])\n",
    "                    course_year = row['post_course_year']\n",
    "                    if pd.notna(course_year):\n",
    "                        age = course_year - dob_year\n",
    "                        # Sanity check for reasonable age\n",
    "                        if 0 <= age <= 120:\n",
    "                            return age\n",
    "                return None\n",
    "            except:\n",
    "                return None\n",
    "        \n",
    "        merged_df['age_at_course'] = merged_df.apply(calculate_age, axis=1)\n",
    "        \n",
    "        # Identify age outliers\n",
    "        age_stats = merged_df['age_at_course'].describe()\n",
    "        print(\"\\nAge at course statistics:\")\n",
    "        print(f\"  Min: {age_stats['min']}\")\n",
    "        print(f\"  Max: {age_stats['max']}\")\n",
    "        print(f\"  Mean: {age_stats['mean']:.1f}\")\n",
    "        \n",
    "        young_outliers = merged_df[merged_df['age_at_course'] < 5].shape[0]\n",
    "        old_outliers = merged_df[merged_df['age_at_course'] > 90].shape[0]\n",
    "        \n",
    "        if young_outliers > 0 or old_outliers > 0:\n",
    "            print(f\"  Found {young_outliers} records with age < 5\")\n",
    "            print(f\"  Found {old_outliers} records with age > 90\")\n",
    "            print(\"  Consider addressing these age outliers separately as mentioned\")\n",
    "    \n",
    "    # Course year consistency\n",
    "    if 'pre_course_year' in merged_df.columns and 'post_course_year' in merged_df.columns:\n",
    "        timeline_issues = sum(merged_df['post_course_year'] < merged_df['pre_course_year'])\n",
    "        \n",
    "        if timeline_issues > 0:\n",
    "            print(f\"\\nFound {timeline_issues} records with pre-course year after post-course year\")\n",
    "            print(\"Consider addressing these timeline issues separately as mentioned\")\n",
    "    \n",
    "    # Step 9: Save the merged dataset\n",
    "    print(f\"\\n7. Saving high-quality merged dataset to {output_file}\")\n",
    "    merged_df.to_excel(output_file, index=False)\n",
    "    \n",
    "    print(\"\\nHigh-quality merged dataset created successfully!\")\n",
    "    print(f\"Total records: {len(merged_df)}\")\n",
    "    print(\"Match breakdown:\")\n",
    "    print(f\"  Email matches: {sum(merged_df['match_type'] == 'email')}\")\n",
    "    print(f\"  Full name + DOB matches: {sum(merged_df['match_type'] == 'full_name_dob')}\")\n",
    "    \n",
    "    # Return the merged dataframe\n",
    "    return merged_df\n",
    "\n",
    "# Run the function with your original files\n",
    "pre_course_path = '/Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/ES/Cleaned & Enhanced ES Data .xlsx'\n",
    "post_course_path = '/Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/PCS/Cleaned_Merged_PCS_2cleaned.xlsx'\n",
    "high_quality_merged_df = create_high_quality_merged_dataset(pre_course_path, post_course_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d0fd18a1-cc8e-4258-ab53-579cbc5282f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold Standard dataset created with 3241 records\n",
      "Saved to: /Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/ES/Gold_Standard_Email_Only_Dataset.xlsx\n"
     ]
    }
   ],
   "source": [
    "#creating gold standard data\n",
    "import pandas as pd\n",
    "\n",
    "# Load the comprehensive dataset\n",
    "comprehensive_path = '/Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/ES/Direct_High_Quality_Merged_Dataset.xlsx'\n",
    "comprehensive_df = pd.read_excel(comprehensive_path)\n",
    "\n",
    "# Create the Gold Standard dataset (email matches only)\n",
    "email_only_df = comprehensive_df[comprehensive_df['match_type'] == 'email']\n",
    "\n",
    "# Save to a new Excel file\n",
    "email_only_path = '/Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/ES/Gold_Standard_Email_Only_Dataset.xlsx'\n",
    "email_only_df.to_excel(email_only_path, index=False)\n",
    "\n",
    "print(f\"Gold Standard dataset created with {len(email_only_df)} records\")\n",
    "print(f\"Saved to: {email_only_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb7ee504-d18c-4dd6-8d79-14b5ced86766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-docx\n",
      "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.12/site-packages (3.8.4)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from python-docx) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from python-docx) (4.12.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.21 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: python-docx\n",
      "Successfully installed python-docx-1.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-docx matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cd828fb7-b3af-4903-9df1-5f59081528d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentation created and saved to: /Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/ES/Data_Matching_Methodology.docx\n"
     ]
    }
   ],
   "source": [
    "#Creating documentation\n",
    "from docx import Document\n",
    "from docx.shared import Pt, Inches, RGBColor\n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Create output directory for documentation\n",
    "output_dir = os.path.dirname(comprehensive_path)\n",
    "doc_path = os.path.join(output_dir, \"Data_Matching_Methodology.docx\")\n",
    "\n",
    "# Create visualizations for the documentation\n",
    "viz_dir = os.path.join(output_dir, \"Visualizations\")\n",
    "os.makedirs(viz_dir, exist_ok=True)\n",
    "\n",
    "# Create a visualization of the match types\n",
    "match_counts = comprehensive_df['match_type'].value_counts()\n",
    "labels = match_counts.index\n",
    "sizes = match_counts.values\n",
    "colors = ['#3498db', '#2ecc71']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "plt.axis('equal')\n",
    "plt.title('Distribution of Match Types in Comprehensive Dataset')\n",
    "pie_chart_path = os.path.join(viz_dir, \"match_type_distribution.png\")\n",
    "plt.savefig(pie_chart_path, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Create a bar chart comparing dataset sizes\n",
    "datasets = ['Pre-enrollment', 'Post-course', 'Comprehensive Matches', 'Gold Standard Matches']\n",
    "counts = [29700, 7806, len(comprehensive_df), len(email_only_df)]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(datasets, counts, color=['#3498db', '#2ecc71', '#9b59b6', '#f1c40f'])\n",
    "\n",
    "# Add count labels on top of each bar\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "             f'{int(height):,}', ha='center', va='bottom', fontsize=11)\n",
    "\n",
    "plt.title('Dataset Sizes')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "dataset_chart_path = os.path.join(viz_dir, \"dataset_sizes.png\")\n",
    "plt.savefig(dataset_chart_path, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Create the Word document\n",
    "doc = Document()\n",
    "\n",
    "# Title\n",
    "title = doc.add_heading('Data Processing and Matching Methodology', 0)\n",
    "title.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "\n",
    "# Introduction\n",
    "doc.add_heading('1. Introduction', 1)\n",
    "p = doc.add_paragraph()\n",
    "p.add_run('This document details the methodology used to process and match pre-enrollment and post-course survey data for analysis. Two datasets were created:')\n",
    "p.add_run().add_break()\n",
    "p.add_run('• Comprehensive Dataset: ').bold = True\n",
    "p.add_run(f'Contains {len(comprehensive_df):,} records matched using both email and full name + date of birth')\n",
    "p.add_run().add_break()\n",
    "p.add_run('• Gold Standard Dataset: ').bold = True\n",
    "p.add_run(f'Contains {len(email_only_df):,} records matched exclusively using email addresses')\n",
    "\n",
    "# Original Data Overview\n",
    "doc.add_heading('2. Original Data Overview', 1)\n",
    "p = doc.add_paragraph()\n",
    "p.add_run('The original data consisted of two separate datasets:')\n",
    "p.add_run().add_break()\n",
    "p.add_run('• Pre-enrollment Survey: ').bold = True\n",
    "p.add_run(f'{29700:,} records')\n",
    "p.add_run().add_break()\n",
    "p.add_run('• Post-course Survey: ').bold = True\n",
    "p.add_run(f'{7806:,} records')\n",
    "\n",
    "p = doc.add_paragraph()\n",
    "p.add_run('Key data quality issues identified in the original datasets:')\n",
    "doc.add_paragraph('• Inconsistent email formats and domains', style='List Bullet')\n",
    "doc.add_paragraph('• Varied name formatting (capitalization, special characters)', style='List Bullet')\n",
    "doc.add_paragraph('• Inconsistent date of birth formats', style='List Bullet')\n",
    "doc.add_paragraph('• Duplicate email addresses', style='List Bullet')\n",
    "doc.add_paragraph('• Timeline inconsistencies between pre and post course dates', style='List Bullet')\n",
    "\n",
    "# Data Standardization\n",
    "doc.add_heading('3. Data Standardization Process', 1)\n",
    "\n",
    "# Email standardization\n",
    "doc.add_heading('3.1 Email Standardization', 2)\n",
    "p = doc.add_paragraph()\n",
    "p.add_run('The email standardization process involved:')\n",
    "doc.add_paragraph('• Converting all emails to lowercase', style='List Bullet')\n",
    "doc.add_paragraph('• Removing spaces', style='List Bullet')\n",
    "doc.add_paragraph('• Correcting common domain misspellings (e.g., \"gmail.com\" vs \"gmai.com\")', style='List Bullet')\n",
    "doc.add_paragraph('• Validating basic email format (must contain @ and .)', style='List Bullet')\n",
    "\n",
    "p = doc.add_paragraph()\n",
    "p.add_run('Results of email standardization:')\n",
    "doc.add_paragraph(f'• {281} pre-enrollment emails standardized', style='List Bullet')\n",
    "doc.add_paragraph(f'• {51} post-course emails standardized', style='List Bullet')\n",
    "doc.add_paragraph('• Common corrections included fixing variations of gmail.com, yahoo.com, and hotmail.com', style='List Bullet')\n",
    "\n",
    "# Name standardization\n",
    "doc.add_heading('3.2 Name Standardization', 2)\n",
    "p = doc.add_paragraph()\n",
    "p.add_run('The name standardization process involved:')\n",
    "doc.add_paragraph('• Converting all names to lowercase', style='List Bullet')\n",
    "doc.add_paragraph('• Removing extra spaces', style='List Bullet')\n",
    "doc.add_paragraph('• Removing non-alphabetic characters', style='List Bullet')\n",
    "doc.add_paragraph('• Standardizing spacing between name parts', style='List Bullet')\n",
    "\n",
    "p = doc.add_paragraph()\n",
    "p.add_run('Results of name standardization:')\n",
    "doc.add_paragraph(f'• {29614} pre-enrollment names normalized', style='List Bullet')\n",
    "doc.add_paragraph(f'• {7749} post-course names normalized', style='List Bullet')\n",
    "\n",
    "# DOB standardization\n",
    "doc.add_heading('3.3 Date of Birth Standardization', 2)\n",
    "p = doc.add_paragraph()\n",
    "p.add_run('The date of birth standardization process involved:')\n",
    "doc.add_paragraph('• Converting all dates to YYYY-MM-DD format', style='List Bullet')\n",
    "doc.add_paragraph('• Handling various input formats (MM/DD/YYYY, MM-DD-YYYY, etc.)', style='List Bullet')\n",
    "doc.add_paragraph('• Properly interpreting two-digit years (e.g., \"74\" as 1974)', style='List Bullet')\n",
    "doc.add_paragraph('• Filtering out implausible dates (e.g., birth years before 1900)', style='List Bullet')\n",
    "\n",
    "p = doc.add_paragraph()\n",
    "p.add_run('Results of date of birth standardization:')\n",
    "doc.add_paragraph(f'• {29583} pre-enrollment DOBs standardized', style='List Bullet')\n",
    "doc.add_paragraph(f'• {7708} post-course DOBs standardized', style='List Bullet')\n",
    "\n",
    "# Matching Methodology\n",
    "doc.add_heading('4. Matching Methodology', 1)\n",
    "\n",
    "# Email matching\n",
    "doc.add_heading('4.1 Email Matching', 2)\n",
    "p = doc.add_paragraph()\n",
    "p.add_run('Email matching process:')\n",
    "doc.add_paragraph('• Used standardized email addresses as the primary identifier', style='List Bullet')\n",
    "doc.add_paragraph('• Created an exact match between pre-enrollment and post-course records', style='List Bullet')\n",
    "doc.add_paragraph('• Assigned 100% confidence to email matches', style='List Bullet')\n",
    "\n",
    "p = doc.add_paragraph()\n",
    "p.add_run('Results of email matching:')\n",
    "doc.add_paragraph(f'• {3241} unique matches identified by email', style='List Bullet')\n",
    "\n",
    "# Name+DOB matching\n",
    "doc.add_heading('4.2 Full Name + DOB Matching', 2)\n",
    "p = doc.add_paragraph()\n",
    "p.add_run('Full name + DOB matching process:')\n",
    "doc.add_paragraph('• Used normalized full names combined with standardized date of birth', style='List Bullet')\n",
    "doc.add_paragraph('• Required exact match on both name and DOB', style='List Bullet')\n",
    "doc.add_paragraph('• Assigned 90% confidence to name+DOB matches', style='List Bullet')\n",
    "\n",
    "p = doc.add_paragraph()\n",
    "p.add_run('Results of name+DOB matching:')\n",
    "doc.add_paragraph(f'• {5314} initial matches identified by name+DOB', style='List Bullet')\n",
    "doc.add_paragraph(f'• {2854} unique matches by name+DOB after deduplication', style='List Bullet')\n",
    "doc.add_paragraph(f'• Final count: {2726} name+DOB matches in the comprehensive dataset', style='List Bullet')\n",
    "\n",
    "# Deduplication\n",
    "doc.add_heading('4.3 Deduplication Process', 2)\n",
    "p = doc.add_paragraph()\n",
    "p.add_run('The deduplication process involved:')\n",
    "doc.add_paragraph('• Identifying records matched by both email and name+DOB', style='List Bullet')\n",
    "doc.add_paragraph('• Prioritizing email matches (higher confidence) over name+DOB matches', style='List Bullet')\n",
    "doc.add_paragraph('• Ensuring each pre-enrollment record was matched to only one post-course record', style='List Bullet')\n",
    "\n",
    "p = doc.add_paragraph()\n",
    "p.add_run('Deduplication results:')\n",
    "doc.add_paragraph(f'• {8555} total raw matches before deduplication', style='List Bullet')\n",
    "doc.add_paragraph(f'• {2460} duplicate match pairs removed', style='List Bullet')\n",
    "doc.add_paragraph(f'• {123} pre-enrollment records matched to multiple post-course records (kept highest quality match)', style='List Bullet')\n",
    "doc.add_paragraph(f'• {6095} unique matches after initial deduplication', style='List Bullet')\n",
    "doc.add_paragraph(f'• {5967} final matches after resolving all duplications', style='List Bullet')\n",
    "\n",
    "# Final Datasets\n",
    "doc.add_heading('5. Final Datasets', 1)\n",
    "\n",
    "# Comprehensive Dataset\n",
    "doc.add_heading('5.1 Comprehensive Dataset', 2)\n",
    "p = doc.add_paragraph()\n",
    "p.add_run('Comprehensive Dataset characteristics:')\n",
    "doc.add_paragraph(f'• Total records: {len(comprehensive_df):,}', style='List Bullet')\n",
    "doc.add_paragraph(f'• Email matches: {sum(comprehensive_df[\"match_type\"] == \"email\"):,} ({sum(comprehensive_df[\"match_type\"] == \"email\")/len(comprehensive_df)*100:.1f}%)', style='List Bullet')\n",
    "doc.add_paragraph(f'• Name+DOB matches: {sum(comprehensive_df[\"match_type\"] == \"full_name_dob\"):,} ({sum(comprehensive_df[\"match_type\"] == \"full_name_dob\")/len(comprehensive_df)*100:.1f}%)', style='List Bullet')\n",
    "doc.add_paragraph('• Includes both high and very high confidence matches', style='List Bullet')\n",
    "doc.add_paragraph('• Provides maximum sample size while maintaining high reliability', style='List Bullet')\n",
    "doc.add_paragraph('• Recommended for most analytical purposes', style='List Bullet')\n",
    "\n",
    "# Gold Standard Dataset\n",
    "doc.add_heading('5.2 Gold Standard Dataset', 2)\n",
    "p = doc.add_paragraph()\n",
    "p.add_run('Gold Standard Dataset characteristics:')\n",
    "doc.add_paragraph(f'• Total records: {len(email_only_df):,}', style='List Bullet')\n",
    "doc.add_paragraph('• Email matches only (100% confidence)', style='List Bullet')\n",
    "doc.add_paragraph('• Highest level of confidence in matches', style='List Bullet')\n",
    "doc.add_paragraph('• Recommended for analyses requiring absolute certainty in matches', style='List Bullet')\n",
    "doc.add_paragraph('• Can be used for validation of findings from the Comprehensive Dataset', style='List Bullet')\n",
    "\n",
    "# Data Quality Issues\n",
    "doc.add_heading('6. Remaining Data Quality Issues', 1)\n",
    "\n",
    "p = doc.add_paragraph()\n",
    "p.add_run('The following quality issues were identified in the final datasets:')\n",
    "\n",
    "# Age outliers\n",
    "doc.add_heading('6.1 Age Outliers', 2)\n",
    "p = doc.add_paragraph()\n",
    "p.add_run('Age calculation was performed based on birth year and course year:')\n",
    "doc.add_paragraph(f'• Age range: {0} to {93} years', style='List Bullet')\n",
    "doc.add_paragraph(f'• Mean age: {44.4:.1f} years', style='List Bullet')\n",
    "doc.add_paragraph(f'• {19} records with age < 5 years', style='List Bullet')\n",
    "doc.add_paragraph(f'• {7} records with age > 90 years', style='List Bullet')\n",
    "p = doc.add_paragraph()\n",
    "p.add_run('Recommendation: ').bold = True\n",
    "p.add_run('These age outliers should be reviewed and potentially excluded from age-dependent analyses.')\n",
    "\n",
    "# Timeline inconsistencies\n",
    "doc.add_heading('6.2 Timeline Inconsistencies', 2)\n",
    "p = doc.add_paragraph()\n",
    "p.add_run('Timeline inconsistencies between pre-enrollment and post-course dates:')\n",
    "doc.add_paragraph(f'• {90} records where pre-course year is after post-course year', style='List Bullet')\n",
    "p = doc.add_paragraph()\n",
    "p.add_run('Recommendation: ').bold = True\n",
    "p.add_run('These records should be reviewed and potentially excluded from time-sensitive analyses.')\n",
    "\n",
    "# Add visualizations\n",
    "doc.add_heading('7. Visualizations', 1)\n",
    "\n",
    "# Add match type distribution chart\n",
    "doc.add_heading('7.1 Match Type Distribution', 2)\n",
    "doc.add_paragraph('The following chart shows the distribution of match types in the Comprehensive Dataset:')\n",
    "doc.add_picture(pie_chart_path, width=Inches(6))\n",
    "p = doc.add_paragraph()\n",
    "p.add_run(f'Email Matches: {sum(comprehensive_df[\"match_type\"] == \"email\"):,} ({sum(comprehensive_df[\"match_type\"] == \"email\")/len(comprehensive_df)*100:.1f}%)')\n",
    "p.add_run().add_break()\n",
    "p.add_run(f'Name+DOB Matches: {sum(comprehensive_df[\"match_type\"] == \"full_name_dob\"):,} ({sum(comprehensive_df[\"match_type\"] == \"full_name_dob\")/len(comprehensive_df)*100:.1f}%)')\n",
    "\n",
    "# Add dataset size comparison chart\n",
    "doc.add_heading('7.2 Dataset Size Comparison', 2)\n",
    "doc.add_paragraph('The following chart compares the sizes of the original and matched datasets:')\n",
    "doc.add_picture(dataset_chart_path, width=Inches(6))\n",
    "\n",
    "# Recommendations\n",
    "doc.add_heading('8. Recommendations for Analysis', 1)\n",
    "p = doc.add_paragraph()\n",
    "p.add_run('Based on the data quality assessment, the following recommendations are provided:')\n",
    "\n",
    "doc.add_paragraph('• Use the Comprehensive Dataset for most analyses to maximize statistical power while maintaining reliability.', style='List Bullet')\n",
    "doc.add_paragraph('• Use the Gold Standard Dataset for analyses requiring absolute certainty in matches or as validation for sensitive findings.', style='List Bullet')\n",
    "doc.add_paragraph('• Consider excluding records with timeline inconsistencies from analyses that depend on temporal sequence.', style='List Bullet')\n",
    "doc.add_paragraph('• Review and potentially exclude age outliers from analyses where age is a factor.', style='List Bullet')\n",
    "doc.add_paragraph('• Document which dataset was used when reporting results.', style='List Bullet')\n",
    "doc.add_paragraph('• Analyze key metrics using both datasets to ensure robustness of findings.', style='List Bullet')\n",
    "\n",
    "# Save the document\n",
    "doc.save(doc_path)\n",
    "print(f\"Documentation created and saved to: {doc_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8122c6c-1266-469d-b127-2d2ca365fa68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "248bcfc3-a135-48f4-92f9-761b1714d40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardizing original pre-enrollment and post-course datasets\n",
      "\n",
      "1. Loading datasets\n",
      "Loading pre-enrollment data from: /Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/ES/Cleaned & Enhanced ES Data .xlsx\n",
      "Loaded 29700 pre-enrollment records\n",
      "Loading post-course data from: /Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/PCS/Cleaned_Merged_PCS_2cleaned.xlsx\n",
      "Loaded 7806 post-course records\n",
      "\n",
      "2. Defining standardization functions\n",
      "\n",
      "3. Standardizing pre-enrollment dataset\n",
      "Standardized 281 pre-enrollment emails\n",
      "Standardized 25975 pre-enrollment phone numbers\n",
      "Standardized 29610 pre-enrollment ZIP codes\n",
      "Calculated age at course for 19237 pre-enrollment records\n",
      "  Age range: 0.0 to 95.0 years\n",
      "  Mean age: 43.0 years\n",
      "  Found 74 records with age < 5 years\n",
      "  Found 26 records with age > 90 years\n",
      "\n",
      "4. Standardizing post-course dataset\n",
      "Standardized 51 post-course emails\n",
      "\n",
      "5. Saving standardized datasets\n",
      "Saved standardized pre-enrollment dataset to: /Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/ES/Standardized_Pre_Enrollment_Data.xlsx\n",
      "Saved standardized post-course dataset to: /Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/ES/Standardized_Post_Course_Data.xlsx\n",
      "\n",
      "Standardization complete!\n"
     ]
    }
   ],
   "source": [
    "#Final Standardization of Pre enrollment and Post Course\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "def standardize_original_datasets(pre_course_path, post_course_path):\n",
    "    \"\"\"\n",
    "    Standardize the original pre-enrollment and post-course datasets.\n",
    "    \n",
    "    Parameters:\n",
    "    pre_course_path (str): Path to the pre-enrollment Excel file\n",
    "    post_course_path (str): Path to the post-course Excel file\n",
    "    \"\"\"\n",
    "    print(\"Standardizing original pre-enrollment and post-course datasets\")\n",
    "    \n",
    "    # Step 1: Load the datasets\n",
    "    print(\"\\n1. Loading datasets\")\n",
    "    print(f\"Loading pre-enrollment data from: {pre_course_path}\")\n",
    "    pre_df = pd.read_excel(pre_course_path)\n",
    "    print(f\"Loaded {len(pre_df)} pre-enrollment records\")\n",
    "    \n",
    "    print(f\"Loading post-course data from: {post_course_path}\")\n",
    "    post_df = pd.read_excel(post_course_path)\n",
    "    print(f\"Loaded {len(post_df)} post-course records\")\n",
    "    \n",
    "    # Create output file paths\n",
    "    output_dir = os.path.dirname(pre_course_path)\n",
    "    pre_output_file = os.path.join(output_dir, \"Standardized_Pre_Enrollment_Data.xlsx\")\n",
    "    post_output_file = os.path.join(output_dir, \"Standardized_Post_Course_Data.xlsx\")\n",
    "    \n",
    "    # Step 2: Define standardization functions\n",
    "    print(\"\\n2. Defining standardization functions\")\n",
    "    \n",
    "    # Domain corrections dictionary\n",
    "    domain_corrections = {\n",
    "        # Gmail variations\n",
    "        'gmai.com': 'gmail.com',\n",
    "        'gmil.com': 'gmail.com',\n",
    "        'gmal.com': 'gmail.com',\n",
    "        'gamail.com': 'gmail.com',\n",
    "        'gimail.com': 'gmail.com',\n",
    "        'gmaill.com': 'gmail.com',\n",
    "        'gmail.comm': 'gmail.com',\n",
    "        'gmial.com': 'gmail.com',\n",
    "        'gmail.cm': 'gmail.com',\n",
    "        'gmail.org': 'gmail.com',\n",
    "        'gemail.com': 'gmail.com',\n",
    "        'gmail.com.com': 'gmail.com',\n",
    "        'gmai.coml': 'gmail.com',\n",
    "        'gmail.coom': 'gmail.com',\n",
    "        'jmail.com': 'gmail.com',\n",
    "        'gmeil.com': 'gmail.com',\n",
    "        'gmall.com': 'gmail.com',\n",
    "        'gail.com': 'gmail.com',\n",
    "        'gmail.co': 'gmail.com',\n",
    "        'gmaiil.com': 'gmail.com',\n",
    "        'gmaim.com': 'gmail.com',\n",
    "        'gmail.om': 'gmail.com',\n",
    "        'gmail.ccom': 'gmail.com',\n",
    "        'g.com': 'gmail.com',\n",
    "        'gmail.vom': 'gmail.com',\n",
    "        'gmali.com': 'gmail.com',\n",
    "        'gmaio.com': 'gmail.com',\n",
    "        'g-mail.com': 'gmail.com',\n",
    "        'gamill.com': 'gmail.com',\n",
    "        'gmaol.com': 'gmail.com',\n",
    "        'gmsil.com': 'gmail.com',\n",
    "        'gmaii.com': 'gmail.com',\n",
    "        'gmail.oeg': 'gmail.com',\n",
    "        'gmanil.com': 'gmail.com',\n",
    "        'gmaili.com': 'gmail.com',\n",
    "        'gmail.crom': 'gmail.com',\n",
    "        'gmail.com11': 'gmail.com',\n",
    "        'gmill.com': 'gmail.com',\n",
    "        'gmail.com3': 'gmail.com',\n",
    "        'gnmail.com': 'gmail.com',\n",
    "        'gmaij.com': 'gmail.com',\n",
    "        'gmail.coml.com': 'gmail.com',\n",
    "        \n",
    "        # Yahoo variations\n",
    "        'yahoo.con': 'yahoo.com',\n",
    "        'yaoo.com': 'yahoo.com',\n",
    "        'hayoo.com': 'yahoo.com',\n",
    "        'yhoo.com': 'yahoo.com',\n",
    "        'yahoo.comm': 'yahoo.com',\n",
    "        'myyahoo.com': 'yahoo.com',\n",
    "        'yahoo.comb': 'yahoo.com',\n",
    "        'yahool.com': 'yahoo.com',\n",
    "        'yaho.cm': 'yahoo.com',\n",
    "        'yahio.un': 'yahoo.com',\n",
    "        \n",
    "        # Hotmail variations\n",
    "        'hmail.com': 'hotmail.com',\n",
    "        'hotmai.com': 'hotmail.com',\n",
    "        'hotamil.com': 'hotmail.com',\n",
    "        'hotmail.con': 'hotmail.com',\n",
    "        'hoo.com': 'hotmail.com',\n",
    "        'hotmiail.com': 'hotmail.com',\n",
    "        'htomail.com': 'hotmail.com',\n",
    "        'hiotmail.com': 'hotmail.com',\n",
    "        'hitmail.com': 'hotmail.com',\n",
    "        \n",
    "        # iCloud variations\n",
    "        'icould.com': 'icloud.com',\n",
    "        'iclod.com': 'icloud.com',\n",
    "        'cloud.com': 'icloud.com',\n",
    "        'icoud.com': 'icloud.com',\n",
    "        'ichoud.com': 'icloud.com',\n",
    "        'iclou.com': 'icloud.com',\n",
    "        \n",
    "        # AOL variations\n",
    "        'ao.com': 'aol.com',\n",
    "        'alo.com': 'aol.com',\n",
    "        'aol.co': 'aol.com',\n",
    "        'aol.om': 'aol.com',\n",
    "        \n",
    "        # Boston Public Schools variations\n",
    "        'bostonk12.com': 'bostonk12.org',\n",
    "        'boston12.org': 'bostonk12.org',\n",
    "        'bostobpublicschools.org': 'bostonpublicschools.org',\n",
    "        'bostonnpublicschools.org': 'bostonpublicschools.org',\n",
    "        'bostonnk12.org': 'bostonk12.org',\n",
    "        'bostonpulicschools.org': 'bostonpublicschools.org',\n",
    "        'bostonpublicschool.org.org': 'bostonpublicschools.org',\n",
    "        \n",
    "        # Other common corrections\n",
    "        'live.con': 'live.com',\n",
    "        'live.cm': 'live.com',\n",
    "        'outlook.pt': 'outlook.com',\n",
    "        'verizon.com': 'verizon.net',\n",
    "        'comcast.com': 'comcast.net',\n",
    "    }\n",
    "    \n",
    "    # Email standardization\n",
    "    def standardize_email(email):\n",
    "        if pd.isna(email):\n",
    "            return None\n",
    "        \n",
    "        # Convert to string, lowercase, and strip spaces\n",
    "        email_str = str(email).lower().strip()\n",
    "        \n",
    "        # Remove any spaces\n",
    "        email_str = email_str.replace(' ', '')\n",
    "        \n",
    "        # Basic validation\n",
    "        if '@' not in email_str or '.' not in email_str:\n",
    "            return None\n",
    "        \n",
    "        # Extract username and domain\n",
    "        username, domain = email_str.split('@', 1)\n",
    "        \n",
    "        # Apply domain correction\n",
    "        corrected_domain = domain_corrections.get(domain, domain)\n",
    "        \n",
    "        # Rebuild corrected email\n",
    "        return f\"{username}@{corrected_domain}\"\n",
    "    \n",
    "    # Phone number standardization\n",
    "    def standardize_phone(phone):\n",
    "        if pd.isna(phone):\n",
    "            return None\n",
    "        \n",
    "        # Convert to string and remove non-digit characters\n",
    "        phone_str = re.sub(r'\\D', '', str(phone))\n",
    "        \n",
    "        # Ensure it's a valid US phone (10 digits)\n",
    "        if len(phone_str) == 10:\n",
    "            # Format as XXX-XXX-XXXX\n",
    "            return f\"{phone_str[:3]}-{phone_str[3:6]}-{phone_str[6:]}\"\n",
    "        elif len(phone_str) == 11 and phone_str[0] == '1':\n",
    "            # Remove country code and format\n",
    "            return f\"{phone_str[1:4]}-{phone_str[4:7]}-{phone_str[7:]}\"\n",
    "        else:\n",
    "            # If format is unclear, return cleaned digits\n",
    "            return phone_str\n",
    "    \n",
    "    # DOB standardization for age calculation\n",
    "    def standardize_dob(dob):\n",
    "        if pd.isna(dob):\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # If it's already a datetime\n",
    "            if isinstance(dob, (pd.Timestamp, datetime)):\n",
    "                return dob\n",
    "            \n",
    "            # Convert to string\n",
    "            dob_str = str(dob).strip()\n",
    "            \n",
    "            # Try parsing with pandas\n",
    "            try:\n",
    "                dob_date = pd.to_datetime(dob_str)\n",
    "                \n",
    "                # Adjust years for two-digit years (19xx vs 20xx)\n",
    "                if dob_date.year > datetime.now().year and dob_date.year < 2100:\n",
    "                    dob_date = dob_date.replace(year=dob_date.year - 100)\n",
    "                \n",
    "                # Sanity check for very old birth years\n",
    "                if dob_date.year < 1900:\n",
    "                    if dob_date.year < 1800:\n",
    "                        return None\n",
    "                \n",
    "                return dob_date\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Try specialized formats\n",
    "            # Handle MM-DD-YYYY format common in your data\n",
    "            if '-' in dob_str:\n",
    "                parts = dob_str.split('-')\n",
    "                if len(parts) == 3:\n",
    "                    try:\n",
    "                        month, day, year = parts\n",
    "                        month = int(month)\n",
    "                        day = int(day)\n",
    "                        year = int(year)\n",
    "                        \n",
    "                        # Adjust if it's a two-digit year\n",
    "                        if year < 100:\n",
    "                            year = 1900 + year if year >= 50 else 2000 + year\n",
    "                            \n",
    "                        return pd.Timestamp(year=year, month=month, day=day)\n",
    "                    except:\n",
    "                        pass\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # If we can't parse it properly\n",
    "        return None\n",
    "    \n",
    "    # Function to calculate age at course\n",
    "    def calculate_age_at_course(dob, course_year):\n",
    "        if pd.isna(dob) or pd.isna(course_year):\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Ensure dob is a datetime\n",
    "            if isinstance(dob, str):\n",
    "                dob = standardize_dob(dob)\n",
    "            \n",
    "            if dob is None:\n",
    "                return None\n",
    "                \n",
    "            # Calculate age as of the course year (December 31st)\n",
    "            course_date = pd.Timestamp(year=int(course_year), month=12, day=31)\n",
    "            age = course_date.year - dob.year\n",
    "            \n",
    "            # Adjust if birthday hasn't occurred yet that year\n",
    "            if course_date.month < dob.month or (course_date.month == dob.month and course_date.day < dob.day):\n",
    "                age -= 1\n",
    "                \n",
    "            # Sanity check for reasonable age\n",
    "            if 0 <= age <= 120:\n",
    "                return age\n",
    "                \n",
    "            return None\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    # ZIP code standardization\n",
    "    def standardize_zip(zip_code):\n",
    "        if pd.isna(zip_code):\n",
    "            return None\n",
    "            \n",
    "        # Convert to string and remove non-digits\n",
    "        zip_str = re.sub(r'\\D', '', str(zip_code))\n",
    "        \n",
    "        # Handle 5-digit ZIP codes\n",
    "        if len(zip_str) >= 5:\n",
    "            return zip_str[:5]  # Keep only first 5 digits\n",
    "        \n",
    "        # If less than 5 digits, pad with leading zeros\n",
    "        elif len(zip_str) > 0:\n",
    "            return zip_str.zfill(5)\n",
    "            \n",
    "        return None\n",
    "    \n",
    "    # Step 3: Apply standardizations to pre-enrollment dataset\n",
    "    print(\"\\n3. Standardizing pre-enrollment dataset\")\n",
    "    \n",
    "    # Email standardization\n",
    "    if 'email_address' in pre_df.columns:\n",
    "        pre_df['email_address_original'] = pre_df['email_address'].copy()\n",
    "        pre_df['email_address'] = pre_df['email_address'].apply(standardize_email)\n",
    "        pre_email_changes = sum((pre_df['email_address'] != pre_df['email_address_original']) & \n",
    "                               pre_df['email_address'].notna())\n",
    "        print(f\"Standardized {pre_email_changes} pre-enrollment emails\")\n",
    "    \n",
    "    # Phone number standardization\n",
    "    if 'phone_number' in pre_df.columns:\n",
    "        pre_df['phone_number_original'] = pre_df['phone_number'].copy()\n",
    "        pre_df['phone_number'] = pre_df['phone_number'].apply(standardize_phone)\n",
    "        pre_phone_changes = sum((pre_df['phone_number'] != pre_df['phone_number_original']) & \n",
    "                               pre_df['phone_number'].notna())\n",
    "        print(f\"Standardized {pre_phone_changes} pre-enrollment phone numbers\")\n",
    "    \n",
    "    # ZIP code standardization (added as a bonus)\n",
    "    if 'zip_code_fixed' in pre_df.columns:\n",
    "        pre_df['zip_code_original'] = pre_df['zip_code_fixed'].copy()\n",
    "        pre_df['zip_code_fixed'] = pre_df['zip_code_fixed'].apply(standardize_zip)\n",
    "        pre_zip_changes = sum((pre_df['zip_code_fixed'] != pre_df['zip_code_original']) & \n",
    "                             pre_df['zip_code_fixed'].notna())\n",
    "        print(f\"Standardized {pre_zip_changes} pre-enrollment ZIP codes\")\n",
    "    \n",
    "    # Age at course calculation\n",
    "    if 'date_of_birth_standardized' in pre_df.columns and 'course_year' in pre_df.columns:\n",
    "        pre_df['date_of_birth_datetime'] = pre_df['date_of_birth_standardized'].apply(standardize_dob)\n",
    "        pre_df['age_at_course'] = pre_df.apply(\n",
    "            lambda row: calculate_age_at_course(row['date_of_birth_datetime'], row['course_year']), \n",
    "            axis=1\n",
    "        )\n",
    "        age_count = pre_df['age_at_course'].notna().sum()\n",
    "        print(f\"Calculated age at course for {age_count} pre-enrollment records\")\n",
    "        \n",
    "        # Report on age distribution\n",
    "        if age_count > 0:\n",
    "            age_stats = pre_df['age_at_course'].describe()\n",
    "            print(f\"  Age range: {age_stats['min']} to {age_stats['max']} years\")\n",
    "            print(f\"  Mean age: {age_stats['mean']:.1f} years\")\n",
    "            \n",
    "            # Flag potential age outliers\n",
    "            young_outliers = sum(pre_df['age_at_course'] < 5)\n",
    "            old_outliers = sum(pre_df['age_at_course'] > 90)\n",
    "            if young_outliers > 0 or old_outliers > 0:\n",
    "                print(f\"  Found {young_outliers} records with age < 5 years\")\n",
    "                print(f\"  Found {old_outliers} records with age > 90 years\")\n",
    "    \n",
    "    # Step 4: Apply standardizations to post-course dataset\n",
    "    print(\"\\n4. Standardizing post-course dataset\")\n",
    "    \n",
    "    # Email standardization\n",
    "    if 'email_address' in post_df.columns:\n",
    "        post_df['email_address_original'] = post_df['email_address'].copy()\n",
    "        post_df['email_address'] = post_df['email_address'].apply(standardize_email)\n",
    "        post_email_changes = sum((post_df['email_address'] != post_df['email_address_original']) & \n",
    "                                post_df['email_address'].notna())\n",
    "        print(f\"Standardized {post_email_changes} post-course emails\")\n",
    "    \n",
    "    # Optional: Additional post-course standardizations\n",
    "    \n",
    "    # Step 5: Save standardized datasets\n",
    "    print(f\"\\n5. Saving standardized datasets\")\n",
    "    \n",
    "    # Save pre-enrollment dataset\n",
    "    pre_df.to_excel(pre_output_file, index=False)\n",
    "    print(f\"Saved standardized pre-enrollment dataset to: {pre_output_file}\")\n",
    "    \n",
    "    # Save post-course dataset\n",
    "    post_df.to_excel(post_output_file, index=False)\n",
    "    print(f\"Saved standardized post-course dataset to: {post_output_file}\")\n",
    "    \n",
    "    print(\"\\nStandardization complete!\")\n",
    "    \n",
    "    return {\n",
    "        'pre_df': pre_df,\n",
    "        'post_df': post_df,\n",
    "        'pre_output_file': pre_output_file,\n",
    "        'post_output_file': post_output_file\n",
    "    }\n",
    "\n",
    "# Run the standardization\n",
    "pre_course_path = '/Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/ES/Cleaned & Enhanced ES Data .xlsx'\n",
    "post_course_path = '/Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/PCS/Cleaned_Merged_PCS_2cleaned.xlsx'\n",
    "results = standardize_original_datasets(pre_course_path, post_course_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02ea5d3-e043-45a6-874f-4631ba4f253a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75add09-7cdf-4844-bd89-6d4feb5ccbab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98118015-cc1f-43ae-a49e-f1c472d4519d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing course years in: /Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/Matched Data/Comprehensive_ES:PCS_matched.xlsx\n",
      "Loaded 5967 records\n",
      "Using column 'post_course_year' for course year analysis\n",
      "\n",
      "Course Year Distribution:\n",
      "--------------------------------------------------\n",
      "Year       Count      Percentage\n",
      "--------------------------------------------------\n",
      "2003       1.0        0.02%\n",
      "2016       5.0        0.08%\n",
      "2020       1.0        0.02%\n",
      "2022       951.0      15.94%\n",
      "2023       1968.0     32.98%\n",
      "2024       2394.0     40.12%\n",
      "2025       192.0      3.22%\n",
      "--------------------------------------------------\n",
      "Total: 5967 records\n",
      "\n",
      "Visualization saved to: /Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/Matched Data/Comprehensive_ES:PCS_matched_course_year_distribution.png\n",
      "Analysis saved to: /Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/Matched Data/Comprehensive_ES:PCS_matched_course_year_analysis.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze_course_years(matched_dataset_path):\n",
    "    \"\"\"\n",
    "    Analyze the distribution of course years in the matched dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    matched_dataset_path (str): Path to the matched dataset Excel file\n",
    "    \"\"\"\n",
    "    print(f\"Analyzing course years in: {matched_dataset_path}\")\n",
    "    \n",
    "    # Load the matched dataset\n",
    "    df = pd.read_excel(matched_dataset_path)\n",
    "    print(f\"Loaded {len(df)} records\")\n",
    "    \n",
    "    # Identify the course year column\n",
    "    # Try different possible column names\n",
    "    course_year_columns = ['post_course_year', 'course_year', 'pre_course_year']\n",
    "    \n",
    "    course_year_col = None\n",
    "    for col in course_year_columns:\n",
    "        if col in df.columns:\n",
    "            course_year_col = col\n",
    "            break\n",
    "    \n",
    "    if not course_year_col:\n",
    "        print(\"Error: Could not find course year column in the dataset\")\n",
    "        print(f\"Available columns: {df.columns.tolist()}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Using column '{course_year_col}' for course year analysis\")\n",
    "    \n",
    "    # Count records by course year\n",
    "    year_counts = df[course_year_col].value_counts().sort_index()\n",
    "    total_records = len(df)\n",
    "    \n",
    "    # Calculate percentages\n",
    "    year_percentages = (year_counts / total_records * 100).round(2)\n",
    "    \n",
    "    # Create a DataFrame for the results\n",
    "    results_df = pd.DataFrame({\n",
    "        'Course Year': year_counts.index,\n",
    "        'Count': year_counts.values,\n",
    "        'Percentage': year_percentages.values\n",
    "    })\n",
    "    \n",
    "    # Sort by year\n",
    "    results_df = results_df.sort_values('Course Year')\n",
    "    \n",
    "    # Display the results\n",
    "    print(\"\\nCourse Year Distribution:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Year':<10} {'Count':<10} {'Percentage':<10}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for _, row in results_df.iterrows():\n",
    "        print(f\"{int(row['Course Year']):<10} {row['Count']:<10} {row['Percentage']}%\")\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Total: {total_records} records\")\n",
    "    \n",
    "    # Create a visualization\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Bar chart\n",
    "    bars = plt.bar(results_df['Course Year'].astype(str), results_df['Count'])\n",
    "    \n",
    "    # Add count and percentage labels\n",
    "    for i, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        percentage = results_df['Percentage'].iloc[i]\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 5,\n",
    "                f'{int(height)}\\n({percentage}%)', \n",
    "                ha='center', va='bottom')\n",
    "    \n",
    "    plt.title('Distribution of Records by Course Year', fontsize=15)\n",
    "    plt.xlabel('Course Year', fontsize=12)\n",
    "    plt.ylabel('Number of Records', fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Save the chart\n",
    "    chart_path = matched_dataset_path.replace('.xlsx', '_course_year_distribution.png')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(chart_path, dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"\\nVisualization saved to: {chart_path}\")\n",
    "    \n",
    "    # Save the results to Excel\n",
    "    excel_path = matched_dataset_path.replace('.xlsx', '_course_year_analysis.xlsx')\n",
    "    results_df.to_excel(excel_path, index=False)\n",
    "    print(f\"Analysis saved to: {excel_path}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Run the analysis\n",
    "matched_dataset_path = '/Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/Matched Data/Comprehensive_ES:PCS_matched.xlsx'\n",
    "course_year_analysis = analyze_course_years(matched_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9488ff9b-f5c0-466a-a9a1-ee14cef023f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09c9f1e9-1256-44a7-8c6a-7598195708f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing course year distributions across all datasets\n",
      "\n",
      "Loading datasets...\n",
      "Pre-enrollment dataset: 29700 records\n",
      "Post-course dataset: 7806 records\n",
      "Matched dataset: 5967 records\n",
      "\n",
      "Selected year columns for analysis:\n",
      "Pre-enrollment: course_year\n",
      "Post-course: course_year\n",
      "Matched dataset: post_course_year\n",
      "\n",
      "Pre-enrollment Course Year Distribution:\n",
      "  Year  Count  Percentage\n",
      "2003.0      8        0.04\n",
      "2016.0     14        0.07\n",
      "2020.0   1340        6.92\n",
      "2021.0   3460       17.87\n",
      "2022.0   3419       17.66\n",
      "2023.0   4712       24.34\n",
      "2024.0   5861       30.27\n",
      "2025.0    549        2.84\n",
      "\n",
      "Post-course Year Distribution:\n",
      "  Year  Count  Percentage\n",
      "2003.0      1        0.01\n",
      "2016.0      4        0.06\n",
      "2020.0      1        0.01\n",
      "2022.0   1022       14.76\n",
      "2023.0   2585       37.34\n",
      "2024.0   3027       43.73\n",
      "2025.0    282        4.07\n",
      "\n",
      "Matched Dataset Year Distribution:\n",
      "  Year  Count  Percentage\n",
      "2003.0      1        0.02\n",
      "2016.0      5        0.09\n",
      "2020.0      1        0.02\n",
      "2022.0    951       17.25\n",
      "2023.0   1968       35.70\n",
      "2024.0   2394       43.43\n",
      "2025.0    192        3.48\n",
      "\n",
      "Analysis saved to: /Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/Matched Data/Course_Year_Analysis/course_year_analysis.xlsx\n",
      "Visualizations saved to: /Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/Matched Data/Course_Year_Analysis\n"
     ]
    }
   ],
   "source": [
    "#course year distribution analysis\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def analyze_course_year_distributions(pre_course_path, post_course_path, matched_dataset_path):\n",
    "    \"\"\"\n",
    "    Analyze and compare the distribution of course years across pre-enrollment,\n",
    "    post-course, and matched datasets.\n",
    "    \n",
    "    Parameters:\n",
    "    pre_course_path (str): Path to pre-enrollment dataset\n",
    "    post_course_path (str): Path to post-course dataset\n",
    "    matched_dataset_path (str): Path to matched dataset\n",
    "    \"\"\"\n",
    "    print(\"Analyzing course year distributions across all datasets\")\n",
    "    \n",
    "    # Load the datasets\n",
    "    print(\"\\nLoading datasets...\")\n",
    "    pre_df = pd.read_excel(pre_course_path)\n",
    "    post_df = pd.read_excel(post_course_path)\n",
    "    matched_df = pd.read_excel(matched_dataset_path)\n",
    "    \n",
    "    print(f\"Pre-enrollment dataset: {len(pre_df)} records\")\n",
    "    print(f\"Post-course dataset: {len(post_df)} records\")\n",
    "    print(f\"Matched dataset: {len(matched_df)} records\")\n",
    "    \n",
    "    # Identify course year columns in each dataset\n",
    "    # These might have different names in each dataset\n",
    "    pre_year_cols = [col for col in pre_df.columns if 'course_year' in col.lower() or 'year' in col.lower()]\n",
    "    post_year_cols = [col for col in post_df.columns if 'course_year' in col.lower() or 'year' in col.lower()]\n",
    "    matched_year_cols = [col for col in matched_df.columns if 'course_year' in col.lower() or 'year' in col.lower()]\n",
    "    \n",
    "    # Select the appropriate column for each dataset\n",
    "    pre_year_col = None\n",
    "    if 'pre_course_year' in pre_df.columns:\n",
    "        pre_year_col = 'pre_course_year'\n",
    "    elif 'course_year' in pre_df.columns:\n",
    "        pre_year_col = 'course_year'\n",
    "    elif len(pre_year_cols) > 0:\n",
    "        pre_year_col = pre_year_cols[0]\n",
    "        \n",
    "    post_year_col = None\n",
    "    if 'post_course_year' in post_df.columns:\n",
    "        post_year_col = 'post_course_year'\n",
    "    elif 'course_year' in post_df.columns:\n",
    "        post_year_col = 'course_year'\n",
    "    elif len(post_year_cols) > 0:\n",
    "        post_year_col = post_year_cols[0]\n",
    "    \n",
    "    matched_year_col = None\n",
    "    if 'post_course_year' in matched_df.columns:\n",
    "        matched_year_col = 'post_course_year'\n",
    "    elif 'pre_course_year' in matched_df.columns:\n",
    "        matched_year_col = 'pre_course_year'\n",
    "    elif 'course_year' in matched_df.columns:\n",
    "        matched_year_col = 'course_year'\n",
    "    elif len(matched_year_cols) > 0:\n",
    "        matched_year_col = matched_year_cols[0]\n",
    "    \n",
    "    # Check if we found suitable columns\n",
    "    print(\"\\nSelected year columns for analysis:\")\n",
    "    print(f\"Pre-enrollment: {pre_year_col}\")\n",
    "    print(f\"Post-course: {post_year_col}\")\n",
    "    print(f\"Matched dataset: {matched_year_col}\")\n",
    "    \n",
    "    if not all([pre_year_col, post_year_col, matched_year_col]):\n",
    "        print(\"\\nWarning: Could not find course year columns in all datasets\")\n",
    "        if not pre_year_col:\n",
    "            print(f\"Pre-enrollment columns: {pre_df.columns.tolist()}\")\n",
    "        if not post_year_col:\n",
    "            print(f\"Post-course columns: {post_df.columns.tolist()}\")\n",
    "        if not matched_year_col:\n",
    "            print(f\"Matched columns: {matched_df.columns.tolist()}\")\n",
    "        return\n",
    "    \n",
    "    # Function to calculate year distribution for a dataset\n",
    "    def get_year_distribution(df, year_col):\n",
    "        # Filter out null values\n",
    "        valid_years = df[df[year_col].notna()]\n",
    "        \n",
    "        # Count by year\n",
    "        year_counts = valid_years[year_col].value_counts().sort_index()\n",
    "        \n",
    "        # Calculate percentages\n",
    "        total = len(valid_years)\n",
    "        year_pcts = (year_counts / total * 100).round(2)\n",
    "        \n",
    "        return pd.DataFrame({\n",
    "            'Year': year_counts.index,\n",
    "            'Count': year_counts.values,\n",
    "            'Percentage': year_pcts.values\n",
    "        })\n",
    "    \n",
    "    # Get distributions for each dataset\n",
    "    pre_dist = get_year_distribution(pre_df, pre_year_col)\n",
    "    post_dist = get_year_distribution(post_df, post_year_col)\n",
    "    matched_dist = get_year_distribution(matched_df, matched_year_col)\n",
    "    \n",
    "    # Print the distributions\n",
    "    print(\"\\nPre-enrollment Course Year Distribution:\")\n",
    "    print(pre_dist.to_string(index=False))\n",
    "    \n",
    "    print(\"\\nPost-course Year Distribution:\")\n",
    "    print(post_dist.to_string(index=False))\n",
    "    \n",
    "    print(\"\\nMatched Dataset Year Distribution:\")\n",
    "    print(matched_dist.to_string(index=False))\n",
    "    \n",
    "    # Create a visualization directory\n",
    "    output_dir = os.path.dirname(matched_dataset_path)\n",
    "    viz_dir = os.path.join(output_dir, \"Course_Year_Analysis\")\n",
    "    os.makedirs(viz_dir, exist_ok=True)\n",
    "    \n",
    "    # Get all unique years across all datasets\n",
    "    all_years = sorted(set(\n",
    "        list(pre_dist['Year']) + \n",
    "        list(post_dist['Year']) + \n",
    "        list(matched_dist['Year'])\n",
    "    ))\n",
    "    \n",
    "    # Function to create consistent dataframe with all years\n",
    "    def standardize_dist(dist_df):\n",
    "        std_df = pd.DataFrame({'Year': all_years})\n",
    "        merged = std_df.merge(dist_df, on='Year', how='left').fillna(0)\n",
    "        return merged\n",
    "    \n",
    "    pre_dist_std = standardize_dist(pre_dist)\n",
    "    post_dist_std = standardize_dist(post_dist)\n",
    "    matched_dist_std = standardize_dist(matched_dist)\n",
    "    \n",
    "    # Create side-by-side bar chart\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    x = np.arange(len(all_years))\n",
    "    width = 0.25\n",
    "    \n",
    "    pre_bars = plt.bar(x - width, pre_dist_std['Percentage'], width, label='Pre-enrollment')\n",
    "    post_bars = plt.bar(x, post_dist_std['Percentage'], width, label='Post-course')\n",
    "    matched_bars = plt.bar(x + width, matched_dist_std['Percentage'], width, label='Matched')\n",
    "    \n",
    "    plt.xlabel('Course Year', fontsize=12)\n",
    "    plt.ylabel('Percentage of Records', fontsize=12)\n",
    "    plt.title('Course Year Distribution Comparison', fontsize=15)\n",
    "    plt.xticks(x, [str(int(year)) for year in all_years])\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Save the comparison chart\n",
    "    comparison_chart_path = os.path.join(viz_dir, \"course_year_comparison.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(comparison_chart_path, dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Create individual charts for each dataset\n",
    "    def create_year_chart(dist_df, title, filename):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        bars = plt.bar(dist_df['Year'].astype(str), dist_df['Percentage'])\n",
    "        \n",
    "        # Add percentage labels\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                    f'{height}%', ha='center', va='bottom')\n",
    "        \n",
    "        plt.title(title, fontsize=15)\n",
    "        plt.xlabel('Course Year', fontsize=12)\n",
    "        plt.ylabel('Percentage of Records', fontsize=12)\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(viz_dir, filename), dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    create_year_chart(pre_dist, 'Pre-enrollment Course Year Distribution', 'pre_course_years.png')\n",
    "    create_year_chart(post_dist, 'Post-course Year Distribution', 'post_course_years.png')\n",
    "    create_year_chart(matched_dist, 'Matched Dataset Year Distribution', 'matched_course_years.png')\n",
    "    \n",
    "    # Create a complete analysis Excel file\n",
    "    analysis_path = os.path.join(viz_dir, \"course_year_analysis.xlsx\")\n",
    "    \n",
    "    with pd.ExcelWriter(analysis_path, engine='openpyxl') as writer:\n",
    "        # Create comparison sheet\n",
    "        comparison_df = pd.DataFrame({'Year': all_years})\n",
    "        \n",
    "        # Add pre-enrollment percentages\n",
    "        pre_pcts = pre_dist_std[['Year', 'Percentage']].rename(columns={'Percentage': 'Pre-enrollment %'})\n",
    "        comparison_df = comparison_df.merge(pre_pcts, on='Year', how='left')\n",
    "        \n",
    "        # Add post-course percentages\n",
    "        post_pcts = post_dist_std[['Year', 'Percentage']].rename(columns={'Percentage': 'Post-course %'})\n",
    "        comparison_df = comparison_df.merge(post_pcts, on='Year', how='left')\n",
    "        \n",
    "        # Add matched percentages\n",
    "        matched_pcts = matched_dist_std[['Year', 'Percentage']].rename(columns={'Percentage': 'Matched %'})\n",
    "        comparison_df = comparison_df.merge(matched_pcts, on='Year', how='left')\n",
    "        \n",
    "        # Add counts\n",
    "        pre_counts = pre_dist_std[['Year', 'Count']].rename(columns={'Count': 'Pre-enrollment Count'})\n",
    "        comparison_df = comparison_df.merge(pre_counts, on='Year', how='left')\n",
    "        \n",
    "        post_counts = post_dist_std[['Year', 'Count']].rename(columns={'Count': 'Post-course Count'})\n",
    "        comparison_df = comparison_df.merge(post_counts, on='Year', how='left')\n",
    "        \n",
    "        matched_counts = matched_dist_std[['Year', 'Count']].rename(columns={'Count': 'Matched Count'})\n",
    "        comparison_df = comparison_df.merge(matched_counts, on='Year', how='left')\n",
    "        \n",
    "        # Calculate match rates\n",
    "        comparison_df['Match Rate (% of Pre)'] = (comparison_df['Matched Count'] / comparison_df['Pre-enrollment Count'] * 100).round(2)\n",
    "        comparison_df['Match Rate (% of Post)'] = (comparison_df['Matched Count'] / comparison_df['Post-course Count'] * 100).round(2)\n",
    "        \n",
    "        # Save each sheet\n",
    "        comparison_df.to_excel(writer, sheet_name='Comparison', index=False)\n",
    "        pre_dist.to_excel(writer, sheet_name='Pre-enrollment', index=False)\n",
    "        post_dist.to_excel(writer, sheet_name='Post-course', index=False)\n",
    "        matched_dist.to_excel(writer, sheet_name='Matched', index=False)\n",
    "    \n",
    "    print(f\"\\nAnalysis saved to: {analysis_path}\")\n",
    "    print(f\"Visualizations saved to: {viz_dir}\")\n",
    "    \n",
    "    return {\n",
    "        'pre_distribution': pre_dist,\n",
    "        'post_distribution': post_dist,\n",
    "        'matched_distribution': matched_dist,\n",
    "        'comparison': comparison_df\n",
    "    }\n",
    "\n",
    "# Run the analysis\n",
    "pre_course_path = '/Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/ES/Cleaned & Enhanced ES Data .xlsx'\n",
    "post_course_path = '/Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/PCS/Cleaned_Merged_PCS_2cleaned.xlsx'\n",
    "matched_dataset_path = '/Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/Matched Data/Comprehensive_ES_PCS_matched.xlsx'\n",
    "\n",
    "results = analyze_course_year_distributions(pre_course_path, post_course_path, matched_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb041570-a4b4-4a2d-8d23-b1b29b62106e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auditing year distributions in pre-enrollment datasets\n",
      "\n",
      "1. Loading datasets...\n",
      "Loaded raw dataset: 34742 records\n",
      "Loaded cleaned dataset: 29700 records\n",
      "\n",
      "2. Identifying relevant columns...\n",
      "Raw dataset - Course year column: have_you_received_a_tgh_device_chromebook_or_ipad_in_the_last_two_years\n",
      "Raw dataset - Submitted date column: submitted_date\n",
      "Cleaned dataset - Course year column: course_year\n",
      "Cleaned dataset - Submitted date column: submitted_date\n",
      "\n",
      "3. Extracting years from columns...\n",
      "\n",
      "4. Analyzing year distributions...\n",
      "\n",
      "Raw Dataset - Course Year Distribution:\n",
      "Empty DataFrame\n",
      "Columns: [Year, Count, Percentage]\n",
      "Index: []\n",
      "\n",
      "Raw Dataset - Submitted Year Distribution:\n",
      "  Year  Count  Percentage\n",
      "2016.0      3        0.01\n",
      "2017.0   1047        3.03\n",
      "2018.0   4820       13.96\n",
      "2019.0   3186        9.22\n",
      "2020.0   3825       11.08\n",
      "2021.0   4428       12.82\n",
      "2022.0   4065       11.77\n",
      "2023.0   5271       15.26\n",
      "2024.0   7278       21.07\n",
      "2025.0    614        1.78\n",
      "\n",
      "Cleaned Dataset - Course Year Distribution:\n",
      "  Year  Count  Percentage\n",
      "2003.0      8        0.04\n",
      "2016.0     14        0.07\n",
      "2020.0   1340        6.92\n",
      "2021.0   3460       17.87\n",
      "2022.0   3419       17.66\n",
      "2023.0   4712       24.34\n",
      "2024.0   5861       30.27\n",
      "2025.0    549        2.84\n",
      "\n",
      "Cleaned Dataset - Submitted Year Distribution:\n",
      "  Year  Count  Percentage\n",
      "2016.0      1        0.00\n",
      "2017.0    918        3.11\n",
      "2018.0   4216       14.28\n",
      "2019.0   2819        9.55\n",
      "2020.0   3060       10.36\n",
      "2021.0   3528       11.95\n",
      "2022.0   3617       12.25\n",
      "2023.0   4492       15.21\n",
      "2024.0   6311       21.37\n",
      "2025.0    569        1.93\n",
      "\n",
      "5. Cross-tabulating course year vs submitted year...\n",
      "\n",
      "Raw Dataset - Cross-tabulation of Course Year vs Submitted Year:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "\n",
      "Cleaned Dataset - Cross-tabulation of Course Year vs Submitted Year:\n",
      "submitted_year   2020.0  2021.0  2022.0  2023.0  2024.0  2025.0    All\n",
      "course_year_val                                                       \n",
      "2003.0                0       0       0       8       0       0      8\n",
      "2016.0                0       0      12       0       2       0     14\n",
      "2020.0             1274      50      12       0       4       0   1340\n",
      "2021.0               37    3326      97       0       0       0   3460\n",
      "2022.0                0      52    3356       9       0       1   3418\n",
      "2023.0                0       0      79    4320     257       0   4656\n",
      "2024.0                0       0       0      64    5645      40   5749\n",
      "2025.0                0       0       0       0      31     518    549\n",
      "All                1311    3428    3556    4401    5939     559  19194\n",
      "\n",
      "6. Creating visualizations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q6/v__5dqvn4fsgnld9cp_x0v1m0000gn/T/ipykernel_33164/230460191.py:329: UserWarning: Attempting to set identical low and high xlims makes transformation singular; automatically expanding.\n",
      "  plt.imshow(cross_viz, cmap='YlOrRd')\n",
      "/var/folders/q6/v__5dqvn4fsgnld9cp_x0v1m0000gn/T/ipykernel_33164/230460191.py:329: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n",
      "  plt.imshow(cross_viz, cmap='YlOrRd')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7. Saving analysis to Excel...\n",
      "\n",
      "Audit completed. Results saved to: /Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/ES/Year_Distribution_Audit\n",
      "Analysis Excel file: /Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/ES/Year_Distribution_Audit/year_distribution_audit.xlsx\n"
     ]
    }
   ],
   "source": [
    "#investigating missing records for pre and post covid\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "def audit_year_distributions(raw_file_path, cleaned_file_path):\n",
    "    \"\"\"\n",
    "    Audit year distributions in pre-enrollment datasets to identify potential \n",
    "    missing data issues, comparing course_year and submitted_date columns.\n",
    "    \n",
    "    Parameters:\n",
    "    raw_file_path (str): Path to the raw pre-enrollment dataset with duplicates\n",
    "    cleaned_file_path (str): Path to the cleaned pre-enrollment dataset\n",
    "    \"\"\"\n",
    "    print(\"Auditing year distributions in pre-enrollment datasets\")\n",
    "    \n",
    "    # Create output directory for results\n",
    "    output_dir = os.path.dirname(raw_file_path)\n",
    "    audit_dir = os.path.join(output_dir, \"Year_Distribution_Audit\")\n",
    "    os.makedirs(audit_dir, exist_ok=True)\n",
    "    \n",
    "    # Step 1: Load the datasets\n",
    "    print(\"\\n1. Loading datasets...\")\n",
    "    try:\n",
    "        raw_df = pd.read_excel(raw_file_path)\n",
    "        print(f\"Loaded raw dataset: {len(raw_df)} records\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading raw dataset: {e}\")\n",
    "        return\n",
    "        \n",
    "    try:\n",
    "        cleaned_df = pd.read_excel(cleaned_file_path)\n",
    "        print(f\"Loaded cleaned dataset: {len(cleaned_df)} records\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading cleaned dataset: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Step 2: Identify relevant columns\n",
    "    print(\"\\n2. Identifying relevant columns...\")\n",
    "    \n",
    "    # Check for course_year columns\n",
    "    raw_year_cols = [col for col in raw_df.columns if 'course_year' in col.lower() or 'year' in col.lower()]\n",
    "    cleaned_year_cols = [col for col in cleaned_df.columns if 'course_year' in col.lower() or 'year' in col.lower()]\n",
    "    \n",
    "    raw_year_col = None\n",
    "    if 'course_year' in raw_df.columns:\n",
    "        raw_year_col = 'course_year'\n",
    "    elif 'pre_course_year' in raw_df.columns:\n",
    "        raw_year_col = 'pre_course_year'\n",
    "    elif len(raw_year_cols) > 0:\n",
    "        raw_year_col = raw_year_cols[0]\n",
    "    \n",
    "    cleaned_year_col = None\n",
    "    if 'course_year' in cleaned_df.columns:\n",
    "        cleaned_year_col = 'course_year'\n",
    "    elif 'pre_course_year' in cleaned_df.columns:\n",
    "        cleaned_year_col = 'pre_course_year'\n",
    "    elif len(cleaned_year_cols) > 0:\n",
    "        cleaned_year_col = cleaned_year_cols[0]\n",
    "    \n",
    "    # Check for submitted_date columns\n",
    "    raw_date_cols = [col for col in raw_df.columns if 'submitted' in col.lower() or 'date' in col.lower()]\n",
    "    cleaned_date_cols = [col for col in cleaned_df.columns if 'submitted' in col.lower() or 'date' in col.lower()]\n",
    "    \n",
    "    raw_date_col = None\n",
    "    if 'pre_submitted_date' in raw_df.columns:\n",
    "        raw_date_col = 'pre_submitted_date'\n",
    "    elif 'submitted_date' in raw_df.columns:\n",
    "        raw_date_col = 'submitted_date'\n",
    "    elif len(raw_date_cols) > 0:\n",
    "        # Try to find the most likely submission date column\n",
    "        for col in raw_date_cols:\n",
    "            if 'submit' in col.lower():\n",
    "                raw_date_col = col\n",
    "                break\n",
    "        if not raw_date_col and raw_date_cols:\n",
    "            raw_date_col = raw_date_cols[0]\n",
    "    \n",
    "    cleaned_date_col = None\n",
    "    if 'pre_submitted_date' in cleaned_df.columns:\n",
    "        cleaned_date_col = 'pre_submitted_date'\n",
    "    elif 'submitted_date' in cleaned_df.columns:\n",
    "        cleaned_date_col = 'submitted_date'\n",
    "    elif len(cleaned_date_cols) > 0:\n",
    "        # Try to find the most likely submission date column\n",
    "        for col in cleaned_date_cols:\n",
    "            if 'submit' in col.lower():\n",
    "                cleaned_date_col = col\n",
    "                break\n",
    "        if not cleaned_date_col and cleaned_date_cols:\n",
    "            cleaned_date_col = cleaned_date_cols[0]\n",
    "    \n",
    "    print(f\"Raw dataset - Course year column: {raw_year_col}\")\n",
    "    print(f\"Raw dataset - Submitted date column: {raw_date_col}\")\n",
    "    print(f\"Cleaned dataset - Course year column: {cleaned_year_col}\")\n",
    "    print(f\"Cleaned dataset - Submitted date column: {cleaned_date_col}\")\n",
    "    \n",
    "    if not all([raw_year_col, raw_date_col, cleaned_year_col, cleaned_date_col]):\n",
    "        print(\"\\nWarning: Could not identify all necessary columns\")\n",
    "        # Print the first few column names if we couldn't find what we need\n",
    "        if not raw_year_col or not raw_date_col:\n",
    "            print(f\"Raw dataset columns: {raw_df.columns.tolist()[:20]}...\")\n",
    "        if not cleaned_year_col or not cleaned_date_col:\n",
    "            print(f\"Cleaned dataset columns: {cleaned_df.columns.tolist()[:20]}...\")\n",
    "        return\n",
    "    \n",
    "    # Step 3: Define function to extract year from various date formats\n",
    "    def extract_year_from_date(date_val):\n",
    "        if pd.isna(date_val):\n",
    "            return None\n",
    "        \n",
    "        # If already a datetime\n",
    "        if isinstance(date_val, (pd.Timestamp, datetime)):\n",
    "            return date_val.year\n",
    "        \n",
    "        # Convert to string\n",
    "        date_str = str(date_val).strip()\n",
    "        \n",
    "        # Try direct year extraction with regex\n",
    "        # Look for 4-digit year\n",
    "        year_match = re.search(r'20\\d{2}', date_str)\n",
    "        if year_match:\n",
    "            return int(year_match.group(0))\n",
    "            \n",
    "        # Look for 2-digit year and assume 20YY\n",
    "        yy_match = re.search(r'\\b\\d{1,2}/\\d{1,2}/(\\d{2})\\b', date_str)\n",
    "        if yy_match:\n",
    "            year = int(yy_match.group(1))\n",
    "            return 2000 + year\n",
    "        \n",
    "        # Try standard date parsing\n",
    "        try:\n",
    "            date_obj = pd.to_datetime(date_str)\n",
    "            return date_obj.year\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Try various common formats\n",
    "        formats = ['%m/%d/%Y', '%Y-%m-%d', '%m-%d-%Y', '%d/%m/%Y', '%m/%d/%y']\n",
    "        for fmt in formats:\n",
    "            try:\n",
    "                date_obj = datetime.strptime(date_str, fmt)\n",
    "                return date_obj.year\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        # If nothing works, return None\n",
    "        return None\n",
    "    \n",
    "    # Step 4: Extract years from course_year and submitted_date\n",
    "    print(\"\\n3. Extracting years from columns...\")\n",
    "    \n",
    "    # For raw dataset\n",
    "    raw_df['course_year_val'] = raw_df[raw_year_col].copy()\n",
    "    # Make sure course_year is numeric\n",
    "    raw_df['course_year_val'] = pd.to_numeric(raw_df['course_year_val'], errors='coerce')\n",
    "    \n",
    "    raw_df['submitted_year'] = raw_df[raw_date_col].apply(extract_year_from_date)\n",
    "    \n",
    "    # For cleaned dataset\n",
    "    cleaned_df['course_year_val'] = cleaned_df[cleaned_year_col].copy()\n",
    "    # Make sure course_year is numeric\n",
    "    cleaned_df['course_year_val'] = pd.to_numeric(cleaned_df['course_year_val'], errors='coerce')\n",
    "    \n",
    "    cleaned_df['submitted_year'] = cleaned_df[cleaned_date_col].apply(extract_year_from_date)\n",
    "    \n",
    "    # Step 5: Analyze the distributions\n",
    "    print(\"\\n4. Analyzing year distributions...\")\n",
    "    \n",
    "    # Function to get year distribution from a dataset\n",
    "    def get_year_distribution(df, year_col, year_name):\n",
    "        year_counts = df[year_col].value_counts().sort_index()\n",
    "        total = year_counts.sum()\n",
    "        year_pcts = (year_counts / total * 100).round(2)\n",
    "        \n",
    "        result_df = pd.DataFrame({\n",
    "            'Year': year_counts.index,\n",
    "            'Count': year_counts.values,\n",
    "            'Percentage': year_pcts.values,\n",
    "            'Source': year_name\n",
    "        })\n",
    "        return result_df\n",
    "    \n",
    "    # Get distributions\n",
    "    raw_course_dist = get_year_distribution(\n",
    "        raw_df.dropna(subset=['course_year_val']), \n",
    "        'course_year_val', \n",
    "        'Raw - Course Year'\n",
    "    )\n",
    "    \n",
    "    raw_submitted_dist = get_year_distribution(\n",
    "        raw_df.dropna(subset=['submitted_year']), \n",
    "        'submitted_year', \n",
    "        'Raw - Submitted Year'\n",
    "    )\n",
    "    \n",
    "    cleaned_course_dist = get_year_distribution(\n",
    "        cleaned_df.dropna(subset=['course_year_val']), \n",
    "        'course_year_val', \n",
    "        'Cleaned - Course Year'\n",
    "    )\n",
    "    \n",
    "    cleaned_submitted_dist = get_year_distribution(\n",
    "        cleaned_df.dropna(subset=['submitted_year']), \n",
    "        'submitted_year', \n",
    "        'Cleaned - Submitted Year'\n",
    "    )\n",
    "    \n",
    "    # Print the distributions\n",
    "    print(\"\\nRaw Dataset - Course Year Distribution:\")\n",
    "    print(raw_course_dist[['Year', 'Count', 'Percentage']].to_string(index=False))\n",
    "    \n",
    "    print(\"\\nRaw Dataset - Submitted Year Distribution:\")\n",
    "    print(raw_submitted_dist[['Year', 'Count', 'Percentage']].to_string(index=False))\n",
    "    \n",
    "    print(\"\\nCleaned Dataset - Course Year Distribution:\")\n",
    "    print(cleaned_course_dist[['Year', 'Count', 'Percentage']].to_string(index=False))\n",
    "    \n",
    "    print(\"\\nCleaned Dataset - Submitted Year Distribution:\")\n",
    "    print(cleaned_submitted_dist[['Year', 'Count', 'Percentage']].to_string(index=False))\n",
    "    \n",
    "    # Step 6: Cross-tabulate course_year vs submitted_year\n",
    "    print(\"\\n5. Cross-tabulating course year vs submitted year...\")\n",
    "    \n",
    "    # For raw dataset\n",
    "    raw_cross = pd.crosstab(\n",
    "        raw_df['course_year_val'], \n",
    "        raw_df['submitted_year'],\n",
    "        margins=True,\n",
    "        normalize=False\n",
    "    )\n",
    "    \n",
    "    # For cleaned dataset\n",
    "    cleaned_cross = pd.crosstab(\n",
    "        cleaned_df['course_year_val'], \n",
    "        cleaned_df['submitted_year'],\n",
    "        margins=True,\n",
    "        normalize=False\n",
    "    )\n",
    "    \n",
    "    print(\"\\nRaw Dataset - Cross-tabulation of Course Year vs Submitted Year:\")\n",
    "    print(raw_cross)\n",
    "    \n",
    "    print(\"\\nCleaned Dataset - Cross-tabulation of Course Year vs Submitted Year:\")\n",
    "    print(cleaned_cross)\n",
    "    \n",
    "    # Step 7: Visualize the distributions\n",
    "    print(\"\\n6. Creating visualizations...\")\n",
    "    \n",
    "    # Combine all distributions for comparison\n",
    "    all_dist = pd.concat([\n",
    "        raw_course_dist, \n",
    "        raw_submitted_dist, \n",
    "        cleaned_course_dist, \n",
    "        cleaned_submitted_dist\n",
    "    ])\n",
    "    \n",
    "    # Get all unique years\n",
    "    all_years = sorted(all_dist['Year'].unique())\n",
    "    \n",
    "    # Create standardized dataframes with all years\n",
    "    distributions = {}\n",
    "    for source in all_dist['Source'].unique():\n",
    "        dist = all_dist[all_dist['Source'] == source]\n",
    "        \n",
    "        # Create a standardized dataframe with all years\n",
    "        std_df = pd.DataFrame({'Year': all_years})\n",
    "        merged = std_df.merge(dist[['Year', 'Count', 'Percentage']], on='Year', how='left').fillna(0)\n",
    "        distributions[source] = merged\n",
    "    \n",
    "    # Create comparison chart\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Line chart with markers\n",
    "    for source, dist in distributions.items():\n",
    "        plt.plot(dist['Year'], dist['Percentage'], marker='o', linewidth=2, label=source)\n",
    "    \n",
    "    plt.xlabel('Year', fontsize=12)\n",
    "    plt.ylabel('Percentage of Records', fontsize=12)\n",
    "    plt.title('Year Distribution Comparison', fontsize=15)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend()\n",
    "    plt.xticks(all_years)\n",
    "    \n",
    "    # Add vertical line around 2019-2021 to highlight the years with issues\n",
    "    plt.axvspan(2019, 2021, alpha=0.2, color='red')\n",
    "    plt.text(2020, 5, 'Years with potential issues', ha='center', fontsize=10)\n",
    "    \n",
    "    # Save the comparison chart\n",
    "    comparison_chart_path = os.path.join(audit_dir, \"year_distribution_comparison.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(comparison_chart_path, dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Create individual charts for each distribution\n",
    "    def create_year_chart(dist_df, title, filename):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        bars = plt.bar(dist_df['Year'].astype(str), dist_df['Percentage'])\n",
    "        \n",
    "        # Add percentage labels\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                    f'{height:.1f}%', ha='center', va='bottom')\n",
    "        \n",
    "        plt.title(title, fontsize=15)\n",
    "        plt.xlabel('Year', fontsize=12)\n",
    "        plt.ylabel('Percentage of Records', fontsize=12)\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(audit_dir, filename), dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    for source, dist in distributions.items():\n",
    "        create_year_chart(dist, f'{source} Distribution', f\"{source.lower().replace(' - ', '_').replace(' ', '_')}_distribution.png\")\n",
    "    \n",
    "    # Create heatmap for cross-tabulations\n",
    "    def create_cross_heatmap(cross_df, title, filename):\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Remove the 'All' row and column for the visualization\n",
    "        cross_viz = cross_df.iloc[:-1, :-1].copy()\n",
    "        \n",
    "        # Create the heatmap\n",
    "        plt.imshow(cross_viz, cmap='YlOrRd')\n",
    "        \n",
    "        # Add text annotations\n",
    "        for i in range(cross_viz.shape[0]):\n",
    "            for j in range(cross_viz.shape[1]):\n",
    "                plt.text(j, i, f'{cross_viz.iloc[i, j]:.0f}', \n",
    "                         ha='center', va='center', \n",
    "                         color='black' if cross_viz.iloc[i, j] < cross_viz.max().max()/2 else 'white')\n",
    "        \n",
    "        plt.colorbar(label='Count')\n",
    "        plt.title(title, fontsize=15)\n",
    "        plt.xlabel('Submitted Year', fontsize=12)\n",
    "        plt.ylabel('Course Year', fontsize=12)\n",
    "        \n",
    "        # Set tick labels\n",
    "        plt.xticks(range(len(cross_viz.columns)), cross_viz.columns)\n",
    "        plt.yticks(range(len(cross_viz.index)), cross_viz.index)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(audit_dir, filename), dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    create_cross_heatmap(raw_cross, 'Raw Dataset: Course Year vs Submitted Year', 'raw_cross_heatmap.png')\n",
    "    create_cross_heatmap(cleaned_cross, 'Cleaned Dataset: Course Year vs Submitted Year', 'cleaned_cross_heatmap.png')\n",
    "    \n",
    "    # Step 8: Save comprehensive analysis to Excel\n",
    "    print(\"\\n7. Saving analysis to Excel...\")\n",
    "    \n",
    "    excel_path = os.path.join(audit_dir, \"year_distribution_audit.xlsx\")\n",
    "    \n",
    "    with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
    "        # Summary sheet\n",
    "        summary_data = []\n",
    "        \n",
    "        # Add row counts\n",
    "        summary_data.append({\n",
    "            'Metric': 'Raw Dataset - Total Records', \n",
    "            'Value': len(raw_df)\n",
    "        })\n",
    "        summary_data.append({\n",
    "            'Metric': 'Raw Dataset - Records with Course Year', \n",
    "            'Value': raw_df['course_year_val'].notna().sum()\n",
    "        })\n",
    "        summary_data.append({\n",
    "            'Metric': 'Raw Dataset - Records with Submitted Year', \n",
    "            'Value': raw_df['submitted_year'].notna().sum()\n",
    "        })\n",
    "        summary_data.append({\n",
    "            'Metric': 'Cleaned Dataset - Total Records', \n",
    "            'Value': len(cleaned_df)\n",
    "        })\n",
    "        summary_data.append({\n",
    "            'Metric': 'Cleaned Dataset - Records with Course Year', \n",
    "            'Value': cleaned_df['course_year_val'].notna().sum()\n",
    "        })\n",
    "        summary_data.append({\n",
    "            'Metric': 'Cleaned Dataset - Records with Submitted Year', \n",
    "            'Value': cleaned_df['submitted_year'].notna().sum()\n",
    "        })\n",
    "        \n",
    "        # Add record counts for 2019-2021\n",
    "        for year in [2019, 2020, 2021]:\n",
    "            raw_course_count = sum(raw_df['course_year_val'] == year)\n",
    "            raw_submitted_count = sum(raw_df['submitted_year'] == year)\n",
    "            cleaned_course_count = sum(cleaned_df['course_year_val'] == year)\n",
    "            cleaned_submitted_count = sum(cleaned_df['submitted_year'] == year)\n",
    "            \n",
    "            summary_data.append({\n",
    "                'Metric': f'Raw Dataset - {year} Course Year Records', \n",
    "                'Value': raw_course_count\n",
    "            })\n",
    "            summary_data.append({\n",
    "                'Metric': f'Raw Dataset - {year} Submitted Year Records', \n",
    "                'Value': raw_submitted_count\n",
    "            })\n",
    "            summary_data.append({\n",
    "                'Metric': f'Cleaned Dataset - {year} Course Year Records', \n",
    "                'Value': cleaned_course_count\n",
    "            })\n",
    "            summary_data.append({\n",
    "                'Metric': f'Cleaned Dataset - {year} Submitted Year Records', \n",
    "                'Value': cleaned_submitted_count\n",
    "            })\n",
    "        \n",
    "        pd.DataFrame(summary_data).to_excel(writer, sheet_name='Summary', index=False)\n",
    "        \n",
    "        # Year distributions\n",
    "        all_dist.to_excel(writer, sheet_name='Year Distributions', index=False)\n",
    "        \n",
    "        # Cross-tabulations\n",
    "        raw_cross.to_excel(writer, sheet_name='Raw Cross-tabulation')\n",
    "        cleaned_cross.to_excel(writer, sheet_name='Cleaned Cross-tabulation')\n",
    "        \n",
    "        # Records for problematic years\n",
    "        for year in [2019, 2020, 2021]:\n",
    "            # Raw records for this year\n",
    "            raw_year_records = raw_df[\n",
    "                (raw_df['course_year_val'] == year) | \n",
    "                (raw_df['submitted_year'] == year)\n",
    "            ]\n",
    "            if len(raw_year_records) > 0:\n",
    "                raw_year_records.to_excel(writer, sheet_name=f'Raw {year} Records', index=False)\n",
    "            \n",
    "            # Cleaned records for this year\n",
    "            cleaned_year_records = cleaned_df[\n",
    "                (cleaned_df['course_year_val'] == year) | \n",
    "                (cleaned_df['submitted_year'] == year)\n",
    "            ]\n",
    "            if len(cleaned_year_records) > 0:\n",
    "                cleaned_year_records.to_excel(writer, sheet_name=f'Cleaned {year} Records', index=False)\n",
    "    \n",
    "    print(f\"\\nAudit completed. Results saved to: {audit_dir}\")\n",
    "    print(f\"Analysis Excel file: {excel_path}\")\n",
    "    \n",
    "    # Return results for further analysis if needed\n",
    "    return {\n",
    "        'raw_course_dist': raw_course_dist,\n",
    "        'raw_submitted_dist': raw_submitted_dist,\n",
    "        'cleaned_course_dist': cleaned_course_dist,\n",
    "        'cleaned_submitted_dist': cleaned_submitted_dist,\n",
    "        'raw_cross': raw_cross,\n",
    "        'cleaned_cross': cleaned_cross\n",
    "    }\n",
    "\n",
    "# Run the audit\n",
    "raw_file_path = '/Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/ES/Merged_ES_Raw.xlsx'\n",
    "cleaned_file_path = '/Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/2nd_cleaned_data_excel.xlsx'\n",
    "\n",
    "results = audit_year_distributions(raw_file_path, cleaned_file_path)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dcf6534c-eca0-4355-b190-fca4965f9fa7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82991dc5-f685-4425-9671-afe6146e11e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: /Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/ES/Cleaned & Enhanced ES Data .xlsx\n",
      "Loaded 29700 records\n",
      "Found 10337 records with missing course years\n",
      "\n",
      "Distribution of submission years (for records with missing course years):\n",
      "  2016.0: 1 records\n",
      "  2017.0: 918 records\n",
      "  2018.0: 4216 records\n",
      "  2019.0: 2819 records\n",
      "  2020.0: 1749 records\n",
      "  2021.0: 100 records\n",
      "  2022.0: 61 records\n",
      "  2023.0: 91 records\n",
      "  2024.0: 372 records\n",
      "  2025.0: 10 records\n",
      "\n",
      "Filled 10337 missing course years using submission years\n",
      "\n",
      "Course year distribution after filling:\n",
      "  2003.0: 8 records\n",
      "  2016.0: 15 records\n",
      "  2017.0: 918 records\n",
      "  2018.0: 4216 records\n",
      "  2019.0: 2819 records\n",
      "  2020.0: 3089 records\n",
      "  2021.0: 3560 records\n",
      "  2022.0: 3480 records\n",
      "  2023.0: 4803 records\n",
      "  2024.0: 6233 records\n",
      "  2025.0: 559 records\n",
      "\n",
      "Saved updated dataset to: /Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/ES/Cleaned_Enhanced_ES_Data_with_Fixed_Years.xlsx\n"
     ]
    }
   ],
   "source": [
    "#fixing course years\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def fill_missing_course_years(input_file_path, output_file_path=None):\n",
    "    \"\"\"\n",
    "    Fill missing course years using submission date as a proxy.\n",
    "    \n",
    "    Parameters:\n",
    "    input_file_path (str): Path to the dataset with missing course years\n",
    "    output_file_path (str, optional): Path to save the corrected dataset\n",
    "    \"\"\"\n",
    "    print(f\"Processing dataset: {input_file_path}\")\n",
    "    \n",
    "    # Load the dataset\n",
    "    df = pd.read_excel(input_file_path)\n",
    "    print(f\"Loaded {len(df)} records\")\n",
    "    \n",
    "    # Identify the course year and submitted date columns\n",
    "    course_year_col = 'course_year' if 'course_year' in df.columns else None\n",
    "    submitted_date_col = 'submitted_date' if 'submitted_date' in df.columns else None\n",
    "    \n",
    "    if not course_year_col or not submitted_date_col:\n",
    "        print(\"Error: Could not find required columns\")\n",
    "        return df\n",
    "    \n",
    "    # Count missing course years\n",
    "    missing_years = df[course_year_col].isna().sum()\n",
    "    print(f\"Found {missing_years} records with missing course years\")\n",
    "    \n",
    "    # Create a copy of the dataframe\n",
    "    df_updated = df.copy()\n",
    "    \n",
    "    # Function to extract year from date\n",
    "    def extract_year(date_val):\n",
    "        if pd.isna(date_val):\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Convert to datetime and extract year\n",
    "            date_obj = pd.to_datetime(date_val)\n",
    "            return date_obj.year\n",
    "        except:\n",
    "            # If conversion fails, return None\n",
    "            return None\n",
    "    \n",
    "    # Extract submission years\n",
    "    df_updated['submission_year'] = df_updated[submitted_date_col].apply(extract_year)\n",
    "    \n",
    "    # Count records by submission year (before filling)\n",
    "    print(\"\\nDistribution of submission years (for records with missing course years):\")\n",
    "    missing_year_counts = df_updated[df_updated[course_year_col].isna()]['submission_year'].value_counts().sort_index()\n",
    "    for year, count in missing_year_counts.items():\n",
    "        print(f\"  {year}: {count} records\")\n",
    "    \n",
    "    # Fill missing course years with submission years\n",
    "    before_count = df_updated[course_year_col].notna().sum()\n",
    "    \n",
    "    # Only fill where course_year is missing but submission_year exists\n",
    "    mask = (df_updated[course_year_col].isna()) & (df_updated['submission_year'].notna())\n",
    "    df_updated.loc[mask, course_year_col] = df_updated.loc[mask, 'submission_year']\n",
    "    \n",
    "    after_count = df_updated[course_year_col].notna().sum()\n",
    "    filled_count = after_count - before_count\n",
    "    \n",
    "    print(f\"\\nFilled {filled_count} missing course years using submission years\")\n",
    "    \n",
    "    # Distribution of course years after filling\n",
    "    print(\"\\nCourse year distribution after filling:\")\n",
    "    year_counts = df_updated[course_year_col].value_counts().sort_index()\n",
    "    for year, count in year_counts.items():\n",
    "        print(f\"  {year}: {count} records\")\n",
    "    \n",
    "    # Save the updated dataset if output path provided\n",
    "    if output_file_path:\n",
    "        # Drop the temporary column\n",
    "        df_updated = df_updated.drop(columns=['submission_year'])\n",
    "        \n",
    "        df_updated.to_excel(output_file_path, index=False)\n",
    "        print(f\"\\nSaved updated dataset to: {output_file_path}\")\n",
    "    \n",
    "    return df_updated\n",
    "\n",
    "# Set paths\n",
    "input_path = '/Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/ES/Cleaned & Enhanced ES Data .xlsx'\n",
    "output_path = '/Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/ES/Cleaned_Enhanced_ES_Data_with_Fixed_Years.xlsx'\n",
    "\n",
    "# Run the function\n",
    "updated_df = fill_missing_course_years(input_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e4faaa-a950-442e-b9ec-28dd43633e07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2f79cf3-762b-4891-9c92-7a44492845a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating course years in matched datasets using submission dates\n",
      "\n",
      "1. Loading datasets...\n",
      "Loaded comprehensive dataset: 5967 records\n",
      "Loaded gold standard dataset: 3241 records\n",
      "\n",
      "2. Identifying relevant columns...\n",
      "Comprehensive dataset - Course year column: pre_course_year\n",
      "Comprehensive dataset - Submitted date column: pre_submitted_date\n",
      "Gold standard dataset - Course year column: pre_course_year\n",
      "Gold standard dataset - Submitted date column: pre_submitted_date\n",
      "\n",
      "3. Analyzing course year distribution before update...\n",
      "\n",
      "Comprehensive Dataset - Course Year Distribution (Before Update):\n",
      "  Year  Count  Percentage\n",
      "2003.0      3        0.05\n",
      "2016.0      5        0.08\n",
      "2020.0     38        0.64\n",
      "2021.0    131        2.20\n",
      "2022.0    959       16.07\n",
      "2023.0   1858       31.14\n",
      "2024.0   2426       40.66\n",
      "2025.0    184        3.08\n",
      "\n",
      "Gold Standard Dataset - Course Year Distribution (Before Update):\n",
      "  Year  Count  Percentage\n",
      "2003.0      1        0.03\n",
      "2016.0      2        0.06\n",
      "2020.0     28        0.86\n",
      "2021.0     74        2.28\n",
      "2022.0    664       20.49\n",
      "2023.0   1030       31.78\n",
      "2024.0   1174       36.22\n",
      "2025.0     99        3.05\n",
      "\n",
      "4. Extracting years from submitted dates...\n",
      "\n",
      "Comprehensive Dataset - Submission Year Distribution:\n",
      "  2017.0: 13 records\n",
      "  2018.0: 68 records\n",
      "  2019.0: 47 records\n",
      "  2020.0: 69 records\n",
      "  2021.0: 131 records\n",
      "  2022.0: 1009 records\n",
      "  2023.0: 1781 records\n",
      "  2024.0: 2568 records\n",
      "  2025.0: 167 records\n",
      "\n",
      "Gold Standard Dataset - Submission Year Distribution:\n",
      "  2017.0: 5 records\n",
      "  2018.0: 19 records\n",
      "  2019.0: 19 records\n",
      "  2020.0: 43 records\n",
      "  2021.0: 75 records\n",
      "  2022.0: 689 records\n",
      "  2023.0: 1008 records\n",
      "  2024.0: 1202 records\n",
      "  2025.0: 94 records\n",
      "\n",
      "5. Filling missing course years...\n",
      "Comprehensive dataset: Filled 363 missing course years\n",
      "Gold standard dataset: Filled 169 missing course years\n",
      "\n",
      "6. Analyzing course year distribution after update...\n",
      "\n",
      "Comprehensive Dataset - Course Year Distribution (Before Update):\n",
      "  Year  Count  Percentage\n",
      "2003.0      3        0.05\n",
      "2016.0      5        0.08\n",
      "2017.0     13        0.22\n",
      "2018.0     68        1.14\n",
      "2019.0     47        0.79\n",
      "2020.0     71        1.19\n",
      "2021.0    133        2.23\n",
      "2022.0    984       16.49\n",
      "2023.0   1876       31.44\n",
      "2024.0   2583       43.29\n",
      "2025.0    184        3.08\n",
      "\n",
      "Gold Standard Dataset - Course Year Distribution (Before Update):\n",
      "  Year  Count  Percentage\n",
      "2003.0      1        0.03\n",
      "2016.0      2        0.06\n",
      "2017.0      5        0.15\n",
      "2018.0     19        0.59\n",
      "2019.0     19        0.59\n",
      "2020.0     43        1.33\n",
      "2021.0     75        2.31\n",
      "2022.0    683       21.07\n",
      "2023.0   1034       31.90\n",
      "2024.0   1261       38.91\n",
      "2025.0     99        3.05\n",
      "\n",
      "7. Creating visualizations...\n",
      "\n",
      "8. Saving updated datasets...\n",
      "Saved updated comprehensive dataset to: /Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/Matched Data/Comprehensive_ES_PCS_matched_updated.xlsx\n",
      "Saved updated gold standard dataset to: /Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/Matched Data/Gold_Standard_Email_Only_Dataset_updated.xlsx\n",
      "\n",
      "9. Saving analysis to Excel...\n",
      "Saved analysis to: /Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/Matched Data/Course_Year_Update_Analysis/course_year_update_analysis.xlsx\n",
      "\n",
      "Update completed successfully!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#updating matched data course years\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def update_course_years_in_matched_datasets(comprehensive_path, gold_standard_path):\n",
    "    \"\"\"\n",
    "    Update course years in matched datasets using pre_submitted_date for missing values.\n",
    "    Analyze course year distribution before and after the update.\n",
    "    \n",
    "    Parameters:\n",
    "    comprehensive_path (str): Path to the comprehensive matched dataset\n",
    "    gold_standard_path (str): Path to the gold standard matched dataset\n",
    "    \"\"\"\n",
    "    print(\"Updating course years in matched datasets using submission dates\")\n",
    "    \n",
    "    # Create output directory for results\n",
    "    output_dir = os.path.dirname(comprehensive_path)\n",
    "    analysis_dir = os.path.join(output_dir, \"Course_Year_Update_Analysis\")\n",
    "    os.makedirs(analysis_dir, exist_ok=True)\n",
    "    \n",
    "    # Step 1: Load the datasets\n",
    "    print(\"\\n1. Loading datasets...\")\n",
    "    comprehensive_df = pd.read_excel(comprehensive_path)\n",
    "    gold_standard_df = pd.read_excel(gold_standard_path)\n",
    "    \n",
    "    print(f\"Loaded comprehensive dataset: {len(comprehensive_df)} records\")\n",
    "    print(f\"Loaded gold standard dataset: {len(gold_standard_df)} records\")\n",
    "    \n",
    "    # Step 2: Identify the relevant columns\n",
    "    print(\"\\n2. Identifying relevant columns...\")\n",
    "    \n",
    "    # Common column patterns\n",
    "    course_year_patterns = ['course_year', 'pre_course_year']\n",
    "    submitted_date_patterns = ['pre_submitted_date', 'submitted_date']\n",
    "    \n",
    "    # Find course year columns\n",
    "    comprehensive_year_col = None\n",
    "    for pattern in course_year_patterns:\n",
    "        for col in comprehensive_df.columns:\n",
    "            if pattern in col.lower():\n",
    "                comprehensive_year_col = col\n",
    "                break\n",
    "        if comprehensive_year_col:\n",
    "            break\n",
    "            \n",
    "    gold_standard_year_col = None\n",
    "    for pattern in course_year_patterns:\n",
    "        for col in gold_standard_df.columns:\n",
    "            if pattern in col.lower():\n",
    "                gold_standard_year_col = col\n",
    "                break\n",
    "        if gold_standard_year_col:\n",
    "            break\n",
    "    \n",
    "    # Find submitted date columns\n",
    "    comprehensive_date_col = None\n",
    "    for pattern in submitted_date_patterns:\n",
    "        for col in comprehensive_df.columns:\n",
    "            if pattern in col.lower():\n",
    "                comprehensive_date_col = col\n",
    "                break\n",
    "        if comprehensive_date_col:\n",
    "            break\n",
    "            \n",
    "    gold_standard_date_col = None\n",
    "    for pattern in submitted_date_patterns:\n",
    "        for col in gold_standard_df.columns:\n",
    "            if pattern in col.lower():\n",
    "                gold_standard_date_col = col\n",
    "                break\n",
    "        if gold_standard_date_col:\n",
    "            break\n",
    "    \n",
    "    print(f\"Comprehensive dataset - Course year column: {comprehensive_year_col}\")\n",
    "    print(f\"Comprehensive dataset - Submitted date column: {comprehensive_date_col}\")\n",
    "    print(f\"Gold standard dataset - Course year column: {gold_standard_year_col}\")\n",
    "    print(f\"Gold standard dataset - Submitted date column: {gold_standard_date_col}\")\n",
    "    \n",
    "    if not all([comprehensive_year_col, comprehensive_date_col, gold_standard_year_col, gold_standard_date_col]):\n",
    "        print(\"\\nWarning: Could not find all necessary columns\")\n",
    "        return\n",
    "    \n",
    "    # Step 3: Analyze course year distribution before update\n",
    "    print(\"\\n3. Analyzing course year distribution before update...\")\n",
    "    \n",
    "    def get_year_distribution(df, year_col, dataset_name):\n",
    "        # Count by year\n",
    "        year_counts = df[year_col].value_counts().sort_index()\n",
    "        \n",
    "        # Calculate percentages\n",
    "        total = len(df)\n",
    "        year_pcts = (year_counts / total * 100).round(2)\n",
    "        \n",
    "        result_df = pd.DataFrame({\n",
    "            'Year': year_counts.index,\n",
    "            'Count': year_counts.values,\n",
    "            'Percentage': year_pcts.values\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n{dataset_name} - Course Year Distribution (Before Update):\")\n",
    "        print(result_df.to_string(index=False))\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    comp_before_dist = get_year_distribution(comprehensive_df, comprehensive_year_col, \"Comprehensive Dataset\")\n",
    "    gold_before_dist = get_year_distribution(gold_standard_df, gold_standard_year_col, \"Gold Standard Dataset\")\n",
    "    \n",
    "    # Step 4: Extract year from submitted date\n",
    "    print(\"\\n4. Extracting years from submitted dates...\")\n",
    "    \n",
    "    def extract_year_from_date(date_val):\n",
    "        if pd.isna(date_val):\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Convert to datetime and extract year\n",
    "            date_obj = pd.to_datetime(date_val)\n",
    "            return date_obj.year\n",
    "        except:\n",
    "            # If conversion fails, return None\n",
    "            return None\n",
    "    \n",
    "    # Process comprehensive dataset\n",
    "    comprehensive_df['submission_year'] = comprehensive_df[comprehensive_date_col].apply(extract_year_from_date)\n",
    "    \n",
    "    # Process gold standard dataset\n",
    "    gold_standard_df['submission_year'] = gold_standard_df[gold_standard_date_col].apply(extract_year_from_date)\n",
    "    \n",
    "    # Count records by submission year\n",
    "    print(\"\\nComprehensive Dataset - Submission Year Distribution:\")\n",
    "    comp_subm_counts = comprehensive_df['submission_year'].value_counts().sort_index()\n",
    "    for year, count in comp_subm_counts.items():\n",
    "        print(f\"  {year}: {count} records\")\n",
    "    \n",
    "    print(\"\\nGold Standard Dataset - Submission Year Distribution:\")\n",
    "    gold_subm_counts = gold_standard_df['submission_year'].value_counts().sort_index()\n",
    "    for year, count in gold_subm_counts.items():\n",
    "        print(f\"  {year}: {count} records\")\n",
    "    \n",
    "    # Step 5: Fill missing course years with submission years\n",
    "    print(\"\\n5. Filling missing course years...\")\n",
    "    \n",
    "    # For comprehensive dataset\n",
    "    comp_before_count = comprehensive_df[comprehensive_year_col].notna().sum()\n",
    "    \n",
    "    # Only fill where course_year is missing and submission_year exists\n",
    "    comp_mask = (comprehensive_df[comprehensive_year_col].isna()) & (comprehensive_df['submission_year'].notna())\n",
    "    comprehensive_df.loc[comp_mask, comprehensive_year_col] = comprehensive_df.loc[comp_mask, 'submission_year']\n",
    "    \n",
    "    comp_after_count = comprehensive_df[comprehensive_year_col].notna().sum()\n",
    "    comp_filled_count = comp_after_count - comp_before_count\n",
    "    \n",
    "    print(f\"Comprehensive dataset: Filled {comp_filled_count} missing course years\")\n",
    "    \n",
    "    # For gold standard dataset\n",
    "    gold_before_count = gold_standard_df[gold_standard_year_col].notna().sum()\n",
    "    \n",
    "    # Only fill where course_year is missing and submission_year exists\n",
    "    gold_mask = (gold_standard_df[gold_standard_year_col].isna()) & (gold_standard_df['submission_year'].notna())\n",
    "    gold_standard_df.loc[gold_mask, gold_standard_year_col] = gold_standard_df.loc[gold_mask, 'submission_year']\n",
    "    \n",
    "    gold_after_count = gold_standard_df[gold_standard_year_col].notna().sum()\n",
    "    gold_filled_count = gold_after_count - gold_before_count\n",
    "    \n",
    "    print(f\"Gold standard dataset: Filled {gold_filled_count} missing course years\")\n",
    "    \n",
    "    # Step 6: Analyze course year distribution after update\n",
    "    print(\"\\n6. Analyzing course year distribution after update...\")\n",
    "    \n",
    "    comp_after_dist = get_year_distribution(comprehensive_df, comprehensive_year_col, \"Comprehensive Dataset\")\n",
    "    gold_after_dist = get_year_distribution(gold_standard_df, gold_standard_year_col, \"Gold Standard Dataset\")\n",
    "    \n",
    "    # Step 7: Visualize before and after distributions\n",
    "    print(\"\\n7. Creating visualizations...\")\n",
    "    \n",
    "    # Function to create comparative visualization\n",
    "    def create_comparison_chart(before_dist, after_dist, title, filename):\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        \n",
    "        # Get all unique years\n",
    "        all_years = sorted(set(list(before_dist['Year']) + list(after_dist['Year'])))\n",
    "        \n",
    "        # Create standardized dataframes with all years\n",
    "        before_std = pd.DataFrame({'Year': all_years})\n",
    "        before_std = before_std.merge(before_dist[['Year', 'Percentage']], on='Year', how='left').fillna(0)\n",
    "        \n",
    "        after_std = pd.DataFrame({'Year': all_years})\n",
    "        after_std = after_std.merge(after_dist[['Year', 'Percentage']], on='Year', how='left').fillna(0)\n",
    "        \n",
    "        # Create bar chart\n",
    "        x = np.arange(len(all_years))\n",
    "        width = 0.35\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(14, 8))\n",
    "        before_bars = ax.bar(x - width/2, before_std['Percentage'], width, label='Before Update')\n",
    "        after_bars = ax.bar(x + width/2, after_std['Percentage'], width, label='After Update')\n",
    "        \n",
    "        # Add labels and title\n",
    "        ax.set_xlabel('Course Year')\n",
    "        ax.set_ylabel('Percentage of Records')\n",
    "        ax.set_title(title)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels([str(int(year)) for year in all_years])\n",
    "        ax.legend()\n",
    "        \n",
    "        # Add value labels\n",
    "        def add_labels(bars):\n",
    "            for bar in bars:\n",
    "                height = bar.get_height()\n",
    "                if height > 0:\n",
    "                    ax.annotate(f'{height:.1f}%',\n",
    "                                xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                                xytext=(0, 3),  # 3 points vertical offset\n",
    "                                textcoords=\"offset points\",\n",
    "                                ha='center', va='bottom')\n",
    "        \n",
    "        add_labels(before_bars)\n",
    "        add_labels(after_bars)\n",
    "        \n",
    "        # Highlight 2019-2021 period\n",
    "        ax.axvspan(x[all_years.index(2019)] - width, x[all_years.index(2021)] + width, alpha=0.2, color='red')\n",
    "        \n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(analysis_dir, filename), dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    create_comparison_chart(\n",
    "        comp_before_dist, \n",
    "        comp_after_dist, \n",
    "        'Comprehensive Dataset - Course Year Distribution Before and After Update', \n",
    "        'comprehensive_before_after.png'\n",
    "    )\n",
    "    \n",
    "    create_comparison_chart(\n",
    "        gold_before_dist, \n",
    "        gold_after_dist, \n",
    "        'Gold Standard Dataset - Course Year Distribution Before and After Update', \n",
    "        'gold_standard_before_after.png'\n",
    "    )\n",
    "    \n",
    "    # Step 8: Save updated datasets\n",
    "    print(\"\\n8. Saving updated datasets...\")\n",
    "    \n",
    "    # Remove temporary columns\n",
    "    comprehensive_df = comprehensive_df.drop(columns=['submission_year'])\n",
    "    gold_standard_df = gold_standard_df.drop(columns=['submission_year'])\n",
    "    \n",
    "    # Create output paths\n",
    "    comprehensive_output = os.path.join(output_dir, \"Comprehensive_ES_PCS_matched_updated.xlsx\")\n",
    "    gold_standard_output = os.path.join(output_dir, \"Gold_Standard_Email_Only_Dataset_updated.xlsx\")\n",
    "    \n",
    "    comprehensive_df.to_excel(comprehensive_output, index=False)\n",
    "    gold_standard_df.to_excel(gold_standard_output, index=False)\n",
    "    \n",
    "    print(f\"Saved updated comprehensive dataset to: {comprehensive_output}\")\n",
    "    print(f\"Saved updated gold standard dataset to: {gold_standard_output}\")\n",
    "    \n",
    "    # Step 9: Save analysis to Excel\n",
    "    print(\"\\n9. Saving analysis to Excel...\")\n",
    "    \n",
    "    excel_path = os.path.join(analysis_dir, \"course_year_update_analysis.xlsx\")\n",
    "    \n",
    "    with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
    "        # Summary sheet\n",
    "        summary_data = []\n",
    "        \n",
    "        # Add dataset counts\n",
    "        summary_data.append({\n",
    "            'Metric': 'Comprehensive Dataset - Total Records', \n",
    "            'Value': len(comprehensive_df)\n",
    "        })\n",
    "        summary_data.append({\n",
    "            'Metric': 'Comprehensive Dataset - Records with Course Year (Before)', \n",
    "            'Value': comp_before_count\n",
    "        })\n",
    "        summary_data.append({\n",
    "            'Metric': 'Comprehensive Dataset - Records with Course Year (After)', \n",
    "            'Value': comp_after_count\n",
    "        })\n",
    "        summary_data.append({\n",
    "            'Metric': 'Comprehensive Dataset - Course Years Filled', \n",
    "            'Value': comp_filled_count\n",
    "        })\n",
    "        summary_data.append({\n",
    "            'Metric': 'Gold Standard Dataset - Total Records', \n",
    "            'Value': len(gold_standard_df)\n",
    "        })\n",
    "        summary_data.append({\n",
    "            'Metric': 'Gold Standard Dataset - Records with Course Year (Before)', \n",
    "            'Value': gold_before_count\n",
    "        })\n",
    "        summary_data.append({\n",
    "            'Metric': 'Gold Standard Dataset - Records with Course Year (After)', \n",
    "            'Value': gold_after_count\n",
    "        })\n",
    "        summary_data.append({\n",
    "            'Metric': 'Gold Standard Dataset - Course Years Filled', \n",
    "            'Value': gold_filled_count\n",
    "        })\n",
    "        \n",
    "        # Add counts for 2019-2021\n",
    "        for year in [2019, 2020, 2021]:\n",
    "            # Comprehensive dataset\n",
    "            comp_before = len(comp_before_dist[comp_before_dist['Year'] == year]) > 0\n",
    "            comp_before_count = comp_before_dist[comp_before_dist['Year'] == year]['Count'].sum() if comp_before else 0\n",
    "            \n",
    "            comp_after = len(comp_after_dist[comp_after_dist['Year'] == year]) > 0\n",
    "            comp_after_count = comp_after_dist[comp_after_dist['Year'] == year]['Count'].sum() if comp_after else 0\n",
    "            \n",
    "            summary_data.append({\n",
    "                'Metric': f'Comprehensive Dataset - {year} Records (Before)', \n",
    "                'Value': comp_before_count\n",
    "            })\n",
    "            summary_data.append({\n",
    "                'Metric': f'Comprehensive Dataset - {year} Records (After)', \n",
    "                'Value': comp_after_count\n",
    "            })\n",
    "            summary_data.append({\n",
    "                'Metric': f'Comprehensive Dataset - {year} Records Added', \n",
    "                'Value': comp_after_count - comp_before_count\n",
    "            })\n",
    "            \n",
    "            # Gold standard dataset\n",
    "            gold_before = len(gold_before_dist[gold_before_dist['Year'] == year]) > 0\n",
    "            gold_before_count = gold_before_dist[gold_before_dist['Year'] == year]['Count'].sum() if gold_before else 0\n",
    "            \n",
    "            gold_after = len(gold_after_dist[gold_after_dist['Year'] == year]) > 0\n",
    "            gold_after_count = gold_after_dist[gold_after_dist['Year'] == year]['Count'].sum() if gold_after else 0\n",
    "            \n",
    "            summary_data.append({\n",
    "                'Metric': f'Gold Standard Dataset - {year} Records (Before)', \n",
    "                'Value': gold_before_count\n",
    "            })\n",
    "            summary_data.append({\n",
    "                'Metric': f'Gold Standard Dataset - {year} Records (After)', \n",
    "                'Value': gold_after_count\n",
    "            })\n",
    "            summary_data.append({\n",
    "                'Metric': f'Gold Standard Dataset - {year} Records Added', \n",
    "                'Value': gold_after_count - gold_before_count\n",
    "            })\n",
    "        \n",
    "        pd.DataFrame(summary_data).to_excel(writer, sheet_name='Summary', index=False)\n",
    "        \n",
    "        # Distributions\n",
    "        comp_before_dist.to_excel(writer, sheet_name='Comp Before', index=False)\n",
    "        comp_after_dist.to_excel(writer, sheet_name='Comp After', index=False)\n",
    "        gold_before_dist.to_excel(writer, sheet_name='Gold Before', index=False)\n",
    "        gold_after_dist.to_excel(writer, sheet_name='Gold After', index=False)\n",
    "    \n",
    "    print(f\"Saved analysis to: {excel_path}\")\n",
    "    print(\"\\nUpdate completed successfully!\")\n",
    "    \n",
    "    return {\n",
    "        'comprehensive_before': comp_before_dist,\n",
    "        'comprehensive_after': comp_after_dist,\n",
    "        'gold_standard_before': gold_before_dist,\n",
    "        'gold_standard_after': gold_after_dist,\n",
    "        'comprehensive_df': comprehensive_df,\n",
    "        'gold_standard_df': gold_standard_df\n",
    "    }\n",
    "\n",
    "# Run the update\n",
    "comprehensive_path = '/Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/Matched Data/Comprehensive_ES_PCS_matched.xlsx'\n",
    "gold_standard_path = '/Users/sreeharsha/Documents/TGH Data Management Cleaning/FA Data/MERGED FA Data/Matched Data/Gold_Standard_Email_Only_Dataset.xlsx'\n",
    "\n",
    "results = update_course_years_in_matched_datasets(comprehensive_path, gold_standard_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76acd6d8-8f33-4886-b986-e62953059ae7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6df0f1-7005-467b-a639-c7ef76880ff1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228a6061-194d-42e4-b323-4f9b686255d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883b7cda-e617-462b-8347-1da25f11ff13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63e9f38-56bb-4f98-9c7a-f5e4cd7456f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
